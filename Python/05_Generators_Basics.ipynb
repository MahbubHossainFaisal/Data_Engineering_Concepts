{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65026bd5",
   "metadata": {},
   "source": [
    "\n",
    "### üßë‚Äçüíª **Q1: Generate Numbers 1 to N**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator function `generate_numbers(n)` that yields numbers from `1` to `n` one by one.\n",
    "\n",
    "üí° **Why:** This mirrors scenarios like streaming row IDs or sequential event timestamps.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "gen = generate_numbers(5)\n",
    "print(list(gen))  # Output: [1, 2, 3, 4, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f79e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def generate_numbers(n):\n",
    "    for i in range(1,n+1):\n",
    "        yield i\n",
    "        \n",
    "gen = generate_numbers(5)\n",
    "\n",
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045e1f5",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q2: Infinite Even Number Stream**\n",
    "\n",
    "**Task:**  \n",
    "Create a generator `generate_even_numbers()` that produces an **infinite sequence of even numbers** starting from `2`. Stop iteration must be controlled from the consumer side.\n",
    "\n",
    "üí° **Why:** Many data systems process streaming data with unknown size, and generators fit perfectly.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "gen = generate_even_numbers()\n",
    "for _ in range(5):\n",
    "    print(next(gen), end=' ')  # Output: 2 4 6 8 10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d27106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 6 8 10 12 14 "
     ]
    }
   ],
   "source": [
    "def generate_even_numbers(num):\n",
    "    for i in range(1,num+1):\n",
    "        if i % 2 == 0:\n",
    "            yield i\n",
    "    \n",
    "    \n",
    "gen = generate_even_numbers(15)\n",
    "for i in gen:\n",
    "    print(i,end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b597d92",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q3: Square Values of a List**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator `square_numbers(lst)` that takes a list of numbers and lazily yields their squares.\n",
    "\n",
    "üí° **Why:** Mimics data transformation pipelines ‚Äî e.g., normalizing numerical fields.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "lst = [1, 2, 3, 4]\n",
    "gen = square_numbers(lst)\n",
    "print(list(gen))  # Output: [1, 4, 9, 16]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec05947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 9 16 "
     ]
    }
   ],
   "source": [
    "def square_numbers(lst):\n",
    "    for i in lst:\n",
    "        yield i**2\n",
    "        \n",
    "gen = square_numbers([1,2,3,4])\n",
    "for i in gen:\n",
    "    print(i,end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93d16d",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q4: Chunk a Large Dataset**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator `chunker(iterable, chunk_size)` that splits large data into smaller chunks of size `chunk_size`.\n",
    "\n",
    "üí° **Why:** Chunking is fundamental for batch processing ‚Äî Snowflake, S3, GCS, Kafka all use this.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "lst = [1, 2, 3, 4, 5, 6, 7]\n",
    "gen = chunker(lst, 3)\n",
    "for chunk in gen:\n",
    "    print(chunk)  # Output: [1,2,3], [4,5,6], [7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da5c7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(iterable, chunk_size):\n",
    "    chunk = []\n",
    "    \n",
    "    for item in iterable:\n",
    "        chunk.append(item)\n",
    "        if len(chunk) == chunk_size:\n",
    "            yield chunk\n",
    "            chunk=[]\n",
    "    if chunk:\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a3f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[4, 5, 6]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "lst = [1, 2, 3, 4, 5, 6, 7]\n",
    "gen = chunker(lst, 3)\n",
    "for chunk in gen:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6ac63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßë‚Äçüíª **Q5: CSV Row Reader**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator `read_csv(file_path)` that reads a CSV file line by line, yielding each row as a list.\n",
    "\n",
    "üí° **Why:**  \n",
    "When dealing with huge files like logs or transaction records, you never want to load the whole file into memory.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Suppose file content is:\n",
    "# John,30,NY\n",
    "# Alice,25,LA\n",
    "\n",
    "gen = read_csv('users.csv')\n",
    "print(next(gen))  # Output: ['John', '30', 'NY']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "082b3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def gen_read_csv(file_path):\n",
    "    with open(file_path,newline='\\n') as csvfile:\n",
    "        line = csv.reader(csvfile)\n",
    "        for row in line:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df45d7b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', '30', 'NY']\n",
      "['Mahbub', '30', 'NA']\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/Mahbub/Desktop/Data Engineering/Python/data/info.csv'\n",
    "\n",
    "gen = gen_read_csv(path)\n",
    "\n",
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50807b",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q6: Filter Positive Numbers**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator `filter_positive_numbers(numbers)` that yields only **positive integers** from a given list.\n",
    "\n",
    "üí° **Why:** Filtering raw data before further processing saves compute and network.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "lst = [-1, 4, -2, 5, 0]\n",
    "gen = filter_positive_numbers(lst)\n",
    "print(list(gen))  # Output: [4, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d859c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]\n"
     ]
    }
   ],
   "source": [
    "def filter_positive_numbers(lst):\n",
    "    for i in lst:\n",
    "        if i>0:\n",
    "            yield i\n",
    "            \n",
    "lst = [-1, 4, -2, 5, 0]            \n",
    "gen = filter_positive_numbers(lst)\n",
    "print(list(gen))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759cd82",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q7: Paginated API Simulation**\n",
    "\n",
    "**Task:**  \n",
    "Create a generator `mock_api_paginator(data_list, page_size)` that yields one \"page\" at a time from a list.\n",
    "\n",
    "üí° **Why:** APIs like AWS S3 or BigQuery use pagination when the result set is too large.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "data = list(range(1, 11))\n",
    "gen = mock_api_paginator(data, 4)\n",
    "print(next(gen))  # Output: [1, 2, 3, 4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47e41204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[5, 6, 7, 8]\n",
      "[9, 10]\n"
     ]
    }
   ],
   "source": [
    "def mock_api_paginator(data_list, page_size):\n",
    "    for data in range(0,len(data_list),page_size):\n",
    "        yield data_list[data:data+page_size]\n",
    "        \n",
    "        \n",
    "data = list(range(1, 11))\n",
    "gen = mock_api_paginator(data, 4)\n",
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a846999",
   "metadata": {},
   "source": [
    "\n",
    "### üßë‚Äçüíª **Q8: Infinite Fibonacci Sequence**\n",
    "\n",
    "**Task:**  \n",
    "Write `fibonacci_sequence()` ‚Äî a generator that yields an infinite Fibonacci series:  \n",
    "`0, 1, 1, 2, 3, 5, 8, ...`\n",
    "\n",
    "üí° **Why:** Lazy evaluation is key when the end of the series is unknown.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bafdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fibonacci_sequence():\n",
    "    first = 0\n",
    "    second = 1\n",
    "    while True:\n",
    "        yield first   \n",
    "        first, second = second, first + second  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45273a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "13\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "gen = gen_fibonacci_sequence()\n",
    "\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfb3a0",
   "metadata": {},
   "source": [
    "\n",
    "### üßë‚Äçüíª **Q9: Unique ID Generator**\n",
    "\n",
    "**Task:**  \n",
    "Write `unique_id_generator(prefix)` that yields:  \n",
    "`prefix_1, prefix_2, prefix_3...`\n",
    "\n",
    "üí° **Why:** Used to create synthetic primary keys or filenames.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80eef7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_id_generator(prefix, start=1, width=3):\n",
    "    num = start\n",
    "    while True:\n",
    "        yield f\"{prefix}_{str(num).zfill(width)}\"\n",
    "        num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1b26ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_001\n",
      "user_002\n",
      "user_003\n"
     ]
    }
   ],
   "source": [
    "gen = unique_id_generator(\"user\", start=1, width=3)\n",
    "\n",
    "print(next(gen))  # user_001\n",
    "print(next(gen))  # user_002\n",
    "print(next(gen))  # user_003\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df48c2",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q10: Date Range Generator**\n",
    "\n",
    "**Task:**  \n",
    "Write `date_range(start_date, end_date)` that yields every date from `start` to `end`.\n",
    "\n",
    "üí° **Why:** Generating date partitions for data lake queries or backfilling.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from datetime import date\n",
    "gen = date_range(date(2024, 1, 1), date(2024, 1, 4))\n",
    "print(list(gen))  \n",
    "# Output: [2024-01-01, 2024-01-02, 2024-01-03, 2024-01-04]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1520e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_dates(start_date, end_date):\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        yield current\n",
    "        current += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2aa6a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01\n",
      "2024-01-02\n",
      "2024-01-03\n",
      "2024-01-04\n",
      "2024-01-05\n"
     ]
    }
   ],
   "source": [
    "start = datetime(2024, 1, 1)\n",
    "end = datetime(2024, 1, 5)\n",
    "\n",
    "for date in generate_dates(start, end):\n",
    "    print(date.strftime('%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968e866",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßë‚Äçüíª **Q11: Sliding Window over a Sequence**\n",
    "\n",
    "**Task:**  \n",
    "Write `sliding_window(sequence, window_size)` that yields overlapping sublists (windows).\n",
    "\n",
    "üí° **Why:** Used in **time-series anomaly detection** and **rolling averages**.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "gen = sliding_window([1, 2, 3, 4, 5], 3)\n",
    "# Output: [1,2,3], [2,3,4], [3,4,5]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3999574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(sequence, window_size):\n",
    "    if window_size > len(sequence):\n",
    "        return \n",
    "    for i in range(len(sequence)-window_size+1):\n",
    "        yield sequence[i:i+window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0450e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[2, 3, 4]\n",
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "gen = sliding_window([1, 2, 3, 4, 5], 3)\n",
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d81afc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßë‚Äçüíª **Q12: Apply Transformation to a Stream**\n",
    "\n",
    "**Task:**  \n",
    "Write `apply_transformation(generator, func)` to yield `func(item)` for every item in the input generator.\n",
    "\n",
    "üí° **Why:** Fundamental for transformation stages in ETL pipelines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8672af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "def apply_transformation(generator,func):\n",
    "    for i in generator:\n",
    "        yield func(i)\n",
    "\n",
    "def number_stream(lst):\n",
    "    for i in lst:\n",
    "        yield i\n",
    "        \n",
    "        \n",
    "lst = [1,2,3,4,5] \n",
    "\n",
    "gen1 = number_stream(lst)\n",
    "gen2 = apply_transformation(gen1,square)\n",
    "\n",
    "for i in gen2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18feb099",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßë‚Äçüíª **Q13: Flatten Nested Lists**\n",
    "\n",
    "**Task:**  \n",
    "Write `flatten(nested_list)` that flattens nested lists:  \n",
    "Example: `[[1, 2], [3, 4]]` ‚Üí `1, 2, 3, 4`\n",
    "\n",
    "üí° **Why:** Flattening nested records for schema normalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e9d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "def flatten(lst):\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            yield from flatten(item)\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "nested = [1, [2, 3], [4, [5, 6]], 7]\n",
    "flat = list(flatten(nested))\n",
    "print(flat)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ccf8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßë‚Äçüíª **Q14: Log File Tailer**\n",
    "\n",
    "**Task:**  \n",
    "Write `tail(file_path)` that watches a log file and yields new lines as they are written.\n",
    "\n",
    "üí° **Why:** Log monitoring without loading entire files ‚Äî similar to `tail -f`. Stop after max line number\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e7b24b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hossain\n",
      "Abeer\n",
      "Imran\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def tail(file_path,max_lines):\n",
    "    with open(file_path,'r') as f:\n",
    "        line_count=0\n",
    "        f.seek(0,2)\n",
    "        while line_count<=max_lines:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            \n",
    "            yield line.strip()\n",
    "            line_count+=1\n",
    "            \n",
    "\n",
    "            \n",
    "file_path = 'C:/Users/Mahbub/Desktop/Data Engineering/Python/data/info.log'\n",
    "gen = tail(file_path,3)\n",
    "\n",
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999888c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßë‚Äçüíª **Q15: Build Generator Pipelines**\n",
    "\n",
    "**Task:**  \n",
    "Design a sequence of generators where each generator processes the output of the previous one, e.g.:  \n",
    "- Generator1: reads lines from a file.  \n",
    "- Generator2: filters valid JSON lines.  \n",
    "- Generator3: extracts fields.\n",
    "\n",
    "üí° **Why:** Used in ETL pipelines, Kafka consumers, and Spark-like frameworks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e310756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_lines(file_path):\n",
    "    with open(file_path,'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            yield line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab2317c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def filter_valid_json(lines):\n",
    "    for line in lines:\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            yield json_obj\n",
    "        except json.JSONDecodeError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc2ad596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields(json_lines,field_names):\n",
    "    for json_obj in json_lines:\n",
    "        extracted = {}\n",
    "        for field in field_names:\n",
    "            extracted[field] = json_obj.get(field)\n",
    "        yield extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1b5eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Mahbub/Desktop/Data Engineering/Python/data/log_file.json'\n",
    "\n",
    "pipeline1 = read_lines(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fc8210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = filter_valid_json(pipeline1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ad2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'timestamp': '2024-04-17T10:00:00', 'message': 'System started'}\n",
      "{'id': 2, 'timestamp': '2024-04-17T10:05:00', 'message': 'User logged in'}\n",
      "{'id': 3, 'timestamp': '2024-04-17T10:10:00', 'message': None}\n"
     ]
    }
   ],
   "source": [
    "fields = ['id','timestamp','message']\n",
    "pipeline3 = extract_fields(pipeline2,fields)\n",
    "\n",
    "\n",
    "for i in pipeline3:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2592e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
