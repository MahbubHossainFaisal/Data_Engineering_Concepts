{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65026bd5",
   "metadata": {},
   "source": [
    "\n",
    "### üßë‚Äçüíª **Q1: Generate Numbers 1 to N**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator function `generate_numbers(n)` that yields numbers from `1` to `n` one by one.\n",
    "\n",
    "üí° **Why:** This mirrors scenarios like streaming row IDs or sequential event timestamps.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "gen = generate_numbers(5)\n",
    "print(list(gen))  # Output: [1, 2, 3, 4, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f79e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def generate_numbers(n):\n",
    "    for i in range(1,n+1):\n",
    "        yield i\n",
    "        \n",
    "gen = generate_numbers(5)\n",
    "\n",
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045e1f5",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q2: Infinite Even Number Stream**\n",
    "\n",
    "**Task:**  \n",
    "Create a generator `generate_even_numbers()` that produces an **infinite sequence of even numbers** starting from `2`. Stop iteration must be controlled from the consumer side.\n",
    "\n",
    "üí° **Why:** Many data systems process streaming data with unknown size, and generators fit perfectly.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "gen = generate_even_numbers()\n",
    "for _ in range(5):\n",
    "    print(next(gen), end=' ')  # Output: 2 4 6 8 10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d27106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 6 8 10 12 14 "
     ]
    }
   ],
   "source": [
    "def generate_even_numbers(num):\n",
    "    for i in range(1,num+1):\n",
    "        if i % 2 == 0:\n",
    "            yield i\n",
    "    \n",
    "    \n",
    "gen = generate_even_numbers(15)\n",
    "for i in gen:\n",
    "    print(i,end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b597d92",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q3: Square Values of a List**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator `square_numbers(lst)` that takes a list of numbers and lazily yields their squares.\n",
    "\n",
    "üí° **Why:** Mimics data transformation pipelines ‚Äî e.g., normalizing numerical fields.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "lst = [1, 2, 3, 4]\n",
    "gen = square_numbers(lst)\n",
    "print(list(gen))  # Output: [1, 4, 9, 16]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec05947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 9 16 "
     ]
    }
   ],
   "source": [
    "def square_numbers(lst):\n",
    "    for i in lst:\n",
    "        yield i**2\n",
    "        \n",
    "gen = square_numbers([1,2,3,4])\n",
    "for i in gen:\n",
    "    print(i,end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93d16d",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q4: Chunk a Large Dataset**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator `chunker(iterable, chunk_size)` that splits large data into smaller chunks of size `chunk_size`.\n",
    "\n",
    "üí° **Why:** Chunking is fundamental for batch processing ‚Äî Snowflake, S3, GCS, Kafka all use this.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "lst = [1, 2, 3, 4, 5, 6, 7]\n",
    "gen = chunker(lst, 3)\n",
    "for chunk in gen:\n",
    "    print(chunk)  # Output: [1,2,3], [4,5,6], [7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da5c7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(iterable, chunk_size):\n",
    "    chunk = []\n",
    "    \n",
    "    for item in iterable:\n",
    "        chunk.append(item)\n",
    "        if len(chunk) == chunk_size:\n",
    "            yield chunk\n",
    "            chunk=[]\n",
    "    if chunk:\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a3f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[4, 5, 6]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "lst = [1, 2, 3, 4, 5, 6, 7]\n",
    "gen = chunker(lst, 3)\n",
    "for chunk in gen:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6ac63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßë‚Äçüíª **Q5: CSV Row Reader**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator `read_csv(file_path)` that reads a CSV file line by line, yielding each row as a list.\n",
    "\n",
    "üí° **Why:**  \n",
    "When dealing with huge files like logs or transaction records, you never want to load the whole file into memory.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Suppose file content is:\n",
    "# John,30,NY\n",
    "# Alice,25,LA\n",
    "\n",
    "gen = read_csv('users.csv')\n",
    "print(next(gen))  # Output: ['John', '30', 'NY']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "082b3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def gen_read_csv(file_path):\n",
    "    with open(file_path,newline='\\n') as csvfile:\n",
    "        line = csv.reader(csvfile)\n",
    "        for row in line:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df45d7b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', '30', 'NY']\n",
      "['Mahbub', '30', 'NA']\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/Mahbub/Desktop/Data Engineering/Python/data/info.csv'\n",
    "\n",
    "gen = gen_read_csv(path)\n",
    "\n",
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50807b",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q6: Filter Positive Numbers**\n",
    "\n",
    "**Task:**  \n",
    "Write a generator `filter_positive_numbers(numbers)` that yields only **positive integers** from a given list.\n",
    "\n",
    "üí° **Why:** Filtering raw data before further processing saves compute and network.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "lst = [-1, 4, -2, 5, 0]\n",
    "gen = filter_positive_numbers(lst)\n",
    "print(list(gen))  # Output: [4, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d859c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]\n"
     ]
    }
   ],
   "source": [
    "def filter_positive_numbers(lst):\n",
    "    for i in lst:\n",
    "        if i>0:\n",
    "            yield i\n",
    "            \n",
    "lst = [-1, 4, -2, 5, 0]            \n",
    "gen = filter_positive_numbers(lst)\n",
    "print(list(gen))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759cd82",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q7: Paginated API Simulation**\n",
    "\n",
    "**Task:**  \n",
    "Create a generator `mock_api_paginator(data_list, page_size)` that yields one \"page\" at a time from a list.\n",
    "\n",
    "üí° **Why:** APIs like AWS S3 or BigQuery use pagination when the result set is too large.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "data = list(range(1, 11))\n",
    "gen = mock_api_paginator(data, 4)\n",
    "print(next(gen))  # Output: [1, 2, 3, 4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47e41204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[5, 6, 7, 8]\n",
      "[9, 10]\n"
     ]
    }
   ],
   "source": [
    "def mock_api_paginator(data_list, page_size):\n",
    "    for data in range(0,len(data_list),page_size):\n",
    "        yield data_list[data:data+page_size]\n",
    "        \n",
    "        \n",
    "data = list(range(1, 11))\n",
    "gen = mock_api_paginator(data, 4)\n",
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a846999",
   "metadata": {},
   "source": [
    "\n",
    "### üßë‚Äçüíª **Q8: Infinite Fibonacci Sequence**\n",
    "\n",
    "**Task:**  \n",
    "Write `fibonacci_sequence()` ‚Äî a generator that yields an infinite Fibonacci series:  \n",
    "`0, 1, 1, 2, 3, 5, 8, ...`\n",
    "\n",
    "üí° **Why:** Lazy evaluation is key when the end of the series is unknown.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bafdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fibonacci_sequence():\n",
    "    first = 0\n",
    "    second = 1\n",
    "    while True:\n",
    "        yield first   \n",
    "        first, second = second, first + second  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45273a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "13\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "gen = gen_fibonacci_sequence()\n",
    "\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfb3a0",
   "metadata": {},
   "source": [
    "\n",
    "### üßë‚Äçüíª **Q9: Unique ID Generator**\n",
    "\n",
    "**Task:**  \n",
    "Write `unique_id_generator(prefix)` that yields:  \n",
    "`prefix_1, prefix_2, prefix_3...`\n",
    "\n",
    "üí° **Why:** Used to create synthetic primary keys or filenames.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80eef7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_id_generator(prefix, start=1, width=3):\n",
    "    num = start\n",
    "    while True:\n",
    "        yield f\"{prefix}_{str(num).zfill(width)}\"\n",
    "        num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1b26ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_001\n",
      "user_002\n",
      "user_003\n"
     ]
    }
   ],
   "source": [
    "gen = unique_id_generator(\"user\", start=1, width=3)\n",
    "\n",
    "print(next(gen))  # user_001\n",
    "print(next(gen))  # user_002\n",
    "print(next(gen))  # user_003\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df48c2",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª **Q10: Date Range Generator**\n",
    "\n",
    "**Task:**  \n",
    "Write `date_range(start_date, end_date)` that yields every date from `start` to `end`.\n",
    "\n",
    "üí° **Why:** Generating date partitions for data lake queries or backfilling.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from datetime import date\n",
    "gen = date_range(date(2024, 1, 1), date(2024, 1, 4))\n",
    "print(list(gen))  \n",
    "# Output: [2024-01-01, 2024-01-02, 2024-01-03, 2024-01-04]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1520e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_dates(start_date, end_date):\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        yield current\n",
    "        current += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2aa6a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01\n",
      "2024-01-02\n",
      "2024-01-03\n",
      "2024-01-04\n",
      "2024-01-05\n"
     ]
    }
   ],
   "source": [
    "start = datetime(2024, 1, 1)\n",
    "end = datetime(2024, 1, 5)\n",
    "\n",
    "for date in generate_dates(start, end):\n",
    "    print(date.strftime('%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb994d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
