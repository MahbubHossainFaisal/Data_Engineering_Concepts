Prompt Engineering
	- What is it by definition
	- Output length
		- Reducing the output length of the LLM doesn’t cause the LLM to become more stylistically or textually succinct in the output it creates, it just causes the LLM to stop predicting more 
		tokens once the limit is reached.
		-  generating more tokens requires more computation from the LLM, leading to higher energy consumption and potentially slower response times, which leads to 
		higher costs
	-  Sampling controls
		- Temperature, top-K, and top-P are the most common configuration settings that determine how predicted token probabilities are processed to choose a single output token.
		
	-  Temperature
		- Temperature controls the degree of randomness in token selection.
		- Lower temperatures are good for prompts that expect a more deterministic response.
		- Higher temperatures can lead to more diverse or unexpected results.
		- What happens when temperature is set to 0?
		- If two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0.
		- As temperature gets higher and higher, all tokens become equally likely to be the next predicted token.
		
	- Top-K and top-P
		- Also known as nucleus sampling
		- What is Top-K?
			- Purpose?
			- Usecase?
			- What problem it solves?
			- The higher top-K, the more creative and varied the model’s output
			- The lower top-K, the more restive and factual the model’s output.
			- A top-K of 1 is equivalent to greedy decoding
		- What is Top-P?
			- Purpose?
			- Usecase?
			- What problem it solves?
		- Like temperature, these sampling settings control the randomness and diversity of generated text.
		- The best way to choose between top-K and top-P is to experiment with both methods. Why and How?
		- If you set temperature to 0, top-K and top-P become irrelevant
		-  If you set temperature extremely high (above 1–generally into the 10s), temperature becomes irrelevant and whatever tokens make it through the top-K and/or top-P criteria
		are then randomly sampled to choose a next predicted token.
		- If you set top-K to 1, temperature and top-P become irrelevant.
		- If you set top-P to 0 (or a very small value), most LLM sampling implementations will then only consider the most probable token to meet the top-P criteria, making temperature and top-K irrelevant.
		- A temperature of .2, top-P of .95, and top-K of 30 will give you relatively coherent results that can be creative but not excessively so.
		-  If you want especially creative results, try starting with a temperature of .9, top-P of .99, and top-K of 40.
		- If you want less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20. 
		-  If your task always has a single correct answer (e.g., answering a math problem), start with a temperature of 0
		- What is repetition loop bug? Explain with a real case scenario.
			- At low temperatures, the model becomes overly deterministic, sticking rigidly to the highest probability path, which can lead to a loop if that path revisits previously generated text.
			- at high temperatures, the model's output becomes excessively random, increasing the probability that a randomly chosen word or phrase will, by chance, lead back to a prior state,
			creating a loop due to the vast number of available options.
		
	- Prompting techniques
		- zero shot
			- What is it?
			- How does it work?
			- What purpose it serve?
			- The name zero-shot stands for ’no examples’.
		- Create a sample table format structure for writting prompt applying the best practices and keeping it for future modifications and development.
		- When zero-shot doesn’t work, you can provide demonstrations or examples in the prompt, which leads to “one-shot” and “few-shot” prompting.
		- Examples are especially useful when you want to steer the model to a certain output structure or pattern.
		- A one-shot prompt, provides a single example
		- A few-shot prompt 7 provides multiple examples to the model. Multiple examples of the desired pattern increases the chance the model follows the pattern.
		- General rule of thumb, you should use at least three to five examples for few-shot prompting. Can be added more or less based on complex task and input token limit.
		- The examples should be diverse, of high quality, and well written. One small mistake can confuse the model and will result in undesired output.
		-  If you are trying to generate output that is robust to a variety of inputs, then it is important to include edge cases in your examples.
		
	-  System, contextual and role prompting
		- System prompting
			- Sets the overall context and purpose for the language model.
			- Defines the ‘big picture’ of what the model should be doing.
			- Defines the model’s fundamental capabilities and overarching purpose.
		- Contextual prompting
			- Provides specific details or background information relevant to the current conversation or task.
			- Helps the model to understand the nuances of what’s being asked and tailor the response accordingly.
			-  Provides immediate, task-specific information to guide the response. It’s highly specific to the current task or input, which is dynamic.
			- Help ensure that your AI interactions are as seamless and efficient as possible. Model will be able to more quickly understand your 
			request and be able to generate more accurate and relevant responses.
		- Role prompting
			- Assigns a specific character or identity for the language model to adopt.
			- Helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior.
			-  Frames the model’s output style and voice. It adds a layer of specificity and personality.
			-  Defining a role perspective for an AI model gives it a blueprint of the tone, style, and focused expertise you’re looking for to improve the quality, relevance, and effectiveness
			of your output.
			- Some styles for role prompting :
				-  Confrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, Inspirational, Persuasive.
		- There can be considerable overlap between system, contextual, and role prompting. Explain with real scenario.
		- Create a real case scenario and define all these three prompting for that scenario and explain them in detail.
		- Make a perfect system prompt using all the best practices for this goal -> Classify movie reviews as positive, neutral or negative, return JSON.
		- By prompting for a JSON format it forces the model to create a structure and limit hallucinations.
		- System prompts can also be really useful for safety and toxicity. To control the output, simply add an additional line to your prompt like: ‘You should be respectful in your answer.
		- Make a role prompt using all the best practices for this goal -> Act as travel guide and provide 3 travel suggestions.
			- Provide a different role to that above scenario with the best practices used and show how outputs can be changed based on role and style changes.
		- Make a context prompt using all the best practices for this goal ->  Suggest articles for a blog about retro games.
		- 
		