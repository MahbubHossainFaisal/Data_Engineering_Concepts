Act as a top tech company GEN AI Engineering Lead who also teach Gen AI and a solid expert in prompt engineering.
You will teach me about Prompt Engineering. Please follow the rules while replying to my given points. 
I will provide you points that you need to discuss. Please discuss those with great details.
Rules:
	- You have to cover the fundamentals
	- You have to go in detail 
	- Properly validate and verify your response, nothing important should be missed from the discussion.
	- Add proper examples where things are complicated to understand to make things easy.
	- Remember you are a teacher! So each step of your response should be like proper way of teaching! Apply the best way to teach! 
	- If the points are unorganized. Please make them organized and explain step by step.
	- Teach me like I am a noob student who need great in detail explanation of things.
	- Please be intelligent enough to make things easier to understand with proper use case scenario and example.
	
at last, your goal would be to provide detail in such a way that would clear my fundamentals in prompt engineering. 

When you are ready, ask me for the topics!



Prompt Engineering
	- What is it by definition
	- Output length
		- Reducing the output length of the LLM doesn’t cause the LLM to become more stylistically or textually succinct in the output it creates, it just causes the LLM to stop predicting more 
		tokens once the limit is reached.
		-  generating more tokens requires more computation from the LLM, leading to higher energy consumption and potentially slower response times, which leads to 
		higher costs
	-  Sampling controls
		- Temperature, top-K, and top-P are the most common configuration settings that determine how predicted token probabilities are processed to choose a single output token.
		
	-  Temperature
		- Temperature controls the degree of randomness in token selection.
		- Lower temperatures are good for prompts that expect a more deterministic response.
		- Higher temperatures can lead to more diverse or unexpected results.
		- What happens when temperature is set to 0?
		- If two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0.
		- As temperature gets higher and higher, all tokens become equally likely to be the next predicted token.
		
	- Top-K and top-P
		- Also known as nucleus sampling
		- What is Top-K?
			- Purpose?
			- Usecase?
			- What problem it solves?
			- The higher top-K, the more creative and varied the model’s output
			- The lower top-K, the more restive and factual the model’s output.
			- A top-K of 1 is equivalent to greedy decoding
		- What is Top-P?
			- Purpose?
			- Usecase?
			- What problem it solves?
		- Like temperature, these sampling settings control the randomness and diversity of generated text.
		- The best way to choose between top-K and top-P is to experiment with both methods. Why and How?
		- If you set temperature to 0, top-K and top-P become irrelevant
		-  If you set temperature extremely high (above 1–generally into the 10s), temperature becomes irrelevant and whatever tokens make it through the top-K and/or top-P criteria
		are then randomly sampled to choose a next predicted token.
		- If you set top-K to 1, temperature and top-P become irrelevant.
		- If you set top-P to 0 (or a very small value), most LLM sampling implementations will then only consider the most probable token to meet the top-P criteria, making temperature and top-K irrelevant.
		- A temperature of .2, top-P of .95, and top-K of 30 will give you relatively coherent results that can be creative but not excessively so.
		-  If you want especially creative results, try starting with a temperature of .9, top-P of .99, and top-K of 40.
		- If you want less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20. 
		-  If your task always has a single correct answer (e.g., answering a math problem), start with a temperature of 0
		- What is repetition loop bug? Explain with a real case scenario.
			- At low temperatures, the model becomes overly deterministic, sticking rigidly to the highest probability path, which can lead to a loop if that path revisits previously generated text.
			- at high temperatures, the model's output becomes excessively random, increasing the probability that a randomly chosen word or phrase will, by chance, lead back to a prior state,
			creating a loop due to the vast number of available options.
		
	- Prompting techniques
		- zero shot
			- What is it?
			- How does it work?
			- What purpose it serve?
			- The name zero-shot stands for ’no examples’.
		- Create a sample table format structure for writting prompt applying the best practices and keeping it for future modifications and development.
		- When zero-shot doesn’t work, you can provide demonstrations or examples in the prompt, which leads to “one-shot” and “few-shot” prompting.
		- Examples are especially useful when you want to steer the model to a certain output structure or pattern.
		- A one-shot prompt, provides a single example
		- A few-shot prompt 7 provides multiple examples to the model. Multiple examples of the desired pattern increases the chance the model follows the pattern.
		- General rule of thumb, you should use at least three to five examples for few-shot prompting. Can be added more or less based on complex task and input token limit.
		- The examples should be diverse, of high quality, and well written. One small mistake can confuse the model and will result in undesired output.
		-  If you are trying to generate output that is robust to a variety of inputs, then it is important to include edge cases in your examples.
		
	-  System, contextual and role prompting
		- System prompting
			- Sets the overall context and purpose for the language model.
			- Defines the ‘big picture’ of what the model should be doing.
			- Defines the model’s fundamental capabilities and overarching purpose.
		- Contextual prompting
			- Provides specific details or background information relevant to the current conversation or task.
			- Helps the model to understand the nuances of what’s being asked and tailor the response accordingly.
			-  Provides immediate, task-specific information to guide the response. It’s highly specific to the current task or input, which is dynamic.
			- Help ensure that your AI interactions are as seamless and efficient as possible. Model will be able to more quickly understand your 
			request and be able to generate more accurate and relevant responses.
		- Role prompting
			- Assigns a specific character or identity for the language model to adopt.
			- Helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior.
			-  Frames the model’s output style and voice. It adds a layer of specificity and personality.
			-  Defining a role perspective for an AI model gives it a blueprint of the tone, style, and focused expertise you’re looking for to improve the quality, relevance, and effectiveness
			of your output.
			- Some styles for role prompting :
				-  Confrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, Inspirational, Persuasive.
		- There can be considerable overlap between system, contextual, and role prompting. Explain with real scenario.
		- Create a real case scenario and define all these three prompting for that scenario and explain them in detail.
		- Make a perfect system prompt using all the best practices for this goal -> Classify movie reviews as positive, neutral or negative, return JSON.
		- By prompting for a JSON format it forces the model to create a structure and limit hallucinations.
		- System prompts can also be really useful for safety and toxicity. To control the output, simply add an additional line to your prompt like: ‘You should be respectful in your answer.
		- Make a role prompt using all the best practices for this goal -> Act as travel guide and provide 3 travel suggestions.
			- Provide a different role to that above scenario with the best practices used and show how outputs can be changed based on role and style changes.
		- Make a context prompt using all the best practices for this goal ->  Suggest articles for a blog about retro games.
		
		
		
	- Step-back prompting
		- What it is ?
		- What is the concept of it?
		- What problem it focuses on?
		- Write a step back prompt following below rule 
			- First create a generic prompt scenario.
				- Goal -> Write a storyline for Ironman 4 (In Marvel Cinematic Universe)
			- Then create show how step back prompt can give us more advantage, and better results instead of that generic prompt for the same goal.
		- By using step back prompting techniques you can increase the accuracy of your prompts
		
		
	- Chain of Thought (CoT)
		- What it is ?
		- What is the concept of it?
		- What problem it focuses on?
		- Write a CoT Prompt using the best prompting practices with a real case scenario and compare it with a generic prompt like why it's better?
		- How we can add single shot with CoT or few shot with CoT which makes our prompt response way more better than only CoT?
		- It is a technique for improving the reasoning capabilities of LLMs by generating intermediate reasoning steps.
		- You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding 
		- Explain the best practices specific to CoT
			- Self-consistency
			- [Others if available]
			
			
	- Tree of Thoughts (ToT)
		- What it is ?
		- What is the concept of it?
		- What problem it focuses on?
		- It allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought.
		- How it differs from CoT?
		- ‘Large Language Model Guided Tree-of-Thought’ -> From this paper, explain the ToT in best way possible 
		- Write a ToT Prompt using the best prompting practices with a real case scenario and compare it with a generic prompt like why it's better?
		
	- ReAct (reason & act)
		- What it is ?
		- What is the concept of it?
		- What problem it focuses on?
		- It is a paradigm for enabling LLMs to solve complex tasks using natural language reasoning combined with external tools (search, code interpreter etc.).
		- ReAct mimics how humans operate in the real world, as we reason verbally and can take actions to gain information. 
		-  ReAct prompting works by combining reasoning and acting into a thought-action loop. The 
		LLM first reasons about the problem and generates a plan of action. It then performs the 
		actions in the plan and observes the results. The LLM then uses the observations to update 
		its reasoning and generate a new plan of action. This process continues until the LLM 
		reaches a solution to the problem
		- Write a ReAct Prompt using the best prompting practices with a real case scenario and compare it with a generic prompt like why it's better?
		
		
	- Automatic Prompt Engineering
		- What it is ?
		- What is the concept of it?
		- What problem it focuses on?
		- Write a prompt to write prompts
		- You will prompt a model to generate more prompts. Evaluate them, possibly alter the good ones. And repeat
		- What is BLEU
		- What is ROUGE
		- Explain the whole scenario of Automatic Prompt Engineering with solving a real case scenario with the best prompt practices.
		
		
		
	- Best Practices
		- Provide examples
			-  It’s like giving the model a reference point or target to aim for, improving the accuracy, style, and tone of its response to better match your expectations.
		- Design with simplicity
			- Prompts should be concise, clear, and easy to understand for both you and the model.
			- If it’s already confusing for you it will likely be also confusing for the model.
			- Try not to use complex language and don’t provide unnecessary information.
			-  Try using verbs that describe the action. Here’s a set of examples:
				Act, Analyze, Categorize, Classify, Contrast, Compare, Create, Describe, Define, 
				Evaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick, 
				Predict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort, 
				Summarize, Translate, Write
		- Be specific about the output
			-  Providing specific details in the prompt (through system or context prompting) can help the model to focus on what’s relevant, improving the overall accuracy.
		
		- Use Instructions over Constraints
			-  Growing research suggests that focusing on positive instructions in prompting can be more effective than relying heavily on constraints.
			This approach aligns with how humans prefer positive instructions over lists of what not to do.
			-  Instructions directly communicate the desired outcome, whereas constraints might leave the model guessing about what is allowed.
			It gives flexibility and encourages creativity within the defined boundaries, while constraints can limit the model’s potential. 
			Also a list of constraints can clash with each other.
			-  Constraints are still valuable but in certain situations. To prevent the model from generating harmful or biased content or when a strict output format or style is needed.
			If possible, use positive instructions: instead of telling the model what not to do, tell it what to do instead. 
			This can avoid confusion and improve the accuracy of the output.
			- start by prioritizing instructions, clearly stating what you want the model to do and only use constraints when necessary for safety, clarity or specific requirements. 
			Experiment and iterate to test different combinations of instructions and constraints to find what works best for your specific tasks, and document these.
			
		- Max token limit
			-  To control the length of a generated LLM response, you can either set a max token limit in the configuration or explicitly request a specific length in your prompt.
			
		- Use variables in prompts
			-  Variables can save you time and effort by allowing you to avoid repeating yourself. If you need to use the same piece of information in multiple prompts, 
			you can store it in a variable and then reference that variable in each prompt. This makes a lot of sense when integrating prompts into your own applications.
			
		-  For few-shot prompting with classification tasks, mix up the classes
			-  the order of your few-shots examples should not matter much. However, when doing classification tasks, make sure you mix up the possible response classes in the 
			few shot examples. This is because you might otherwise be overfitting to the specific order of the examples. By mixing up the possible response classes, you can ensure that 
			the model is learning to identify the key features of each class, rather than simply memorizing the order of the examples. 
			This will lead to more robust and generalizable performance on unseen data.
			-  A good rule of thumb is to start with 6 few shot examples and start testing the accuracy from there.
			
		-  Adapt to model updates
			
		-  Experiment with output formats
			-  For non creative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try having your output returned in a structured format like JSON or XML.
			- benefits of using JSON for your output:
				• Returns always in the same style
				• Focus on the data you want to receive
				• Less chance for hallucinations
				• Make it relationship aware
				• You get data types
				• You can sort it
		
		- JSON Repair
			- The structured nature of JSON, while beneficial for parsing and use in applications, requires significantly more tokens than plain text, leading to increased processing time and higher costs.
			- JSON's verbosity can easily consume the entire output window, becoming especially problematic when the generation is abruptly cut off due to token limits. 
			This truncation often results in invalid JSON, missing crucial closing braces or brackets, rendering the output unusable.
			- json-repair library can be invaluable in above situation.
			
		- Working with Schemas
			- A JSON Schema defines the expected structure and data types of your JSON input. By providing a schema, you give the LLM a clear blueprint of the data it should expect,
			helping it focus its attention on the relevant information and reducing the risk of misinterpreting the input. 
			- Schemas can help establish relationships between different pieces of data and even make the LLM "time-aware" by including date or timestamp fields with specific formats.
			-  By preprocessing your data and instead of providing full documents only providing both the schema and the data, you give the LLM a clear understanding of the product's 
			attributes, including its release date, making it much more likely to generate an accurate and relevant description. This structured input approach, guiding the LLM's attention 
			to the relevant fields, is especially valuable when working with large volumes of data or when integrating LLMs into complex applications.
			
		- Experiment together with other prompt engineers
		
		- CoT Best practices
			-  putting the answer after the reasoning is required because the generation of the reasoning changes the tokens that the model gets when it predicts the final answer
			- For CoT prompting, set the temperature to 0. Chain of thought prompting is based on greedy decoding, predicting the next word in a sequence based on the highest probability 
			assigned by the language model. Generally speaking, when using reasoning, to come up with the final answer, there’s likely one single correct answer. Therefore the temperature should always set to 0.
		
		- Document the various prompt attempts
			- document your prompt attempts in full detail so you can learn over time what went well and what did not.
			- Prompt outputs can differ across models, across sampling settings, and even across different versions of the same model. 
			- even across identical prompts to the same model, small differences in output sentence formatting and word choice can occur. 
			- As a prompt engineer you should rely on automated tests and evaluation procedures to understand how well your prompt generalizes to a task.
			- Prompt engineering is an iterative process. Craft and test different prompts, analyze, and document the results. Refine your prompt based on the model’s performance. Keep 
			experimenting until you achieve the desired output. When you change a model or model configuration, go back and keep experimenting with the previously used prompts.