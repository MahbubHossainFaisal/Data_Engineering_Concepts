{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173ce76e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß† First, Why Sampling Matters in Prompt Engineering?\n",
    "\n",
    "Imagine you're building a **smart writing assistant** for a screenwriter. You don‚Äôt want it to always write the same plot ‚Äî but you also don‚Äôt want it to go off-topic and add aliens to a historical drama.\n",
    "üëâ That‚Äôs where sampling techniques like **Top-K**, **Top-P**, and **temperature** help us **balance creativity and control**.\n",
    "\n",
    "---\n",
    "\n",
    "# üìö Section 1: Step-by-Step ‚Äî How Sampling Works Internally\n",
    "\n",
    "Let‚Äôs first understand the **sampling pipeline** (this happens every time the LLM generates a token):\n",
    "\n",
    "### üßÆ Step-by-Step Order in Token Sampling:\n",
    "\n",
    "1. **Model computes logits** (raw scores) for all possible next tokens\n",
    "   üëâ Like a probability heatmap over the vocabulary (\\~50,000 words).\n",
    "\n",
    "2. **Apply Top-K filtering** (optional)\n",
    "   üëâ Keep only the **top K tokens** with the highest scores.\n",
    "\n",
    "3. **Apply Top-P filtering** (optional)\n",
    "   üëâ From the **remaining tokens**, keep only the **smallest set** whose total probability ‚â• **P (e.g., 0.9)**.\n",
    "\n",
    "4. **Temperature Scaling**\n",
    "   üëâ Adjust sharpness of the probability distribution. Lower temp ‚Üí more confident/skewed; higher temp ‚Üí more random.\n",
    "\n",
    "5. **Softmax + Sampling**\n",
    "   üëâ Convert scores into actual probabilities. Randomly pick 1 token based on this distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Section 2: Understanding Top-K\n",
    "\n",
    "### üîπ What is Top-K Sampling?\n",
    "\n",
    "Top-K tells the model:\n",
    "üó£Ô∏è *‚ÄúOnly consider the **K most likely tokens**, and discard the rest. Then sample randomly from those.‚Äù*\n",
    "\n",
    "**Example:**\n",
    "Let‚Äôs say the model is predicting the next word after:\n",
    "**‚ÄúThe cat sat on the‚Äù**\n",
    "And the probability list is:\n",
    "\n",
    "| Token | Probability |\n",
    "| ----- | ----------- |\n",
    "| mat   | 0.35        |\n",
    "| bed   | 0.25        |\n",
    "| floor | 0.20        |\n",
    "| moon  | 0.08        |\n",
    "| chair | 0.07        |\n",
    "| lava  | 0.05        |\n",
    "\n",
    "If **Top-K = 3**, then it will only consider **mat, bed, floor**. Others (moon, chair, lava) are dropped completely.\n",
    "\n",
    "### ‚úÖ When is Top-K Better?\n",
    "\n",
    "* **üéØ Deterministic Outputs (Low K like 1-5)**\n",
    "  You want control and consistency.\n",
    "  E.g., **summarizing a legal contract**, **writing SQL code**.\n",
    "\n",
    "* **‚ö° Performance Optimization**\n",
    "  Lower K ‚Üí fewer options to score and sample from ‚Üí **faster inference**.\n",
    "  Great for **real-time systems** or **chatbots on low-end devices**.\n",
    "\n",
    "---\n",
    "\n",
    "## üåä Section 3: Understanding Top-P (a.k.a. Nucleus Sampling)\n",
    "\n",
    "### üîπ What is Top-P Sampling?\n",
    "\n",
    "Top-P says:\n",
    "üó£Ô∏è *‚ÄúKeep the **smallest set of tokens** whose **total probability adds up to P** (like 0.9 or 0.95), and discard the rest.‚Äù*\n",
    "\n",
    "It‚Äôs **dynamic**, not fixed like K.\n",
    "\n",
    "**Example:**\n",
    "Same sentence: ‚ÄúThe cat sat on the‚Äù\n",
    "\n",
    "| Token | Probability | Cumulative          |\n",
    "| ----- | ----------- | ------------------- |\n",
    "| mat   | 0.35        | 0.35                |\n",
    "| bed   | 0.25        | 0.60                |\n",
    "| floor | 0.20        | 0.80                |\n",
    "| moon  | 0.08        | 0.88                |\n",
    "| chair | 0.07        | 0.95 ‚¨ÖÔ∏è ‚Üê stop here |\n",
    "| lava  | 0.05        | 1.00                |\n",
    "\n",
    "So, Top-P = 0.95 ‚Üí picks first 5 tokens.\n",
    "It's **probability-aware**, which makes it **context-sensitive**.\n",
    "\n",
    "### ‚úÖ When is Top-P Better?\n",
    "\n",
    "* **üß† Creative, Diverse Outputs**\n",
    "  Great for **story generation**, **dialogue**, **music**, or **marketing copy**.\n",
    "\n",
    "* **üåÄ Contextual Adaptability**\n",
    "  Since it adapts to the shape of the probability distribution, it reacts better when some tokens are very dominant (e.g., in repetitive tasks).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Section 4: Key Differences Between Top-K vs Top-P\n",
    "\n",
    "| Aspect           | Top-K                         | Top-P                      |\n",
    "| ---------------- | ----------------------------- | -------------------------- |\n",
    "| Filtering Type   | Fixed number of tokens (K)    | Dynamic set by probability |\n",
    "| Flexibility      | Less flexible                 | More context-sensitive     |\n",
    "| Control Level    | High                          | Medium                     |\n",
    "| Output Diversity | Low (unless K is large)       | High                       |\n",
    "| Good For         | Code, Q\\&A, technical writing | Stories, conversation      |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Section 5: Why You **Shouldn‚Äôt Use Top-K & Top-P Together** (Usually)\n",
    "\n",
    "### ‚ùå Over-constraining\n",
    "\n",
    "Applying both can reduce too many tokens. Imagine:\n",
    "\n",
    "* **Top-K keeps 10 tokens**\n",
    "* **Top-P keeps only 4 from those**\n",
    "\n",
    "‚Üí You end up with a **tiny pool**, leading to unnatural, repetitive outputs.\n",
    "\n",
    "### ‚ùå Redundancy\n",
    "\n",
    "They‚Äôre both **filters** ‚Äî using both together is like:\n",
    "\n",
    "> ‚ÄúUse only the top 10 best students... but only those scoring 90%+ from them.‚Äù\n",
    "> You‚Äôve already filtered once!\n",
    "\n",
    "### ‚ùå Debugging Nightmare\n",
    "\n",
    "Hard to tell which setting is messing up the output.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Section 6: When You *Must* Use Both Together\n",
    "\n",
    "While **not recommended**, it can be done carefully.\n",
    "\n",
    "‚úÖ **Best Practice Combo:**\n",
    "\n",
    "* `top_k = 50`\n",
    "* `top_p = 0.9`\n",
    "\n",
    "üëâ This means: ‚ÄúStart with top 50 tokens, then from there, keep enough to add up to 90% probability.‚Äù\n",
    "\n",
    "### ‚ö†Ô∏è Be Careful:\n",
    "\n",
    "* **Make sure K is large enough**, so P has room to pick a variety.\n",
    "* **Always monitor outputs** for degradation or weirdness.\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ Section 7: Limitations of Top-K and Top-P\n",
    "\n",
    "| Limitation                             | Description                                                    |\n",
    "| -------------------------------------- | -------------------------------------------------------------- |\n",
    "| ‚úÖ Doesn‚Äôt guarantee quality            | It only limits the pool ‚Äî sampling can still pick a bad token. |\n",
    "| üìâ Not adaptive to long-term coherence | It controls **local randomness**, not long-range structure.    |\n",
    "| üîÑ Trial-and-error tuning              | Choosing the right values often needs experimentation.         |\n",
    "| üîç Poor for hallucination control      | They don‚Äôt prevent factual mistakes.                           |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Section 8: Practical Advice\n",
    "\n",
    "‚úÖ **Use One, Not Both**, unless you know what you're doing.\n",
    "‚úÖ For **code, summarization, Q\\&A**:\n",
    "‚Üí `top_k = 10` or even just use `top_k = 1` (argmax).\n",
    "‚úÖ For **stories, creative tasks**:\n",
    "‚Üí `top_p = 0.8‚Äì0.95`, maybe `temperature = 0.9`.\n",
    "‚úÖ For **consistent but not too repetitive** output:\n",
    "‚Üí `temperature = 0.7`, `top_k = 40` or `top_p = 0.9`.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Section 9: Story-Based Example (Real Use Case)\n",
    "\n",
    "### Scenario: You're building a writing assistant for fiction authors.\n",
    "\n",
    "#### ‚ùå If you use:\n",
    "\n",
    "* `top_k=5`, `top_p=0.7`, `temperature=0.5`\n",
    "  üëâ Output becomes repetitive: \"The knight went to the castle. The knight walked in. The knight said hello...\"\n",
    "\n",
    "#### ‚úÖ But with:\n",
    "\n",
    "* `top_p=0.92`, `temperature=0.9`\n",
    "  üëâ Output becomes imaginative:\n",
    "\n",
    "> \"The knight wandered into the moonlit ruins, clutching a forgotten relic. Shadows whispered secrets as he passed...\"\n",
    "\n",
    "#### üîÑ Now add Top-K=10 too?\n",
    "\n",
    "You might lose that poetic flow because fewer options survived filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Summary Cheatsheet\n",
    "\n",
    "| Setting       | Best Use Case              | Sample Values  |\n",
    "| ------------- | -------------------------- | -------------- |\n",
    "| `top_k`       | Code, Q\\&A, fixed logic    | 5‚Äì50           |\n",
    "| `top_p`       | Creative writing, dialogue | 0.85‚Äì0.95      |\n",
    "| `temperature` | All tasks (fine tuning)    | 0.2‚Äì1.0        |\n",
    "| Use Both?     | Not advised unless careful | If yes, `k>50` |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6a78a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß™ First: How Sampling Happens Internally in details?\n",
    "\n",
    "When **Top-K and Top-P are used together**, the **probabilities are *not reset or renormalized*** between Top-K and Top-P.\n",
    "\n",
    "Instead:\n",
    "\n",
    "1. **Model produces logits** ‚Üí these are raw scores for each token.\n",
    "\n",
    "2. Logits are turned into probabilities (via **softmax**).\n",
    "\n",
    "3. **Top-K filtering**:\n",
    "\n",
    "   * The model keeps only the **K highest-probability tokens**, *ignoring the rest* (sets them to -infinity or masks them out).\n",
    "   * So now, only K tokens remain **with their original probabilities**.\n",
    "\n",
    "4. **Top-P filtering** is then applied **on top of those remaining K tokens**.\n",
    "\n",
    "   * It **sorts those K tokens** (by probability).\n",
    "   * It picks the **smallest number of them** whose **cumulative sum ‚â• P** (e.g., 0.9).\n",
    "   * **It does NOT reset or renormalize probabilities** before doing this.\n",
    "\n",
    "üëâ Think of it as a **two-stage funnel**:\n",
    "\n",
    "```\n",
    "      All tokens\n",
    "         ‚Üì\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ   Top-K     ‚îÇ   ‚Üê Keeps K highest probs only\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ   Top-P     ‚îÇ   ‚Üê Chooses subset from these (based on original probs)\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "    Sampling happens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ So, to answer directly:\n",
    "\n",
    "> ‚ùì **When Top-K is applied first, are the token probabilities reset before Top-P applies?**\n",
    "\n",
    "**üü© No.**\n",
    "The probabilities are **not reset or renormalized**.\n",
    "Top-P uses the **original softmax probabilities** of the **Top-K set**, but just applies the cumulative cutoff (e.g., 90%).\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Example to Make It Clear\n",
    "\n",
    "### Let‚Äôs say the model predicts:\n",
    "\n",
    "| Token | Probability |\n",
    "| ----- | ----------- |\n",
    "| A     | 0.30        |\n",
    "| B     | 0.25        |\n",
    "| C     | 0.20        |\n",
    "| D     | 0.10        |\n",
    "| E     | 0.08        |\n",
    "| F     | 0.04        |\n",
    "| G     | 0.03        |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Apply `Top-K = 5`:\n",
    "\n",
    "We keep only tokens A, B, C, D, E.\n",
    "F and G are dropped.\n",
    "\n",
    "Remaining set:\n",
    "\n",
    "| Token | Probability |\n",
    "| ----- | ----------- |\n",
    "| A     | 0.30        |\n",
    "| B     | 0.25        |\n",
    "| C     | 0.20        |\n",
    "| D     | 0.10        |\n",
    "| E     | 0.08        |\n",
    "\n",
    "‚ö†Ô∏è **These probabilities are not renormalized.**\n",
    "The sum here is **0.93** (not 1.0), but that's fine ‚Äî we just proceed to Top-P.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Apply `Top-P = 0.9` on this Top-K result:\n",
    "\n",
    "Now from these 5, we sort by probability and cumulatively add:\n",
    "\n",
    "| Token | Probability | Cumulative                    |\n",
    "| ----- | ----------- | ----------------------------- |\n",
    "| A     | 0.30        | 0.30                          |\n",
    "| B     | 0.25        | 0.55                          |\n",
    "| C     | 0.20        | 0.75                          |\n",
    "| D     | 0.10        | 0.85                          |\n",
    "| E     | 0.08        | 0.93 ‚Üê ‚úÖ Top-P satisfied here |\n",
    "\n",
    "So Top-P includes all 5 in this case.\n",
    "If Top-P was 0.8, it would have stopped at C.\n",
    "\n",
    "Again: **Probabilities are not renormalized.**\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Why This Matters\n",
    "\n",
    "If probabilities **were reset**, the model would behave differently.\n",
    "It would lose the sense of the **relative importance** between tokens across the vocabulary.\n",
    "\n",
    "Keeping the original probabilities ensures that **Top-P still respects the model's belief** about what tokens make sense ‚Äî but only within the Top-K boundary.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Bonus Tip: If you set Top-K too small (like K=5), and Top-P too low (like P=0.7), then:\n",
    "\n",
    "* The **overlap between both filters** can get so narrow that **only 1‚Äì2 tokens survive**.\n",
    "* That‚Äôs when you get weird, robotic, or repetitive outputs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc594a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
