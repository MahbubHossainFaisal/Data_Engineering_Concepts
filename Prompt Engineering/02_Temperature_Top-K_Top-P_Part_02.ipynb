{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173ce76e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 First, Why Sampling Matters in Prompt Engineering?\n",
    "\n",
    "Imagine you're building a **smart writing assistant** for a screenwriter. You don’t want it to always write the same plot — but you also don’t want it to go off-topic and add aliens to a historical drama.\n",
    "👉 That’s where sampling techniques like **Top-K**, **Top-P**, and **temperature** help us **balance creativity and control**.\n",
    "\n",
    "---\n",
    "\n",
    "# 📚 Section 1: Step-by-Step — How Sampling Works Internally\n",
    "\n",
    "Let’s first understand the **sampling pipeline** (this happens every time the LLM generates a token):\n",
    "\n",
    "### 🧮 Step-by-Step Order in Token Sampling:\n",
    "\n",
    "1. **Model computes logits** (raw scores) for all possible next tokens\n",
    "   👉 Like a probability heatmap over the vocabulary (\\~50,000 words).\n",
    "\n",
    "2. **Apply Top-K filtering** (optional)\n",
    "   👉 Keep only the **top K tokens** with the highest scores.\n",
    "\n",
    "3. **Apply Top-P filtering** (optional)\n",
    "   👉 From the **remaining tokens**, keep only the **smallest set** whose total probability ≥ **P (e.g., 0.9)**.\n",
    "\n",
    "4. **Temperature Scaling**\n",
    "   👉 Adjust sharpness of the probability distribution. Lower temp → more confident/skewed; higher temp → more random.\n",
    "\n",
    "5. **Softmax + Sampling**\n",
    "   👉 Convert scores into actual probabilities. Randomly pick 1 token based on this distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Section 2: Understanding Top-K\n",
    "\n",
    "### 🔹 What is Top-K Sampling?\n",
    "\n",
    "Top-K tells the model:\n",
    "🗣️ *“Only consider the **K most likely tokens**, and discard the rest. Then sample randomly from those.”*\n",
    "\n",
    "**Example:**\n",
    "Let’s say the model is predicting the next word after:\n",
    "**“The cat sat on the”**\n",
    "And the probability list is:\n",
    "\n",
    "| Token | Probability |\n",
    "| ----- | ----------- |\n",
    "| mat   | 0.35        |\n",
    "| bed   | 0.25        |\n",
    "| floor | 0.20        |\n",
    "| moon  | 0.08        |\n",
    "| chair | 0.07        |\n",
    "| lava  | 0.05        |\n",
    "\n",
    "If **Top-K = 3**, then it will only consider **mat, bed, floor**. Others (moon, chair, lava) are dropped completely.\n",
    "\n",
    "### ✅ When is Top-K Better?\n",
    "\n",
    "* **🎯 Deterministic Outputs (Low K like 1-5)**\n",
    "  You want control and consistency.\n",
    "  E.g., **summarizing a legal contract**, **writing SQL code**.\n",
    "\n",
    "* **⚡ Performance Optimization**\n",
    "  Lower K → fewer options to score and sample from → **faster inference**.\n",
    "  Great for **real-time systems** or **chatbots on low-end devices**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌊 Section 3: Understanding Top-P (a.k.a. Nucleus Sampling)\n",
    "\n",
    "### 🔹 What is Top-P Sampling?\n",
    "\n",
    "Top-P says:\n",
    "🗣️ *“Keep the **smallest set of tokens** whose **total probability adds up to P** (like 0.9 or 0.95), and discard the rest.”*\n",
    "\n",
    "It’s **dynamic**, not fixed like K.\n",
    "\n",
    "**Example:**\n",
    "Same sentence: “The cat sat on the”\n",
    "\n",
    "| Token | Probability | Cumulative          |\n",
    "| ----- | ----------- | ------------------- |\n",
    "| mat   | 0.35        | 0.35                |\n",
    "| bed   | 0.25        | 0.60                |\n",
    "| floor | 0.20        | 0.80                |\n",
    "| moon  | 0.08        | 0.88                |\n",
    "| chair | 0.07        | 0.95 ⬅️ ← stop here |\n",
    "| lava  | 0.05        | 1.00                |\n",
    "\n",
    "So, Top-P = 0.95 → picks first 5 tokens.\n",
    "It's **probability-aware**, which makes it **context-sensitive**.\n",
    "\n",
    "### ✅ When is Top-P Better?\n",
    "\n",
    "* **🧠 Creative, Diverse Outputs**\n",
    "  Great for **story generation**, **dialogue**, **music**, or **marketing copy**.\n",
    "\n",
    "* **🌀 Contextual Adaptability**\n",
    "  Since it adapts to the shape of the probability distribution, it reacts better when some tokens are very dominant (e.g., in repetitive tasks).\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Section 4: Key Differences Between Top-K vs Top-P\n",
    "\n",
    "| Aspect           | Top-K                         | Top-P                      |\n",
    "| ---------------- | ----------------------------- | -------------------------- |\n",
    "| Filtering Type   | Fixed number of tokens (K)    | Dynamic set by probability |\n",
    "| Flexibility      | Less flexible                 | More context-sensitive     |\n",
    "| Control Level    | High                          | Medium                     |\n",
    "| Output Diversity | Low (unless K is large)       | High                       |\n",
    "| Good For         | Code, Q\\&A, technical writing | Stories, conversation      |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Section 5: Why You **Shouldn’t Use Top-K & Top-P Together** (Usually)\n",
    "\n",
    "### ❌ Over-constraining\n",
    "\n",
    "Applying both can reduce too many tokens. Imagine:\n",
    "\n",
    "* **Top-K keeps 10 tokens**\n",
    "* **Top-P keeps only 4 from those**\n",
    "\n",
    "→ You end up with a **tiny pool**, leading to unnatural, repetitive outputs.\n",
    "\n",
    "### ❌ Redundancy\n",
    "\n",
    "They’re both **filters** — using both together is like:\n",
    "\n",
    "> “Use only the top 10 best students... but only those scoring 90%+ from them.”\n",
    "> You’ve already filtered once!\n",
    "\n",
    "### ❌ Debugging Nightmare\n",
    "\n",
    "Hard to tell which setting is messing up the output.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Section 6: When You *Must* Use Both Together\n",
    "\n",
    "While **not recommended**, it can be done carefully.\n",
    "\n",
    "✅ **Best Practice Combo:**\n",
    "\n",
    "* `top_k = 50`\n",
    "* `top_p = 0.9`\n",
    "\n",
    "👉 This means: “Start with top 50 tokens, then from there, keep enough to add up to 90% probability.”\n",
    "\n",
    "### ⚠️ Be Careful:\n",
    "\n",
    "* **Make sure K is large enough**, so P has room to pick a variety.\n",
    "* **Always monitor outputs** for degradation or weirdness.\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 Section 7: Limitations of Top-K and Top-P\n",
    "\n",
    "| Limitation                             | Description                                                    |\n",
    "| -------------------------------------- | -------------------------------------------------------------- |\n",
    "| ✅ Doesn’t guarantee quality            | It only limits the pool — sampling can still pick a bad token. |\n",
    "| 📉 Not adaptive to long-term coherence | It controls **local randomness**, not long-range structure.    |\n",
    "| 🔄 Trial-and-error tuning              | Choosing the right values often needs experimentation.         |\n",
    "| 🔍 Poor for hallucination control      | They don’t prevent factual mistakes.                           |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Section 8: Practical Advice\n",
    "\n",
    "✅ **Use One, Not Both**, unless you know what you're doing.\n",
    "✅ For **code, summarization, Q\\&A**:\n",
    "→ `top_k = 10` or even just use `top_k = 1` (argmax).\n",
    "✅ For **stories, creative tasks**:\n",
    "→ `top_p = 0.8–0.95`, maybe `temperature = 0.9`.\n",
    "✅ For **consistent but not too repetitive** output:\n",
    "→ `temperature = 0.7`, `top_k = 40` or `top_p = 0.9`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 9: Story-Based Example (Real Use Case)\n",
    "\n",
    "### Scenario: You're building a writing assistant for fiction authors.\n",
    "\n",
    "#### ❌ If you use:\n",
    "\n",
    "* `top_k=5`, `top_p=0.7`, `temperature=0.5`\n",
    "  👉 Output becomes repetitive: \"The knight went to the castle. The knight walked in. The knight said hello...\"\n",
    "\n",
    "#### ✅ But with:\n",
    "\n",
    "* `top_p=0.92`, `temperature=0.9`\n",
    "  👉 Output becomes imaginative:\n",
    "\n",
    "> \"The knight wandered into the moonlit ruins, clutching a forgotten relic. Shadows whispered secrets as he passed...\"\n",
    "\n",
    "#### 🔄 Now add Top-K=10 too?\n",
    "\n",
    "You might lose that poetic flow because fewer options survived filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Summary Cheatsheet\n",
    "\n",
    "| Setting       | Best Use Case              | Sample Values  |\n",
    "| ------------- | -------------------------- | -------------- |\n",
    "| `top_k`       | Code, Q\\&A, fixed logic    | 5–50           |\n",
    "| `top_p`       | Creative writing, dialogue | 0.85–0.95      |\n",
    "| `temperature` | All tasks (fine tuning)    | 0.2–1.0        |\n",
    "| Use Both?     | Not advised unless careful | If yes, `k>50` |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6a78a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🧪 First: How Sampling Happens Internally in details?\n",
    "\n",
    "When **Top-K and Top-P are used together**, the **probabilities are *not reset or renormalized*** between Top-K and Top-P.\n",
    "\n",
    "Instead:\n",
    "\n",
    "1. **Model produces logits** → these are raw scores for each token.\n",
    "\n",
    "2. Logits are turned into probabilities (via **softmax**).\n",
    "\n",
    "3. **Top-K filtering**:\n",
    "\n",
    "   * The model keeps only the **K highest-probability tokens**, *ignoring the rest* (sets them to -infinity or masks them out).\n",
    "   * So now, only K tokens remain **with their original probabilities**.\n",
    "\n",
    "4. **Top-P filtering** is then applied **on top of those remaining K tokens**.\n",
    "\n",
    "   * It **sorts those K tokens** (by probability).\n",
    "   * It picks the **smallest number of them** whose **cumulative sum ≥ P** (e.g., 0.9).\n",
    "   * **It does NOT reset or renormalize probabilities** before doing this.\n",
    "\n",
    "👉 Think of it as a **two-stage funnel**:\n",
    "\n",
    "```\n",
    "      All tokens\n",
    "         ↓\n",
    "   ┌─────────────┐\n",
    "   │   Top-K     │   ← Keeps K highest probs only\n",
    "   └─────────────┘\n",
    "         ↓\n",
    "   ┌─────────────┐\n",
    "   │   Top-P     │   ← Chooses subset from these (based on original probs)\n",
    "   └─────────────┘\n",
    "         ↓\n",
    "    Sampling happens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ So, to answer directly:\n",
    "\n",
    "> ❓ **When Top-K is applied first, are the token probabilities reset before Top-P applies?**\n",
    "\n",
    "**🟩 No.**\n",
    "The probabilities are **not reset or renormalized**.\n",
    "Top-P uses the **original softmax probabilities** of the **Top-K set**, but just applies the cumulative cutoff (e.g., 90%).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Example to Make It Clear\n",
    "\n",
    "### Let’s say the model predicts:\n",
    "\n",
    "| Token | Probability |\n",
    "| ----- | ----------- |\n",
    "| A     | 0.30        |\n",
    "| B     | 0.25        |\n",
    "| C     | 0.20        |\n",
    "| D     | 0.10        |\n",
    "| E     | 0.08        |\n",
    "| F     | 0.04        |\n",
    "| G     | 0.03        |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Apply `Top-K = 5`:\n",
    "\n",
    "We keep only tokens A, B, C, D, E.\n",
    "F and G are dropped.\n",
    "\n",
    "Remaining set:\n",
    "\n",
    "| Token | Probability |\n",
    "| ----- | ----------- |\n",
    "| A     | 0.30        |\n",
    "| B     | 0.25        |\n",
    "| C     | 0.20        |\n",
    "| D     | 0.10        |\n",
    "| E     | 0.08        |\n",
    "\n",
    "⚠️ **These probabilities are not renormalized.**\n",
    "The sum here is **0.93** (not 1.0), but that's fine — we just proceed to Top-P.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Apply `Top-P = 0.9` on this Top-K result:\n",
    "\n",
    "Now from these 5, we sort by probability and cumulatively add:\n",
    "\n",
    "| Token | Probability | Cumulative                    |\n",
    "| ----- | ----------- | ----------------------------- |\n",
    "| A     | 0.30        | 0.30                          |\n",
    "| B     | 0.25        | 0.55                          |\n",
    "| C     | 0.20        | 0.75                          |\n",
    "| D     | 0.10        | 0.85                          |\n",
    "| E     | 0.08        | 0.93 ← ✅ Top-P satisfied here |\n",
    "\n",
    "So Top-P includes all 5 in this case.\n",
    "If Top-P was 0.8, it would have stopped at C.\n",
    "\n",
    "Again: **Probabilities are not renormalized.**\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 Why This Matters\n",
    "\n",
    "If probabilities **were reset**, the model would behave differently.\n",
    "It would lose the sense of the **relative importance** between tokens across the vocabulary.\n",
    "\n",
    "Keeping the original probabilities ensures that **Top-P still respects the model's belief** about what tokens make sense — but only within the Top-K boundary.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Bonus Tip: If you set Top-K too small (like K=5), and Top-P too low (like P=0.7), then:\n",
    "\n",
    "* The **overlap between both filters** can get so narrow that **only 1–2 tokens survive**.\n",
    "* That’s when you get weird, robotic, or repetitive outputs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc594a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
