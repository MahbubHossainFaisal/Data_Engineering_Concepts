{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c5b1989",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. What Is Automatic Prompt Engineering?\n",
    "\n",
    "### ‚úÖ Definition:\n",
    "\n",
    "> **Automatic Prompt Engineering (APE)** is the process of using LLMs to **automatically generate, test, evaluate, and refine prompts** ‚Äî with the goal of discovering the most effective prompts for a specific task.\n",
    "\n",
    "It turns **prompt design into an optimization problem**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. What‚Äôs the Core Concept?\n",
    "\n",
    "> Rather than handcrafting every prompt manually, you instruct the model to:\n",
    "\n",
    "* **Generate prompt candidates**\n",
    "* **Try them**\n",
    "* **Score them**\n",
    "* **Pick/refine the best**\n",
    "* **Repeat the process**\n",
    "\n",
    "This is especially useful when:\n",
    "\n",
    "* You're unsure of the best prompt style\n",
    "* You want to optimize for accuracy, creativity, or speed\n",
    "* You're scaling GenAI apps that must auto-improve\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. What Problem Does APE Solve?\n",
    "\n",
    "| Problem                                              | APE Solution                                 |\n",
    "| ---------------------------------------------------- | -------------------------------------------- |\n",
    "| ‚ùå Manually writing and testing prompts is slow       | ‚úÖ Automate this process                      |\n",
    "| ‚ùå Human-written prompts may not explore full variety | ‚úÖ LLMs can generate diverse variants         |\n",
    "| ‚ùå No way to choose best prompt automatically         | ‚úÖ Add automatic scoring via metrics or evals |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 4. How to Prompt a Model to Write Prompts (Yes, Really)\n",
    "\n",
    "Here‚Äôs your **meta-prompt** ‚Äî a prompt to generate more prompts:\n",
    "\n",
    "```txt\n",
    "You are an expert prompt engineer. Your task is to generate 5 diverse and effective prompts for the following use case:\n",
    "\n",
    "Use Case: Extract key takeaways from product reviews and return them in bullet point format.\n",
    "\n",
    "Each prompt should:\n",
    "- Be clear and concise\n",
    "- Try a different phrasing or approach\n",
    "- Ask for bullet-pointed output\n",
    "- Include style (formal/casual/concise) if relevant\n",
    "\n",
    "Return them in a numbered list.\n",
    "```\n",
    "\n",
    "‚úÖ Result: LLM gives you 5 candidate prompts. Now, you evaluate them.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ 5. The APE Iterative Loop\n",
    "\n",
    "Here‚Äôs how **automatic prompt engineering works in practice**:\n",
    "\n",
    "```\n",
    "             +--------------------+\n",
    "             |  Initial Use Case  |\n",
    "             +--------------------+\n",
    "                        ‚Üì\n",
    "           +------------------------+\n",
    "           |  Generate Prompt Variants |\n",
    "           +------------------------+\n",
    "                        ‚Üì\n",
    "          +---------------------------+\n",
    "          |  Run Each Prompt on Samples |\n",
    "          +---------------------------+\n",
    "                        ‚Üì\n",
    "            +----------------------+\n",
    "            |  Score Output (BLEU, ROUGE, GPT Judge) |\n",
    "            +----------------------+\n",
    "                        ‚Üì\n",
    "            +-------------------+\n",
    "            |  Pick Top Prompts |\n",
    "            +-------------------+\n",
    "                        ‚Üì\n",
    "            +------------------+\n",
    "            |  Refine & Repeat |\n",
    "            +------------------+\n",
    "```\n",
    "\n",
    "You can run this loop manually or **automate it using LangChain + GPT-based evals**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 6. BLEU and ROUGE (Evaluation Metrics)\n",
    "\n",
    "### ‚úÖ BLEU (Bilingual Evaluation Understudy):\n",
    "\n",
    "* Measures **how similar** the generated output is to a reference text\n",
    "* Based on **n-gram precision**\n",
    "* Common in machine translation\n",
    "\n",
    "```txt\n",
    "If you expect a **known reference answer**, BLEU helps check how close the model's output is.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ ROUGE (Recall-Oriented Understudy for Gisting Evaluation):\n",
    "\n",
    "* Measures **how much of the reference is covered** in the generated output\n",
    "* Based on **n-gram recall and overlap**\n",
    "* Common in summarization\n",
    "\n",
    "```txt\n",
    "Great for checking how much **content from the target answer** was preserved in the generation.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 7. Case Study: Real APE in Action\n",
    "\n",
    "### üéØ Task: Write the best prompt to classify tweets as ‚ÄúPositive‚Äù, ‚ÄúNegative‚Äù or ‚ÄúNeutral‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step 1: Meta-Prompt ‚Äî Generate Prompts\n",
    "\n",
    "```txt\n",
    "You are a professional prompt engineer. Generate 5 diverse prompt variants for the task:\n",
    "\n",
    "Task: Classify the sentiment of a tweet as Positive, Neutral, or Negative.\n",
    "\n",
    "Return the prompts in numbered format. Each should:\n",
    "- Be short\n",
    "- Ask for only the label\n",
    "- Be stylistically different\n",
    "```\n",
    "\n",
    "‚úÖ Sample Output:\n",
    "\n",
    "1. \"Classify the sentiment of this tweet (Positive/Neutral/Negative): \\[Tweet]\"\n",
    "2. \"How would you describe the mood of this tweet?\"\n",
    "3. \"Analyze the tweet and assign it a sentiment category.\"\n",
    "4. \"Given this tweet, return one of these: Positive, Neutral, or Negative.\"\n",
    "5. \"Decide whether the following tweet expresses positivity, negativity, or neutrality.\"\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step 2: Test Prompts on Sample Tweets\n",
    "\n",
    "| Tweet                           | Prompt #1 Output | Prompt #2 Output | ‚Ä¶ |\n",
    "| ------------------------------- | ---------------- | ---------------- | - |\n",
    "| ‚ÄúThis product is amazing!‚Äù      | Positive         | Positive         | ‚Ä¶ |\n",
    "| ‚ÄúNot bad, but could be better.‚Äù | Neutral          | Neutral          | ‚Ä¶ |\n",
    "| ‚ÄúWorst service ever.‚Äù           | Negative         | Negative         | ‚Ä¶ |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step 3: Score with GPT + ROUGE\n",
    "\n",
    "Use LangChain or your own logic to:\n",
    "\n",
    "* Compare generated outputs to expected labels\n",
    "* Use scoring metric (e.g., **Exact Match**, **ROUGE**, or **LLM judge**)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step 4: Pick Top Prompts\n",
    "\n",
    "Suppose Prompt #1 and #4 outperform. You:\n",
    "\n",
    "* Keep them\n",
    "* Optionally **combine** or **refine them** to get:\n",
    "\n",
    "```txt\n",
    "Classify this tweet sentiment. Answer in one word: Positive, Neutral, or Negative. No explanation.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step 5: Automate Loop (Optional)\n",
    "\n",
    "Using LangChain, AutoGPT, or custom script:\n",
    "\n",
    "* Prompt ‚Üí Generate candidates\n",
    "* LLM ‚Üí Execute on samples\n",
    "* Evaluator ‚Üí Score\n",
    "* Refiner ‚Üí Keep top ones, mutate others\n",
    "* Repeat\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Final Prompt Template for APE Workflows\n",
    "\n",
    "```txt\n",
    "You are a prompt generator and evaluator.\n",
    "\n",
    "Task: [Insert use case]\n",
    "1. Generate [N] diverse prompt variants for this task\n",
    "2. Run each prompt on the sample inputs\n",
    "3. Compare outputs to reference answers\n",
    "4. Score using exact match, BLEU or ROUGE\n",
    "5. Select top [K] prompts and return them\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† When to Use APE\n",
    "\n",
    "| Use Case                        | APE?                     |\n",
    "| ------------------------------- | ------------------------ |\n",
    "| Scaling GenAI workflows         | ‚úÖ‚úÖ‚úÖ                      |\n",
    "| Optimizing for precision/recall | ‚úÖ‚úÖ‚úÖ                      |\n",
    "| No idea what prompt works best  | ‚úÖ‚úÖ‚úÖ                      |\n",
    "| One-off creative tasks          | ‚ùå                        |\n",
    "| Human tone & branding required  | ‚úÖ (but human review too) |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb915ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
