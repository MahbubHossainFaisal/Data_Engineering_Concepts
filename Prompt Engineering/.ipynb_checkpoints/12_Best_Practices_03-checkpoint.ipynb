{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1328aeae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 1. Max Token Limit\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Fundamental Principle:\n",
    "\n",
    "> ‚ÄúTokens‚Äù are how LLMs measure input and output. **One token ‚âà 4 characters of English text**, or roughly **¬æ of a word**.\n",
    "\n",
    "If you don‚Äôt **control** token length:\n",
    "\n",
    "* You may run out of context window.\n",
    "* The output may become too long or get cut off mid-sentence.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Two Ways to Control Output Length:\n",
    "\n",
    "#### ‚úÖ Option 1: **Explicit Prompt Instructions**\n",
    "\n",
    "You ask the model to restrict length:\n",
    "\n",
    "```\n",
    "Summarize this article in under 100 words.\n",
    "```\n",
    "\n",
    "Or:\n",
    "\n",
    "```\n",
    "Answer in a single sentence. Max: 15 words.\n",
    "```\n",
    "\n",
    "This is **soft guidance** ‚Äî the model might not always obey, but it helps shape output.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Option 2: **Configuration Parameter (`max_tokens`)**\n",
    "\n",
    "Used in code when you deploy LLMs via API (like OpenAI/Anthropic):\n",
    "\n",
    "```python\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[...],\n",
    "  max_tokens=150\n",
    ")\n",
    "```\n",
    "\n",
    "* This is **hard-capped**. The output gets cut off if the limit is reached.\n",
    "* Always useful to prevent runaway generations and control costs.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Best Practices:\n",
    "\n",
    "| When                       | Use                                       |\n",
    "| -------------------------- | ----------------------------------------- |\n",
    "| You want **tight control** | Set `max_tokens`                          |\n",
    "| You want **soft control**  | Add length guidance in prompt             |\n",
    "| For UI/UX constraints      | Limit summary to fit on screen            |\n",
    "| For pipelines              | Avoid memory overload in downstream steps |\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Teaching Analogy:\n",
    "\n",
    "> Giving an LLM a max token is like saying: \"You only have 30 seconds to answer. Be concise.\"\n",
    "> It helps the model ‚Äúbudget‚Äù its words wisely.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ 2. Use Variables in Prompts\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Fundamental Principle:\n",
    "\n",
    "> Variables make prompts **modular**, **scalable**, and **reusable** ‚Äî especially in real-world LLM applications.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What‚Äôs a Variable in a Prompt?\n",
    "\n",
    "It‚Äôs a **placeholder** for dynamic content:\n",
    "\n",
    "```plaintext\n",
    "\"Translate the following text to French: {input_text}\"\n",
    "```\n",
    "\n",
    "At runtime, `{input_text}` can be filled with:\n",
    "\n",
    "* A user message\n",
    "* A document\n",
    "* A product description\n",
    "* Etc.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Real-World Usage Example (Python + Prompt Template):\n",
    "\n",
    "```python\n",
    "from string import Template\n",
    "\n",
    "template = Template(\"Write a tweet about $topic in a humorous tone.\")\n",
    "prompt = template.substitute(topic=\"artificial intelligence and cats\")\n",
    "```\n",
    "\n",
    "üîÅ Resulting prompt:\n",
    "\n",
    "```\n",
    "Write a tweet about artificial intelligence and cats in a humorous tone.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Why This Matters in Production\n",
    "\n",
    "Let‚Äôs say you're building a customer support bot. Instead of hardcoding 100 prompts:\n",
    "\n",
    "```plaintext\n",
    "\"Respond to complaint about billing issue.\"\n",
    "\"Respond to complaint about login issue.\"\n",
    "```\n",
    "\n",
    "You write:\n",
    "\n",
    "```plaintext\n",
    "\"Respond to a complaint about {issue_type}.\"\n",
    "```\n",
    "\n",
    "‚úÖ Scalable\n",
    "‚úÖ Easier to manage\n",
    "‚úÖ Avoids redundancy\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Prompt Templating Tools to Explore:\n",
    "\n",
    "* **LangChain PromptTemplate**\n",
    "* **Jinja2** (web templating)\n",
    "* **PromptLayer / LangSmith** (for managing prompt versions & variables)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 3. Few-Shot Classification ‚Üí Mix Up the Classes\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Fundamental Principle:\n",
    "\n",
    "> In classification prompts, **mix up class order** in your few-shot examples to avoid **position bias**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why This Happens:\n",
    "\n",
    "LLMs are **sequence-sensitive**. If all examples follow this order:\n",
    "\n",
    "```\n",
    "Text: \"X\" ‚Üí Class A  \n",
    "Text: \"Y\" ‚Üí Class B  \n",
    "Text: \"Z\" ‚Üí Class A  \n",
    "```\n",
    "\n",
    "The model may **bias toward \"Class A\" being more likely** just because it appears first or more frequently.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Solution: Shuffle Examples by Class\n",
    "\n",
    "Mix examples randomly across:\n",
    "\n",
    "* Position\n",
    "* Sentence structure\n",
    "* Class ordering\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Real-World Example:\n",
    "\n",
    "#### ‚ùå BAD Prompt:\n",
    "\n",
    "```\n",
    "Classify sentiment:\n",
    "- \"I love this product!\" ‚Üí Positive\n",
    "- \"This is okay.\" ‚Üí Neutral\n",
    "- \"This is terrible.\" ‚Üí Negative\n",
    "- \"The colors are great!\" ‚Üí Positive\n",
    "- \"Meh.\" ‚Üí Neutral\n",
    "- \"Awful experience.\" ‚Üí Negative\n",
    "```\n",
    "\n",
    "Here, class order is repeated ‚Äî might cause bias.\n",
    "\n",
    "#### ‚úÖ GOOD Prompt:\n",
    "\n",
    "```\n",
    "Classify sentiment:\n",
    "- \"Awful experience.\" ‚Üí Negative\n",
    "- \"This is okay.\" ‚Üí Neutral\n",
    "- \"The colors are great!\" ‚Üí Positive\n",
    "- \"Meh.\" ‚Üí Neutral\n",
    "- \"I love this product!\" ‚Üí Positive\n",
    "- \"This is terrible.\" ‚Üí Negative\n",
    "```\n",
    "\n",
    "Now the model learns **based on features**, not **position**.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Pro Tip:\n",
    "\n",
    "Start with **6 examples**, then test performance.\n",
    "You may optimize further via **A/B testing or prompt tuning**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ 4. Adapt to Model Updates\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Fundamental Principle:\n",
    "\n",
    "> LLMs (like GPT-4, Claude, etc.) **evolve over time**. What works perfectly today might **slightly degrade** tomorrow after an update.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What Changes During Model Updates?\n",
    "\n",
    "* Instruction following might improve (or change format)\n",
    "* Token efficiency improves (changes generation behavior)\n",
    "* Safety alignment may increase\n",
    "* Certain phrasing might work better or worse\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What You Should Do:\n",
    "\n",
    "| Action                           | Reason                             |\n",
    "| -------------------------------- | ---------------------------------- |\n",
    "| **Re-test prompts periodically** | To catch regressions or shifts     |\n",
    "| **Version your prompts**         | Like code. Store working versions. |\n",
    "| **Use Prompt Management tools**  | LangSmith, PromptLayer             |\n",
    "| **Log model version + outcome**  | Helps debugging and tracking       |\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Example Prompt Versioning:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"prompt_version\": \"v2.1\",\n",
    "  \"model\": \"gpt-4-0613\",\n",
    "  \"prompt_template\": \"Summarize this in 3 bullets: {text}\",\n",
    "  \"output\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "üß™ This way, if things break or change after a model update, you can trace it easily.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary: Key Takeaways\n",
    "\n",
    "| Concept                    | Why It Matters                | What To Do                         |\n",
    "| -------------------------- | ----------------------------- | ---------------------------------- |\n",
    "| **Max Token Limit**        | Controls response length/cost | Set in config or prompt            |\n",
    "| **Use Variables**          | Makes prompts modular         | Use `{placeholders}`               |\n",
    "| **Mix Few-Shot Classes**   | Prevents class bias           | Shuffle examples                   |\n",
    "| **Adapt to Model Updates** | LLMs evolve                   | Re-test, version, log, and iterate |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c97b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
