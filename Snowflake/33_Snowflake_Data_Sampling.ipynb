{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa6e86a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Why sampling? (the real-world story)\n",
    "\n",
    "Imagine you lead the data platform for an ecommerce company, **ShopFast**. The events table `events` stores clickstream and purchase events — it’s 3 TB and grows fast. Your product manager wants an exploratory dashboard showing daily conversion rate trends for the last 90 days while the ML team needs a quick training sample for experimentation.\n",
    "\n",
    "Problems:\n",
    "\n",
    "* Running full scans on the 3 TB table for quick EDA or dashboard prototypes is slow and expensive.\n",
    "* You need **fast, \"good enough\" answers** for visualization and experimentation — not always 100% exact.\n",
    "\n",
    "**Solution idea**: take a carefully chosen subset (sample) of the large table and run queries on that subset. Sampling gives:\n",
    "\n",
    "* Faster query response time (smaller data scanned).\n",
    "* Lower warehouse credits used.\n",
    "* A way to bootstrap models and visualizations quickly.\n",
    "\n",
    "But sampling trades exactness for speed — so we must understand *how* Snowflake samples and which sampling algorithm fits which use-case. ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Approximate Query Processing (AQP) — concept + Snowflake affordances\n",
    "\n",
    "**AQP** is the family of techniques that return *approximate* answers far faster and cheaper than exact computation. Two common patterns in Snowflake:\n",
    "\n",
    "1. Use **sampling** (TABLESAMPLE / SAMPLE) to run queries on a subset and extrapolate results.\n",
    "2. Use **built-in approximate functions** (e.g., `APPROX_COUNT_DISTINCT`, `APPROX_PERCENTILE`) which implement probabilistic algorithms (HyperLogLog, t-Digest, etc.) and are optimized for speed/low memory.\n",
    "\n",
    "When to prefer built-in approximate functions:\n",
    "\n",
    "* You need an approximate aggregate (distinct counts, percentiles) — use `APPROX_*` functions (they’re robust and often preferable to manual sampling). ([Snowflake Docs][2])\n",
    "\n",
    "When to prefer sampling:\n",
    "\n",
    "* You need to run arbitrary complex queries (GROUP BY, ML feature extraction, quick visualizations) and are willing to accept small error bounds in exchange for speed.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Snowflake sampling in practice — syntax & demo SQL\n",
    "\n",
    "Snowflake supports `SAMPLE` and `TABLESAMPLE` (synonymous). Two major sampling methods: `BERNOULLI | ROW` (row-based) and `SYSTEM | BLOCK` (block-based). You can also request fixed-size samples (`N ROWS`) and supply a repeatable `SEED` for deterministic samples (only supported for `SYSTEM`/`BLOCK`). ([Snowflake Docs][1])\n",
    "\n",
    "### Basic examples\n",
    "\n",
    "Fraction-based Bernoulli (default):\n",
    "\n",
    "```sql\n",
    "-- ~10% of rows (row-based / Bernoulli)\n",
    "SELECT * FROM events SAMPLE (10);\n",
    "-- equivalent\n",
    "SELECT * FROM events TABLESAMPLE BERNOULLI (10);\n",
    "```\n",
    "\n",
    "Fraction-based block/system with seed:\n",
    "\n",
    "```sql\n",
    "-- ~3% of blocks, repeatable sample with seed 82\n",
    "SELECT * FROM events SAMPLE SYSTEM (3) SEED (82);\n",
    "-- or\n",
    "SELECT * FROM events SAMPLE BLOCK (0.012) REPEATABLE (99992);\n",
    "```\n",
    "\n",
    "Fixed-size:\n",
    "\n",
    "```sql\n",
    "-- exact 100 rows (unless table has fewer)\n",
    "SELECT * FROM events SAMPLE (100 ROWS);\n",
    "```\n",
    "\n",
    "Repeatable/deterministic notes:\n",
    "\n",
    "* `SEED(...)` / `REPEATABLE(...)` makes a `SYSTEM` sample deterministic **for the same unchanged table**. It is not supported for `BERNOULLI` seeds and not supported on views/subqueries. ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 4) How the two sampling methods *work* — intuition + story\n",
    "\n",
    "## A) BERNOULLI / ROW (row-based sampling)\n",
    "\n",
    "**Mechanics (simple)**: imagine flipping a weighted coin for **every row**. Each row is independently included with probability `p/100`. So expected sample size ≈ `p/100 * n`. Because it's per-row randomness, this method yields an unbiased random subset of rows (statistically closest to true random sampling).\n",
    "\n",
    "**When it matters**:\n",
    "\n",
    "* Good for statistical sampling where each row should have an independent inclusion chance.\n",
    "* Useful when data distribution within blocks matters (BERNOULLI won't bias toward specific micro-partitions).\n",
    "\n",
    "**Drawbacks**:\n",
    "\n",
    "* More CPU work because Snowflake must decide inclusion per row across many micro-partitions — can be slower/scan more micro-partition metadata than SYSTEM. For very large tables, overhead is acceptable but still higher than SYSTEM. ([Snowflake Docs][1])\n",
    "\n",
    "**Story**: You want a truly random sample of `events` so that your conversion-rate estimator is unbiased. Use `SAMPLE (5)` (BERNOULLI default) — each event has independent 5% chance to be picked.\n",
    "\n",
    "## B) SYSTEM / BLOCK (block-based sampling)\n",
    "\n",
    "**Mechanics (simple)**: Snowflake flips the coin per **block / micro-partition** (think: choose whole micro-partitions with probability `p/100`). Micro-partitions contain contiguous rows and column statistics.\n",
    "\n",
    "**When it matters**:\n",
    "\n",
    "* SYSTEM is **often much faster** because it works at micro-partition granularity and can avoid decoding lots of rows; great when you need speed and are okay with slight block-level bias. Snowflake documentation specifically notes SYSTEM/BLOCK is often faster than BERNOULLI. ([Snowflake Docs][1])\n",
    "\n",
    "**Drawbacks**:\n",
    "\n",
    "* Potential bias: if your data is ordered (e.g., time-sorted) or micro-partitions cluster similar values, SYSTEM sampling can under- or over-represent certain values (biased sample), especially for **small tables** or when sampling percentages are tiny.\n",
    "* For tiny tables, block-level granularity makes the sample less representative.\n",
    "\n",
    "**Story**: You’re building a dashboard where speed trumps tiny bias. For a humongous `events` table, do `SAMPLE SYSTEM (2)` to get an approximate view fast — you’ll get quick results, but verify if micro-partition layout could bias results (e.g., if all failed payments are in a small subset of micro-partitions).\n",
    "\n",
    "---\n",
    "\n",
    "# 5) When to use which? Practical guidance\n",
    "\n",
    "* **Exploratory analysis / dashboards** where speed is critical and slight bias is acceptable → use `SYSTEM` (block) sampling. Optionally add `SEED` for reproducibility of dashboard preview data.\n",
    "* **Statistical experiments, sampling for model training, or when unbiasedness is required** → use `BERNOULLI`/`ROW` sampling.\n",
    "* **If you need a fixed number of rows** (exact N) → use `SAMPLE (N ROWS)`, but note `SYSTEM` + fixed-size is **not supported**. Fixed-size sampling may prevent some optimizations and can be slower. ([Snowflake Docs][1])\n",
    "* **If consistent sample between runs is needed for reproducible experiments** → use `SYSTEM` sampling with `SEED(...)`. (Note: `SEED` is only supported for SYSTEM/BLOCK sampling and not for ROW/Bernoulli; and sampling on copy may differ.) ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Advantages & disadvantages (quick table)\n",
    "\n",
    "* BERNOULLI / ROW\n",
    "\n",
    "  * Advantage: unbiased per-row sampling; works well for joins if no seed used.\n",
    "  * Disadvantage: slower than SYSTEM; more expensive for huge tables.\n",
    "\n",
    "* SYSTEM / BLOCK\n",
    "\n",
    "  * Advantage: faster, often cheaper because it selects entire micro-partitions.\n",
    "  * Disadvantage: possible sampling bias due to micro-partitioning; `SEED` only supported for block sampling; can't do fixed-size + seed.\n",
    "\n",
    "* General trade-offs:\n",
    "\n",
    "  * Sampling reduces scanned data (cost) but introduces sampling variance. Choose method by error tolerance + performance need. ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Important semantics & gotchas (from the docs + experience)\n",
    "\n",
    "* **Sampling after a JOIN**: sampling on the result of a `JOIN` is allowed only when the sampling is row-based (BERNOULLI) and **no seed** is used. Also, if you apply SAMPLE to tables in a join separately, the sample is applied to each table before joining — it does *not* reduce join cost unless sampling is applied before the join as part of the plan or you sample a subquery result. In some cases sampling is done after join processing — so it might not reduce join compute cost. Always test. ([Snowflake Docs][1])\n",
    "* **Determinism**: If you specify the same `SEED` and the table hasn't changed, `SYSTEM` samples are repeatable. But a copy/clone of the table might produce different sample even with same seed because micro-partitions/state may differ. ([Snowflake Docs][1])\n",
    "* **Fixed-size sampling**: returns exact requested rows (if table larger), but **SYSTEM** and `SEED` aren’t supported with fixed-size sampling. Fixed-size can prevent optimizations and be slower. ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Demo: full practical walkthrough (examples you can run)\n",
    "\n",
    "Assume a table `prod.events` with columns `(event_ts, user_id, event_type, amount)`.\n",
    "\n",
    "### 1) Quick dashboard preview (fast)\n",
    "\n",
    "```sql\n",
    "-- Fast approximate preview, ~1% of micro-partitions\n",
    "SELECT event_type, COUNT(*) as cnt\n",
    "FROM prod.events\n",
    "SAMPLE SYSTEM (1)\n",
    "GROUP BY event_type\n",
    "ORDER BY cnt DESC;\n",
    "```\n",
    "\n",
    "Use this to get a rough distribution within seconds. Use `SEED(42)` if you want the preview to be repeatable across refreshes. ([Snowflake Docs][1])\n",
    "\n",
    "### 2) Bias-aware check: compare SYSTEM vs BERNOULLI\n",
    "\n",
    "```sql\n",
    "-- BERNOULLI (row-based)\n",
    "SELECT event_type, COUNT(*) as cnt_bernoulli\n",
    "FROM prod.events\n",
    "SAMPLE (1)  -- default is ROW / BERNOULLI\n",
    "GROUP BY event_type;\n",
    "\n",
    "-- SYSTEM (block-based)\n",
    "SELECT event_type, COUNT(*) as cnt_system\n",
    "FROM prod.events\n",
    "SAMPLE SYSTEM (1)\n",
    "GROUP BY event_type;\n",
    "```\n",
    "\n",
    "Compare `cnt_bernoulli` vs `cnt_system` to see if SYSTEM introduces visible bias for your partitioning. If they diverge significantly, prefer BERNOULLI.\n",
    "\n",
    "### 3) Reproducible development sample for testing\n",
    "\n",
    "```sql\n",
    "-- Reproducible sample using SYSTEM + SEED\n",
    "CREATE OR REPLACE TABLE dev.events_sample AS\n",
    "SELECT *\n",
    "FROM prod.events\n",
    "SAMPLE SYSTEM (2) SEED (12345);\n",
    "```\n",
    "\n",
    "This creates a dev table quickly (selective rows) you can share with devs.\n",
    "\n",
    "### 4) Fixed-size sample for exact N rows\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM prod.events\n",
    "SAMPLE (1000 ROWS);\n",
    "```\n",
    "\n",
    "Useful when you need a small dataset of exact size for UI demos or tests.\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Sampling vs CLONE — what's the difference and when to use which?\n",
    "\n",
    "**Zero-copy CLONE** creates a metadata-only copy of a table/schema/database — the clone points to the same underlying micro-partitions until you change data (copy-on-write). A clone is effectively an instant, full copy (no immediate storage cost until changes occur). `CREATE ... CLONE` is ideal when you need the *entire* dataset (exact), e.g., for full-scale integration testing or backups. ([Snowflake Docs][3])\n",
    "\n",
    "**Sampling** creates a subset of the original data (physical selection of rows) — smaller data volume, quicker scans, different content from clone.\n",
    "\n",
    "### When sampling is *more efficient* than clone:\n",
    "\n",
    "* You only need a **subset** for rapid prototyping, EDA, or ML experimentation — sampling reduces compute and storage dramatically (you can create a smaller physical table rather than a full clone that references whole dataset).\n",
    "* Cloning is great for an *exact* copy that preserves all rows and metadata but doesn’t reduce the dataset size. Clone is not cheaper if you truly need a smaller working set — clone gives you the full data logically and could still cause operations that scan the parent micro-partitions.\n",
    "\n",
    "### Example decision:\n",
    "\n",
    "* Need **exact** production snapshot for debugging a data issue → use `CREATE TABLE foo_clone CLONE foo;`.\n",
    "* Need **small dataset** for rapid model training or dashboard prototype → `CREATE TABLE foo_sample AS SELECT * FROM foo SAMPLE (1);` — cheaper to scan and process.\n",
    "\n",
    "**Key takeaway:** clone: instant full logical copy (useful when you need full fidelity). Sampling: create a reduced, faster-to-scan dataset (useful for speed/cost). ([Snowflake Docs][3])\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Statistical correctness & validation: how to be safe\n",
    "\n",
    "* **Always validate** samples by comparing a few aggregate metrics (mean, median, counts per category) against full-table results for a few checkpoints.\n",
    "* For estimates derived from samples, compute confidence intervals if possible (e.g., standard error for proportions) — helps convey expected error to stakeholders.\n",
    "* For cardinality/percentile needs, prefer Snowflake’s `APPROX_*` functions — they use formal algorithms (HLL, t-Digest) and will often give better accuracy/perf than naive sampling for those aggregates. ([Snowflake Docs][2])\n",
    "\n",
    "---\n",
    "\n",
    "# 11)  Questions (must-know / quick checks)\n",
    "\n",
    "Use these as quick self-assessment or flashcards:\n",
    "\n",
    "1. Explain the difference between `SAMPLE (10)` and `SAMPLE SYSTEM (10)`. Which is default? What are trade-offs?\n",
    "2. How does `SEED`/`REPEATABLE` work in Snowflake sampling? For which method(s) is it supported?\n",
    "3. If I sample before or after a JOIN, how does it affect cost and accuracy? What are the constraints?\n",
    "4. When would you use `SAMPLE (N ROWS)` vs `SAMPLE (p)`? What optimizations might be affected?\n",
    "5. How does Snowflake’s zero-copy `CLONE` work under the hood and when is cloning better than sampling for development environments?\n",
    "6. Name two Snowflake approximate functions and the algorithms they use (e.g., HyperLogLog, t-Digest).\n",
    "7. Describe a validation procedure to check whether sampling bias affects your metric (list concrete SQL queries or checks).\n",
    "8. What are micro-partitions and why do they matter for `SYSTEM` sampling (explain potential bias)?\n",
    "9. How would you make a sampling strategy reproducible across multiple developers?\n",
    "10. If you get a wildly different result between `SAMPLE SYSTEM (1)` and `SAMPLE (1)`, what are the possible causes and how would you investigate?\n",
    "\n",
    "(If you want, I’ll give model answers for each — say the word and I’ll expand.) ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 12) Practical checklist / best practices (actionable)\n",
    "\n",
    "* Start with `SYSTEM` for quick dashboards; validate vs `BERNOULLI` occasionally.\n",
    "* Use `SEED(...)` on `SYSTEM` when you need reproducible dev datasets.\n",
    "* Use `APPROX_COUNT_DISTINCT` / `APPROX_PERCENTILE` for cardinality/percentile problems instead of sampling + exact function when possible. ([Snowflake Docs][2])\n",
    "* If sampling for ML, ensure class balance (stratified sampling) if necessary — sampling uniformly may under-sample rare classes.\n",
    "* Log sample parameters (method, percent, seed) alongside results — makes analytics reproducible and auditable.\n",
    "* For joins: prefer sampling on subquery result if you intend to reduce the work (apply sample to the join result, not to table operands unless appropriate).\n",
    "\n",
    "---\n",
    "\n",
    "# 13) Extra: stratified sampling & deterministic reproducibility (practical trick)\n",
    "\n",
    "Snowflake's sample is uniform. For **stratified sampling** (e.g., preserve proportions of `country`), do:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE events_stratified AS\n",
    "SELECT * FROM (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (PARTITION BY country ORDER BY HASH(user_id)) AS rn,\n",
    "         COUNT(*) OVER (PARTITION BY country) AS cnt\n",
    "  FROM prod.events\n",
    ") t\n",
    "WHERE rn <= GREATEST(1, ROUND(cnt * 0.01)); -- ~1% per country\n",
    "```\n",
    "\n",
    "This preserves representation per stratum. You can also use `HASH()` or deterministic `RANDOM()` seeds to make selection repeatable.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "[1]: https://docs.snowflake.com/en/sql-reference/constructs/sample \"SAMPLE / TABLESAMPLE | Snowflake Documentation\"\n",
    "[2]: https://docs.snowflake.com/en/sql-reference/functions/approx_count_distinct?utm_source=chatgpt.com \"APPROX_COUNT_DISTINCT\"\n",
    "[3]: https://docs.snowflake.com/en/sql-reference/sql/create-clone?utm_source=chatgpt.com \"CREATE <object> … CLONE\"\n",
    "[4]: https://docs.snowflake.com/en/sql-reference/functions/approx_percentile?utm_source=chatgpt.com \"APPROX_PERCENTILE\"\n",
    "[5]: https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.Table.sample?utm_source=chatgpt.com \"snowflake.snowpark.Table.sample\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae9899",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **1️⃣ What’s the difference between `SAMPLE (10)` and `SAMPLE SYSTEM (10)`? Which is default? What are trade-offs?**\n",
    "\n",
    "### ✅ **Concept**\n",
    "\n",
    "* `SAMPLE (10)` → uses **BERNOULLI (row-based)** sampling.\n",
    "* `SAMPLE SYSTEM (10)` → uses **SYSTEM (block-based)** sampling.\n",
    "\n",
    "**Default** → `BERNOULLI`.\n",
    "\n",
    "### 🧠 **Mechanism**\n",
    "\n",
    "* **BERNOULLI** → decides *per row* whether to include it (independent random coin toss for each row).\n",
    "* **SYSTEM** → decides *per micro-partition (block)* whether to include it (if a block is chosen, all its rows come together).\n",
    "\n",
    "### ⚖️ **Trade-offs**\n",
    "\n",
    "| Method        | Pros                                         | Cons                                                    |\n",
    "| ------------- | -------------------------------------------- | ------------------------------------------------------- |\n",
    "| **BERNOULLI** | True random, unbiased                        | Slower (checks every row), more compute                 |\n",
    "| **SYSTEM**    | Much faster (works at micro-partition level) | May introduce bias if data is clustered (e.g., by date) |\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “By default, Snowflake uses Bernoulli sampling, which randomly selects individual rows. For large tables, I often switch to `SYSTEM` because it’s faster — it samples entire micro-partitions. The trade-off is that `SYSTEM` can be biased if the table is clustered, so I validate it using summary checks.”\n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ How does `SEED` work in Snowflake sampling? For which method is it supported?**\n",
    "\n",
    "### ✅ **Concept**\n",
    "\n",
    "* `SEED()` (or `REPEATABLE()`) makes a sample **deterministic** — same table + same seed = same sample every time.\n",
    "\n",
    "### ⚠️ **Supported only for:**\n",
    "\n",
    "* `SYSTEM` / `BLOCK` sampling.\n",
    "\n",
    "**Not supported for**:\n",
    "\n",
    "* `BERNOULLI` (row-based sampling)\n",
    "* **JOINs**, **views**, or **subqueries**\n",
    "\n",
    "### 🧩 **Example**\n",
    "\n",
    "```sql\n",
    "SELECT * FROM sales SAMPLE SYSTEM (5) SEED (100);\n",
    "```\n",
    "\n",
    "→ Every time you run this query (on the same table), you’ll get the **same 5% of data**.\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “The `SEED` makes the sample repeatable — great for reproducible dashboards or ML experiments. But it only works for `SYSTEM` sampling, not for Bernoulli, since block-level seeds are deterministic while row-level randomization isn’t repeatable in Snowflake.”\n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ If I sample before or after a JOIN, how does it affect cost and accuracy?**\n",
    "\n",
    "### ✅ **Concept**\n",
    "\n",
    "* **Sampling before JOIN** reduces data scanned → lower cost, faster execution.\n",
    "* **Sampling after JOIN** → full data processed → only final output is reduced → cost stays high.\n",
    "\n",
    "### ⚠️ **Important restriction**\n",
    "\n",
    "Snowflake only allows sampling **after a join** when:\n",
    "\n",
    "* It’s **row-based** (BERNOULLI)\n",
    "* **No SEED** is used.\n",
    "\n",
    "### 🧩 **Example**\n",
    "\n",
    "```sql\n",
    "-- Sampling BEFORE join → cheaper\n",
    "SELECT * FROM (\n",
    "    SELECT * FROM customers SAMPLE (10)\n",
    ") c\n",
    "JOIN orders o ON c.id = o.cust_id;\n",
    "\n",
    "-- Sampling AFTER join → full cost\n",
    "SELECT * FROM customers c\n",
    "JOIN orders o ON c.id = o.cust_id\n",
    "SAMPLE (10);\n",
    "```\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “If I apply sampling before the join, it reduces both compute and IO. But if I sample after, Snowflake still joins all rows first — it’s only reducing the output size, not cost. So, I always sample subqueries before joining when cost or performance matters.”\n",
    "\n",
    "---\n",
    "\n",
    "## **4️⃣ When would you use `SAMPLE (N ROWS)` vs `SAMPLE (PERCENT)`?**\n",
    "\n",
    "### ✅ **Concept**\n",
    "\n",
    "* `SAMPLE (N ROWS)` → fixed-size sample.\n",
    "* `SAMPLE (PERCENT)` → percentage-based sample.\n",
    "\n",
    "### ⚖️ **Trade-offs**\n",
    "\n",
    "| Type      | Use-case                                                 | Notes                                                         |\n",
    "| --------- | -------------------------------------------------------- | ------------------------------------------------------------- |\n",
    "| `N ROWS`  | When you need exact count (e.g., 1,000 rows for testing) | Works with **BERNOULLI** only, not `SYSTEM` or `SEED`.        |\n",
    "| `PERCENT` | When dataset size changes                                | Scales dynamically; works with both `BERNOULLI` and `SYSTEM`. |\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “I use `N ROWS` when I need a fixed-sized dataset for debugging or demo environments. For general EDA or analytics, I prefer a percentage-based sample, because it adjusts as table size changes.”\n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ How does Snowflake’s zero-copy CLONE work, and when is CLONE better than SAMPLE?**\n",
    "\n",
    "### ✅ **Concept**\n",
    "\n",
    "* `CREATE ... CLONE` → instant metadata-only copy of a table/schema/database.\n",
    "* Data isn’t duplicated until modified (copy-on-write).\n",
    "* `SAMPLE` → creates a physically smaller subset of rows.\n",
    "\n",
    "### ⚖️ **When to use**\n",
    "\n",
    "| Operation  | Use for                                       | Data size             |\n",
    "| ---------- | --------------------------------------------- | --------------------- |\n",
    "| **CLONE**  | Exact copy for debugging, backups, sandboxing | Full (same as source) |\n",
    "| **SAMPLE** | Small subset for analysis/ML                  | Reduced               |\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “`CLONE` is great for making a full environment snapshot instantly — it’s metadata-only. But if I just need a smaller dataset to prototype, I use `SAMPLE`. Sampling saves both time and compute, while clone preserves the entire dataset structure.”\n",
    "\n",
    "---\n",
    "\n",
    "## **6️⃣ Name two Snowflake approximate functions and the algorithms they use.**\n",
    "\n",
    "### ✅ **Functions**\n",
    "\n",
    "| Function                  | What it estimates          | Algorithm       |\n",
    "| ------------------------- | -------------------------- | --------------- |\n",
    "| `APPROX_COUNT_DISTINCT()` | Cardinality (unique count) | **HyperLogLog** |\n",
    "| `APPROX_PERCENTILE()`     | Percentiles, medians       | **t-Digest**    |\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “For approximate aggregations, Snowflake offers `APPROX_COUNT_DISTINCT`, which uses HyperLogLog to estimate unique counts, and `APPROX_PERCENTILE`, which uses t-Digest for percentile estimation. These functions are faster and memory-efficient — perfect for large datasets.”\n",
    "\n",
    "---\n",
    "\n",
    "## **7️⃣ How can you validate whether sampling bias affects your metrics?**\n",
    "\n",
    "### ✅ **Concept**\n",
    "\n",
    "Compare **aggregates** from the full dataset vs the sample:\n",
    "\n",
    "* Count\n",
    "* Mean\n",
    "* Category distribution\n",
    "* Percentiles\n",
    "\n",
    "### 🧩 **Example**\n",
    "\n",
    "```sql\n",
    "-- Full dataset\n",
    "SELECT event_type, COUNT(*), AVG(amount) FROM events GROUP BY event_type;\n",
    "\n",
    "-- Sampled dataset\n",
    "SELECT event_type, COUNT(*), AVG(amount)\n",
    "FROM events SAMPLE SYSTEM (2)\n",
    "GROUP BY event_type;\n",
    "```\n",
    "\n",
    "→ Compare ratios and ensure differences are small.\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “I validate sampling quality by comparing key metrics — like averages, counts, and category proportions — between the full and sampled datasets. If the deltas are small and consistent across runs, the sample is representative.”\n",
    "\n",
    "---\n",
    "\n",
    "## **8️⃣ What are micro-partitions and why do they matter for SYSTEM sampling?**\n",
    "\n",
    "### ✅ **Concept**\n",
    "\n",
    "* Snowflake physically stores data in **micro-partitions** (compressed blocks of ~16MB uncompressed data).\n",
    "* `SYSTEM` sampling works by randomly choosing **micro-partitions**, not individual rows.\n",
    "\n",
    "### ⚠️ **Impact**\n",
    "\n",
    "If a table is **clustered by time or category**, entire micro-partitions might contain similar data → SYSTEM sample may over/under-represent certain values.\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “SYSTEM sampling selects entire micro-partitions. So, if my table is clustered — say by date — a SYSTEM sample might only pull from certain days. That’s why I validate with Bernoulli or shuffle data before sampling.”\n",
    "\n",
    "---\n",
    "\n",
    "## **9️⃣ How would you make sampling reproducible across multiple developers?**\n",
    "\n",
    "### ✅ **Techniques**\n",
    "\n",
    "1. Use `SAMPLE SYSTEM (...) SEED (<value>)` — ensures same blocks chosen.\n",
    "2. Store seed and percentage in metadata.\n",
    "3. Materialize the sampled dataset in a shared schema.\n",
    "\n",
    "### 🧩 **Example**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE dev.events_sample AS\n",
    "SELECT * FROM prod.events SAMPLE SYSTEM (2) SEED (100);\n",
    "```\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “To make sampling consistent across environments, I fix the `SEED` value and method in SQL. For collaborative projects, I store the sample as a materialized table — this ensures everyone works with the same deterministic subset.”\n",
    "\n",
    "---\n",
    "\n",
    "## **🔟 If results differ wildly between `SAMPLE SYSTEM (1)` and `SAMPLE (1)`, what could cause that?**\n",
    "\n",
    "### ✅ **Possible causes**\n",
    "\n",
    "1. **Micro-partition bias** — SYSTEM chose blocks that are not representative.\n",
    "2. **Table is clustered** (e.g., time-ordered → SYSTEM picks recent data only).\n",
    "3. **Small sample size** → high variance in results.\n",
    "4. **Different inclusion logic** — SYSTEM operates on blocks, BERNOULLI on rows.\n",
    "\n",
    "### 💬 **How to answer**\n",
    "\n",
    "> “Huge discrepancies usually come from micro-partition bias. SYSTEM might have selected partitions containing skewed data — like only recent sales. In that case, I either use Bernoulli or stratified sampling for fair representation.”\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **Summary Table for Revision**\n",
    "\n",
    "| Topic                  | Key Points                                 |\n",
    "| ---------------------- | ------------------------------------------ |\n",
    "| Default sampling       | `BERNOULLI` (row-based)                    |\n",
    "| Fastest method         | `SYSTEM` (block-based)                     |\n",
    "| Deterministic sampling | Only `SYSTEM` supports `SEED`              |\n",
    "| Pre-join sampling      | Reduces cost                               |\n",
    "| Post-join sampling     | Reduces output only                        |\n",
    "| Fixed vs percent       | Fixed = exact N rows; Percent = scalable   |\n",
    "| CLONE vs SAMPLE        | CLONE = full copy; SAMPLE = subset         |\n",
    "| Approximate functions  | HyperLogLog, t-Digest                      |\n",
    "| Micro-partition        | Physical unit; used by SYSTEM sampling     |\n",
    "| Validation             | Compare aggregates between full and sample |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a70729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
