{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa6e86a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Why sampling? (the real-world story)\n",
    "\n",
    "Imagine you lead the data platform for an ecommerce company, **ShopFast**. The events table `events` stores clickstream and purchase events ‚Äî it‚Äôs 3 TB and grows fast. Your product manager wants an exploratory dashboard showing daily conversion rate trends for the last 90 days while the ML team needs a quick training sample for experimentation.\n",
    "\n",
    "Problems:\n",
    "\n",
    "* Running full scans on the 3 TB table for quick EDA or dashboard prototypes is slow and expensive.\n",
    "* You need **fast, \"good enough\" answers** for visualization and experimentation ‚Äî not always 100% exact.\n",
    "\n",
    "**Solution idea**: take a carefully chosen subset (sample) of the large table and run queries on that subset. Sampling gives:\n",
    "\n",
    "* Faster query response time (smaller data scanned).\n",
    "* Lower warehouse credits used.\n",
    "* A way to bootstrap models and visualizations quickly.\n",
    "\n",
    "But sampling trades exactness for speed ‚Äî so we must understand *how* Snowflake samples and which sampling algorithm fits which use-case. ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Approximate Query Processing (AQP) ‚Äî concept + Snowflake affordances\n",
    "\n",
    "**AQP** is the family of techniques that return *approximate* answers far faster and cheaper than exact computation. Two common patterns in Snowflake:\n",
    "\n",
    "1. Use **sampling** (TABLESAMPLE / SAMPLE) to run queries on a subset and extrapolate results.\n",
    "2. Use **built-in approximate functions** (e.g., `APPROX_COUNT_DISTINCT`, `APPROX_PERCENTILE`) which implement probabilistic algorithms (HyperLogLog, t-Digest, etc.) and are optimized for speed/low memory.\n",
    "\n",
    "When to prefer built-in approximate functions:\n",
    "\n",
    "* You need an approximate aggregate (distinct counts, percentiles) ‚Äî use `APPROX_*` functions (they‚Äôre robust and often preferable to manual sampling). ([Snowflake Docs][2])\n",
    "\n",
    "When to prefer sampling:\n",
    "\n",
    "* You need to run arbitrary complex queries (GROUP BY, ML feature extraction, quick visualizations) and are willing to accept small error bounds in exchange for speed.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Snowflake sampling in practice ‚Äî syntax & demo SQL\n",
    "\n",
    "Snowflake supports `SAMPLE` and `TABLESAMPLE` (synonymous). Two major sampling methods: `BERNOULLI | ROW` (row-based) and `SYSTEM | BLOCK` (block-based). You can also request fixed-size samples (`N ROWS`) and supply a repeatable `SEED` for deterministic samples (only supported for `SYSTEM`/`BLOCK`). ([Snowflake Docs][1])\n",
    "\n",
    "### Basic examples\n",
    "\n",
    "Fraction-based Bernoulli (default):\n",
    "\n",
    "```sql\n",
    "-- ~10% of rows (row-based / Bernoulli)\n",
    "SELECT * FROM events SAMPLE (10);\n",
    "-- equivalent\n",
    "SELECT * FROM events TABLESAMPLE BERNOULLI (10);\n",
    "```\n",
    "\n",
    "Fraction-based block/system with seed:\n",
    "\n",
    "```sql\n",
    "-- ~3% of blocks, repeatable sample with seed 82\n",
    "SELECT * FROM events SAMPLE SYSTEM (3) SEED (82);\n",
    "-- or\n",
    "SELECT * FROM events SAMPLE BLOCK (0.012) REPEATABLE (99992);\n",
    "```\n",
    "\n",
    "Fixed-size:\n",
    "\n",
    "```sql\n",
    "-- exact 100 rows (unless table has fewer)\n",
    "SELECT * FROM events SAMPLE (100 ROWS);\n",
    "```\n",
    "\n",
    "Repeatable/deterministic notes:\n",
    "\n",
    "* `SEED(...)` / `REPEATABLE(...)` makes a `SYSTEM` sample deterministic **for the same unchanged table**. It is not supported for `BERNOULLI` seeds and not supported on views/subqueries. ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 4) How the two sampling methods *work* ‚Äî intuition + story\n",
    "\n",
    "## A) BERNOULLI / ROW (row-based sampling)\n",
    "\n",
    "**Mechanics (simple)**: imagine flipping a weighted coin for **every row**. Each row is independently included with probability `p/100`. So expected sample size ‚âà `p/100 * n`. Because it's per-row randomness, this method yields an unbiased random subset of rows (statistically closest to true random sampling).\n",
    "\n",
    "**When it matters**:\n",
    "\n",
    "* Good for statistical sampling where each row should have an independent inclusion chance.\n",
    "* Useful when data distribution within blocks matters (BERNOULLI won't bias toward specific micro-partitions).\n",
    "\n",
    "**Drawbacks**:\n",
    "\n",
    "* More CPU work because Snowflake must decide inclusion per row across many micro-partitions ‚Äî can be slower/scan more micro-partition metadata than SYSTEM. For very large tables, overhead is acceptable but still higher than SYSTEM. ([Snowflake Docs][1])\n",
    "\n",
    "**Story**: You want a truly random sample of `events` so that your conversion-rate estimator is unbiased. Use `SAMPLE (5)` (BERNOULLI default) ‚Äî each event has independent 5% chance to be picked.\n",
    "\n",
    "## B) SYSTEM / BLOCK (block-based sampling)\n",
    "\n",
    "**Mechanics (simple)**: Snowflake flips the coin per **block / micro-partition** (think: choose whole micro-partitions with probability `p/100`). Micro-partitions contain contiguous rows and column statistics.\n",
    "\n",
    "**When it matters**:\n",
    "\n",
    "* SYSTEM is **often much faster** because it works at micro-partition granularity and can avoid decoding lots of rows; great when you need speed and are okay with slight block-level bias. Snowflake documentation specifically notes SYSTEM/BLOCK is often faster than BERNOULLI. ([Snowflake Docs][1])\n",
    "\n",
    "**Drawbacks**:\n",
    "\n",
    "* Potential bias: if your data is ordered (e.g., time-sorted) or micro-partitions cluster similar values, SYSTEM sampling can under- or over-represent certain values (biased sample), especially for **small tables** or when sampling percentages are tiny.\n",
    "* For tiny tables, block-level granularity makes the sample less representative.\n",
    "\n",
    "**Story**: You‚Äôre building a dashboard where speed trumps tiny bias. For a humongous `events` table, do `SAMPLE SYSTEM (2)` to get an approximate view fast ‚Äî you‚Äôll get quick results, but verify if micro-partition layout could bias results (e.g., if all failed payments are in a small subset of micro-partitions).\n",
    "\n",
    "---\n",
    "\n",
    "# 5) When to use which? Practical guidance\n",
    "\n",
    "* **Exploratory analysis / dashboards** where speed is critical and slight bias is acceptable ‚Üí use `SYSTEM` (block) sampling. Optionally add `SEED` for reproducibility of dashboard preview data.\n",
    "* **Statistical experiments, sampling for model training, or when unbiasedness is required** ‚Üí use `BERNOULLI`/`ROW` sampling.\n",
    "* **If you need a fixed number of rows** (exact N) ‚Üí use `SAMPLE (N ROWS)`, but note `SYSTEM` + fixed-size is **not supported**. Fixed-size sampling may prevent some optimizations and can be slower. ([Snowflake Docs][1])\n",
    "* **If consistent sample between runs is needed for reproducible experiments** ‚Üí use `SYSTEM` sampling with `SEED(...)`. (Note: `SEED` is only supported for SYSTEM/BLOCK sampling and not for ROW/Bernoulli; and sampling on copy may differ.) ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Advantages & disadvantages (quick table)\n",
    "\n",
    "* BERNOULLI / ROW\n",
    "\n",
    "  * Advantage: unbiased per-row sampling; works well for joins if no seed used.\n",
    "  * Disadvantage: slower than SYSTEM; more expensive for huge tables.\n",
    "\n",
    "* SYSTEM / BLOCK\n",
    "\n",
    "  * Advantage: faster, often cheaper because it selects entire micro-partitions.\n",
    "  * Disadvantage: possible sampling bias due to micro-partitioning; `SEED` only supported for block sampling; can't do fixed-size + seed.\n",
    "\n",
    "* General trade-offs:\n",
    "\n",
    "  * Sampling reduces scanned data (cost) but introduces sampling variance. Choose method by error tolerance + performance need. ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Important semantics & gotchas (from the docs + experience)\n",
    "\n",
    "* **Sampling after a JOIN**: sampling on the result of a `JOIN` is allowed only when the sampling is row-based (BERNOULLI) and **no seed** is used. Also, if you apply SAMPLE to tables in a join separately, the sample is applied to each table before joining ‚Äî it does *not* reduce join cost unless sampling is applied before the join as part of the plan or you sample a subquery result. In some cases sampling is done after join processing ‚Äî so it might not reduce join compute cost. Always test. ([Snowflake Docs][1])\n",
    "* **Determinism**: If you specify the same `SEED` and the table hasn't changed, `SYSTEM` samples are repeatable. But a copy/clone of the table might produce different sample even with same seed because micro-partitions/state may differ. ([Snowflake Docs][1])\n",
    "* **Fixed-size sampling**: returns exact requested rows (if table larger), but **SYSTEM** and `SEED` aren‚Äôt supported with fixed-size sampling. Fixed-size can prevent optimizations and be slower. ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Demo: full practical walkthrough (examples you can run)\n",
    "\n",
    "Assume a table `prod.events` with columns `(event_ts, user_id, event_type, amount)`.\n",
    "\n",
    "### 1) Quick dashboard preview (fast)\n",
    "\n",
    "```sql\n",
    "-- Fast approximate preview, ~1% of micro-partitions\n",
    "SELECT event_type, COUNT(*) as cnt\n",
    "FROM prod.events\n",
    "SAMPLE SYSTEM (1)\n",
    "GROUP BY event_type\n",
    "ORDER BY cnt DESC;\n",
    "```\n",
    "\n",
    "Use this to get a rough distribution within seconds. Use `SEED(42)` if you want the preview to be repeatable across refreshes. ([Snowflake Docs][1])\n",
    "\n",
    "### 2) Bias-aware check: compare SYSTEM vs BERNOULLI\n",
    "\n",
    "```sql\n",
    "-- BERNOULLI (row-based)\n",
    "SELECT event_type, COUNT(*) as cnt_bernoulli\n",
    "FROM prod.events\n",
    "SAMPLE (1)  -- default is ROW / BERNOULLI\n",
    "GROUP BY event_type;\n",
    "\n",
    "-- SYSTEM (block-based)\n",
    "SELECT event_type, COUNT(*) as cnt_system\n",
    "FROM prod.events\n",
    "SAMPLE SYSTEM (1)\n",
    "GROUP BY event_type;\n",
    "```\n",
    "\n",
    "Compare `cnt_bernoulli` vs `cnt_system` to see if SYSTEM introduces visible bias for your partitioning. If they diverge significantly, prefer BERNOULLI.\n",
    "\n",
    "### 3) Reproducible development sample for testing\n",
    "\n",
    "```sql\n",
    "-- Reproducible sample using SYSTEM + SEED\n",
    "CREATE OR REPLACE TABLE dev.events_sample AS\n",
    "SELECT *\n",
    "FROM prod.events\n",
    "SAMPLE SYSTEM (2) SEED (12345);\n",
    "```\n",
    "\n",
    "This creates a dev table quickly (selective rows) you can share with devs.\n",
    "\n",
    "### 4) Fixed-size sample for exact N rows\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM prod.events\n",
    "SAMPLE (1000 ROWS);\n",
    "```\n",
    "\n",
    "Useful when you need a small dataset of exact size for UI demos or tests.\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Sampling vs CLONE ‚Äî what's the difference and when to use which?\n",
    "\n",
    "**Zero-copy CLONE** creates a metadata-only copy of a table/schema/database ‚Äî the clone points to the same underlying micro-partitions until you change data (copy-on-write). A clone is effectively an instant, full copy (no immediate storage cost until changes occur). `CREATE ... CLONE` is ideal when you need the *entire* dataset (exact), e.g., for full-scale integration testing or backups. ([Snowflake Docs][3])\n",
    "\n",
    "**Sampling** creates a subset of the original data (physical selection of rows) ‚Äî smaller data volume, quicker scans, different content from clone.\n",
    "\n",
    "### When sampling is *more efficient* than clone:\n",
    "\n",
    "* You only need a **subset** for rapid prototyping, EDA, or ML experimentation ‚Äî sampling reduces compute and storage dramatically (you can create a smaller physical table rather than a full clone that references whole dataset).\n",
    "* Cloning is great for an *exact* copy that preserves all rows and metadata but doesn‚Äôt reduce the dataset size. Clone is not cheaper if you truly need a smaller working set ‚Äî clone gives you the full data logically and could still cause operations that scan the parent micro-partitions.\n",
    "\n",
    "### Example decision:\n",
    "\n",
    "* Need **exact** production snapshot for debugging a data issue ‚Üí use `CREATE TABLE foo_clone CLONE foo;`.\n",
    "* Need **small dataset** for rapid model training or dashboard prototype ‚Üí `CREATE TABLE foo_sample AS SELECT * FROM foo SAMPLE (1);` ‚Äî cheaper to scan and process.\n",
    "\n",
    "**Key takeaway:** clone: instant full logical copy (useful when you need full fidelity). Sampling: create a reduced, faster-to-scan dataset (useful for speed/cost). ([Snowflake Docs][3])\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Statistical correctness & validation: how to be safe\n",
    "\n",
    "* **Always validate** samples by comparing a few aggregate metrics (mean, median, counts per category) against full-table results for a few checkpoints.\n",
    "* For estimates derived from samples, compute confidence intervals if possible (e.g., standard error for proportions) ‚Äî helps convey expected error to stakeholders.\n",
    "* For cardinality/percentile needs, prefer Snowflake‚Äôs `APPROX_*` functions ‚Äî they use formal algorithms (HLL, t-Digest) and will often give better accuracy/perf than naive sampling for those aggregates. ([Snowflake Docs][2])\n",
    "\n",
    "---\n",
    "\n",
    "# 11)  Questions (must-know / quick checks)\n",
    "\n",
    "Use these as quick self-assessment or flashcards:\n",
    "\n",
    "1. Explain the difference between `SAMPLE (10)` and `SAMPLE SYSTEM (10)`. Which is default? What are trade-offs?\n",
    "2. How does `SEED`/`REPEATABLE` work in Snowflake sampling? For which method(s) is it supported?\n",
    "3. If I sample before or after a JOIN, how does it affect cost and accuracy? What are the constraints?\n",
    "4. When would you use `SAMPLE (N ROWS)` vs `SAMPLE (p)`? What optimizations might be affected?\n",
    "5. How does Snowflake‚Äôs zero-copy `CLONE` work under the hood and when is cloning better than sampling for development environments?\n",
    "6. Name two Snowflake approximate functions and the algorithms they use (e.g., HyperLogLog, t-Digest).\n",
    "7. Describe a validation procedure to check whether sampling bias affects your metric (list concrete SQL queries or checks).\n",
    "8. What are micro-partitions and why do they matter for `SYSTEM` sampling (explain potential bias)?\n",
    "9. How would you make a sampling strategy reproducible across multiple developers?\n",
    "10. If you get a wildly different result between `SAMPLE SYSTEM (1)` and `SAMPLE (1)`, what are the possible causes and how would you investigate?\n",
    "\n",
    "(If you want, I‚Äôll give model answers for each ‚Äî say the word and I‚Äôll expand.) ([Snowflake Docs][1])\n",
    "\n",
    "---\n",
    "\n",
    "# 12) Practical checklist / best practices (actionable)\n",
    "\n",
    "* Start with `SYSTEM` for quick dashboards; validate vs `BERNOULLI` occasionally.\n",
    "* Use `SEED(...)` on `SYSTEM` when you need reproducible dev datasets.\n",
    "* Use `APPROX_COUNT_DISTINCT` / `APPROX_PERCENTILE` for cardinality/percentile problems instead of sampling + exact function when possible. ([Snowflake Docs][2])\n",
    "* If sampling for ML, ensure class balance (stratified sampling) if necessary ‚Äî sampling uniformly may under-sample rare classes.\n",
    "* Log sample parameters (method, percent, seed) alongside results ‚Äî makes analytics reproducible and auditable.\n",
    "* For joins: prefer sampling on subquery result if you intend to reduce the work (apply sample to the join result, not to table operands unless appropriate).\n",
    "\n",
    "---\n",
    "\n",
    "# 13) Extra: stratified sampling & deterministic reproducibility (practical trick)\n",
    "\n",
    "Snowflake's sample is uniform. For **stratified sampling** (e.g., preserve proportions of `country`), do:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE events_stratified AS\n",
    "SELECT * FROM (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (PARTITION BY country ORDER BY HASH(user_id)) AS rn,\n",
    "         COUNT(*) OVER (PARTITION BY country) AS cnt\n",
    "  FROM prod.events\n",
    ") t\n",
    "WHERE rn <= GREATEST(1, ROUND(cnt * 0.01)); -- ~1% per country\n",
    "```\n",
    "\n",
    "This preserves representation per stratum. You can also use `HASH()` or deterministic `RANDOM()` seeds to make selection repeatable.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "[1]: https://docs.snowflake.com/en/sql-reference/constructs/sample \"SAMPLE / TABLESAMPLE | Snowflake Documentation\"\n",
    "[2]: https://docs.snowflake.com/en/sql-reference/functions/approx_count_distinct?utm_source=chatgpt.com \"APPROX_COUNT_DISTINCT\"\n",
    "[3]: https://docs.snowflake.com/en/sql-reference/sql/create-clone?utm_source=chatgpt.com \"CREATE <object> ‚Ä¶ CLONE\"\n",
    "[4]: https://docs.snowflake.com/en/sql-reference/functions/approx_percentile?utm_source=chatgpt.com \"APPROX_PERCENTILE\"\n",
    "[5]: https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.Table.sample?utm_source=chatgpt.com \"snowflake.snowpark.Table.sample\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae9899",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ What‚Äôs the difference between `SAMPLE (10)` and `SAMPLE SYSTEM (10)`? Which is default? What are trade-offs?**\n",
    "\n",
    "### ‚úÖ **Concept**\n",
    "\n",
    "* `SAMPLE (10)` ‚Üí uses **BERNOULLI (row-based)** sampling.\n",
    "* `SAMPLE SYSTEM (10)` ‚Üí uses **SYSTEM (block-based)** sampling.\n",
    "\n",
    "**Default** ‚Üí `BERNOULLI`.\n",
    "\n",
    "### üß† **Mechanism**\n",
    "\n",
    "* **BERNOULLI** ‚Üí decides *per row* whether to include it (independent random coin toss for each row).\n",
    "* **SYSTEM** ‚Üí decides *per micro-partition (block)* whether to include it (if a block is chosen, all its rows come together).\n",
    "\n",
    "### ‚öñÔ∏è **Trade-offs**\n",
    "\n",
    "| Method        | Pros                                         | Cons                                                    |\n",
    "| ------------- | -------------------------------------------- | ------------------------------------------------------- |\n",
    "| **BERNOULLI** | True random, unbiased                        | Slower (checks every row), more compute                 |\n",
    "| **SYSTEM**    | Much faster (works at micro-partition level) | May introduce bias if data is clustered (e.g., by date) |\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚ÄúBy default, Snowflake uses Bernoulli sampling, which randomly selects individual rows. For large tables, I often switch to `SYSTEM` because it‚Äôs faster ‚Äî it samples entire micro-partitions. The trade-off is that `SYSTEM` can be biased if the table is clustered, so I validate it using summary checks.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ How does `SEED` work in Snowflake sampling? For which method is it supported?**\n",
    "\n",
    "### ‚úÖ **Concept**\n",
    "\n",
    "* `SEED()` (or `REPEATABLE()`) makes a sample **deterministic** ‚Äî same table + same seed = same sample every time.\n",
    "\n",
    "### ‚ö†Ô∏è **Supported only for:**\n",
    "\n",
    "* `SYSTEM` / `BLOCK` sampling.\n",
    "\n",
    "**Not supported for**:\n",
    "\n",
    "* `BERNOULLI` (row-based sampling)\n",
    "* **JOINs**, **views**, or **subqueries**\n",
    "\n",
    "### üß© **Example**\n",
    "\n",
    "```sql\n",
    "SELECT * FROM sales SAMPLE SYSTEM (5) SEED (100);\n",
    "```\n",
    "\n",
    "‚Üí Every time you run this query (on the same table), you‚Äôll get the **same 5% of data**.\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚ÄúThe `SEED` makes the sample repeatable ‚Äî great for reproducible dashboards or ML experiments. But it only works for `SYSTEM` sampling, not for Bernoulli, since block-level seeds are deterministic while row-level randomization isn‚Äôt repeatable in Snowflake.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ If I sample before or after a JOIN, how does it affect cost and accuracy?**\n",
    "\n",
    "### ‚úÖ **Concept**\n",
    "\n",
    "* **Sampling before JOIN** reduces data scanned ‚Üí lower cost, faster execution.\n",
    "* **Sampling after JOIN** ‚Üí full data processed ‚Üí only final output is reduced ‚Üí cost stays high.\n",
    "\n",
    "### ‚ö†Ô∏è **Important restriction**\n",
    "\n",
    "Snowflake only allows sampling **after a join** when:\n",
    "\n",
    "* It‚Äôs **row-based** (BERNOULLI)\n",
    "* **No SEED** is used.\n",
    "\n",
    "### üß© **Example**\n",
    "\n",
    "```sql\n",
    "-- Sampling BEFORE join ‚Üí cheaper\n",
    "SELECT * FROM (\n",
    "    SELECT * FROM customers SAMPLE (10)\n",
    ") c\n",
    "JOIN orders o ON c.id = o.cust_id;\n",
    "\n",
    "-- Sampling AFTER join ‚Üí full cost\n",
    "SELECT * FROM customers c\n",
    "JOIN orders o ON c.id = o.cust_id\n",
    "SAMPLE (10);\n",
    "```\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚ÄúIf I apply sampling before the join, it reduces both compute and IO. But if I sample after, Snowflake still joins all rows first ‚Äî it‚Äôs only reducing the output size, not cost. So, I always sample subqueries before joining when cost or performance matters.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ When would you use `SAMPLE (N ROWS)` vs `SAMPLE (PERCENT)`?**\n",
    "\n",
    "### ‚úÖ **Concept**\n",
    "\n",
    "* `SAMPLE (N ROWS)` ‚Üí fixed-size sample.\n",
    "* `SAMPLE (PERCENT)` ‚Üí percentage-based sample.\n",
    "\n",
    "### ‚öñÔ∏è **Trade-offs**\n",
    "\n",
    "| Type      | Use-case                                                 | Notes                                                         |\n",
    "| --------- | -------------------------------------------------------- | ------------------------------------------------------------- |\n",
    "| `N ROWS`  | When you need exact count (e.g., 1,000 rows for testing) | Works with **BERNOULLI** only, not `SYSTEM` or `SEED`.        |\n",
    "| `PERCENT` | When dataset size changes                                | Scales dynamically; works with both `BERNOULLI` and `SYSTEM`. |\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚ÄúI use `N ROWS` when I need a fixed-sized dataset for debugging or demo environments. For general EDA or analytics, I prefer a percentage-based sample, because it adjusts as table size changes.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ How does Snowflake‚Äôs zero-copy CLONE work, and when is CLONE better than SAMPLE?**\n",
    "\n",
    "### ‚úÖ **Concept**\n",
    "\n",
    "* `CREATE ... CLONE` ‚Üí instant metadata-only copy of a table/schema/database.\n",
    "* Data isn‚Äôt duplicated until modified (copy-on-write).\n",
    "* `SAMPLE` ‚Üí creates a physically smaller subset of rows.\n",
    "\n",
    "### ‚öñÔ∏è **When to use**\n",
    "\n",
    "| Operation  | Use for                                       | Data size             |\n",
    "| ---------- | --------------------------------------------- | --------------------- |\n",
    "| **CLONE**  | Exact copy for debugging, backups, sandboxing | Full (same as source) |\n",
    "| **SAMPLE** | Small subset for analysis/ML                  | Reduced               |\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚Äú`CLONE` is great for making a full environment snapshot instantly ‚Äî it‚Äôs metadata-only. But if I just need a smaller dataset to prototype, I use `SAMPLE`. Sampling saves both time and compute, while clone preserves the entire dataset structure.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **6Ô∏è‚É£ Name two Snowflake approximate functions and the algorithms they use.**\n",
    "\n",
    "### ‚úÖ **Functions**\n",
    "\n",
    "| Function                  | What it estimates          | Algorithm       |\n",
    "| ------------------------- | -------------------------- | --------------- |\n",
    "| `APPROX_COUNT_DISTINCT()` | Cardinality (unique count) | **HyperLogLog** |\n",
    "| `APPROX_PERCENTILE()`     | Percentiles, medians       | **t-Digest**    |\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚ÄúFor approximate aggregations, Snowflake offers `APPROX_COUNT_DISTINCT`, which uses HyperLogLog to estimate unique counts, and `APPROX_PERCENTILE`, which uses t-Digest for percentile estimation. These functions are faster and memory-efficient ‚Äî perfect for large datasets.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **7Ô∏è‚É£ How can you validate whether sampling bias affects your metrics?**\n",
    "\n",
    "### ‚úÖ **Concept**\n",
    "\n",
    "Compare **aggregates** from the full dataset vs the sample:\n",
    "\n",
    "* Count\n",
    "* Mean\n",
    "* Category distribution\n",
    "* Percentiles\n",
    "\n",
    "### üß© **Example**\n",
    "\n",
    "```sql\n",
    "-- Full dataset\n",
    "SELECT event_type, COUNT(*), AVG(amount) FROM events GROUP BY event_type;\n",
    "\n",
    "-- Sampled dataset\n",
    "SELECT event_type, COUNT(*), AVG(amount)\n",
    "FROM events SAMPLE SYSTEM (2)\n",
    "GROUP BY event_type;\n",
    "```\n",
    "\n",
    "‚Üí Compare ratios and ensure differences are small.\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚ÄúI validate sampling quality by comparing key metrics ‚Äî like averages, counts, and category proportions ‚Äî between the full and sampled datasets. If the deltas are small and consistent across runs, the sample is representative.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **8Ô∏è‚É£ What are micro-partitions and why do they matter for SYSTEM sampling?**\n",
    "\n",
    "### ‚úÖ **Concept**\n",
    "\n",
    "* Snowflake physically stores data in **micro-partitions** (compressed blocks of ~16MB uncompressed data).\n",
    "* `SYSTEM` sampling works by randomly choosing **micro-partitions**, not individual rows.\n",
    "\n",
    "### ‚ö†Ô∏è **Impact**\n",
    "\n",
    "If a table is **clustered by time or category**, entire micro-partitions might contain similar data ‚Üí SYSTEM sample may over/under-represent certain values.\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚ÄúSYSTEM sampling selects entire micro-partitions. So, if my table is clustered ‚Äî say by date ‚Äî a SYSTEM sample might only pull from certain days. That‚Äôs why I validate with Bernoulli or shuffle data before sampling.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **9Ô∏è‚É£ How would you make sampling reproducible across multiple developers?**\n",
    "\n",
    "### ‚úÖ **Techniques**\n",
    "\n",
    "1. Use `SAMPLE SYSTEM (...) SEED (<value>)` ‚Äî ensures same blocks chosen.\n",
    "2. Store seed and percentage in metadata.\n",
    "3. Materialize the sampled dataset in a shared schema.\n",
    "\n",
    "### üß© **Example**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE dev.events_sample AS\n",
    "SELECT * FROM prod.events SAMPLE SYSTEM (2) SEED (100);\n",
    "```\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚ÄúTo make sampling consistent across environments, I fix the `SEED` value and method in SQL. For collaborative projects, I store the sample as a materialized table ‚Äî this ensures everyone works with the same deterministic subset.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **üîü If results differ wildly between `SAMPLE SYSTEM (1)` and `SAMPLE (1)`, what could cause that?**\n",
    "\n",
    "### ‚úÖ **Possible causes**\n",
    "\n",
    "1. **Micro-partition bias** ‚Äî SYSTEM chose blocks that are not representative.\n",
    "2. **Table is clustered** (e.g., time-ordered ‚Üí SYSTEM picks recent data only).\n",
    "3. **Small sample size** ‚Üí high variance in results.\n",
    "4. **Different inclusion logic** ‚Äî SYSTEM operates on blocks, BERNOULLI on rows.\n",
    "\n",
    "### üí¨ **How to answer**\n",
    "\n",
    "> ‚ÄúHuge discrepancies usually come from micro-partition bias. SYSTEM might have selected partitions containing skewed data ‚Äî like only recent sales. In that case, I either use Bernoulli or stratified sampling for fair representation.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **Summary Table for Revision**\n",
    "\n",
    "| Topic                  | Key Points                                 |\n",
    "| ---------------------- | ------------------------------------------ |\n",
    "| Default sampling       | `BERNOULLI` (row-based)                    |\n",
    "| Fastest method         | `SYSTEM` (block-based)                     |\n",
    "| Deterministic sampling | Only `SYSTEM` supports `SEED`              |\n",
    "| Pre-join sampling      | Reduces cost                               |\n",
    "| Post-join sampling     | Reduces output only                        |\n",
    "| Fixed vs percent       | Fixed = exact N rows; Percent = scalable   |\n",
    "| CLONE vs SAMPLE        | CLONE = full copy; SAMPLE = subset         |\n",
    "| Approximate functions  | HyperLogLog, t-Digest                      |\n",
    "| Micro-partition        | Physical unit; used by SYSTEM sampling     |\n",
    "| Validation             | Compare aggregates between full and sample |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a70729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
