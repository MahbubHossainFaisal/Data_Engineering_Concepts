{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49444aa6",
   "metadata": {},
   "source": [
    "\n",
    "**Scenario (Retail-X):** hourly `orders_YYYYMMDD.csv` files land in `s3://retailx-raw/orders/`. Some rows occasionally contain bad dates or non-numeric totals. You want to load everything that’s good, capture the bad rows in a place you can fix, then re-load only the fixed rows — safely, repeatably.\n",
    "\n",
    "---\n",
    "\n",
    "# TL;DR (one-line plan)\n",
    "\n",
    "1. Create file format + stage.\n",
    "2. Validate files first (`VALIDATION_MODE='RETURN_ERRORS'`) to see problems.\n",
    "3. Load with `ON_ERROR='CONTINUE'` to let good rows in.\n",
    "4. Capture error metadata with `VALIDATE(...)` or `RESULT_SCAN(LAST_QUERY_ID())`.\n",
    "5. Extract actual bad rows (using `TRY_` functions when needed), write them to an internal stage or error table, fix them, then re-load only those fixed rows.\n",
    "   (Examples follow.) ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "# A — Setup (file format, stage, table) — copy/paste these (edit names)\n",
    "\n",
    "```sql\n",
    "-- 1) File format for CSV\n",
    "CREATE OR REPLACE FILE FORMAT retailx_csv_fmt\n",
    "  TYPE = CSV\n",
    "  FIELD_DELIMITER = ','\n",
    "  SKIP_HEADER = 1\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  TRIM_SPACE = TRUE\n",
    "  NULL_IF = ('', 'NULL', 'null')\n",
    "  COMPRESSION = 'AUTO';\n",
    "\n",
    "-- 2) External stage (assumes you already created STORAGE_INTEGRATION retailx_s3_int)\n",
    "CREATE OR REPLACE STAGE retailx_orders_stage\n",
    "  URL = 's3://retailx-raw/orders/'\n",
    "  STORAGE_INTEGRATION = retailx_s3_int\n",
    "  FILE_FORMAT = retailx_csv_fmt;\n",
    "\n",
    "-- 3) Target (production) table\n",
    "CREATE OR REPLACE TABLE retailx_orders (\n",
    "  order_id    INTEGER,\n",
    "  customer_id STRING,\n",
    "  created_at  TIMESTAMP_NTZ,\n",
    "  total_usd   NUMBER(10,2)\n",
    ");\n",
    "```\n",
    "\n",
    "(If you don’t have `retailx_s3_int` yet, create it as a Storage Integration — we covered that flow earlier.) ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "# B — Step 1: **Validate** files (dry run) — find every error without loading\n",
    "\n",
    "**Why first?** `VALIDATION_MODE` scans files and returns row-level error details — you learn what to fix before touching production tables. Use this when you want to inspect problems first. Example:\n",
    "\n",
    "```sql\n",
    "COPY INTO retailx_orders\n",
    "  FROM @retailx_orders_stage\n",
    "  FILE_FORMAT = (FORMAT_NAME = 'retailx_csv_fmt')\n",
    "  PATTERN = '.*orders_20250828.*[.]csv'\n",
    "  VALIDATION_MODE = 'RETURN_ERRORS';\n",
    "```\n",
    "\n",
    "* This returns a result-set listing each error (error message, file, line, column, row number). You can capture that result with `RESULT_SCAN(LAST_QUERY_ID())` or create a table from it. (Snowflake docs show the `VALIDATION_MODE` behavior.) ([Snowflake Documentation][1])\n",
    "\n",
    "Example: save the validation output to an error-table:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE retailx_orders_validation_errors AS\n",
    "SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));\n",
    "```\n",
    "\n",
    "`RESULT_SCAN(LAST_QUERY_ID())` reads the result set returned by the previous `COPY ... VALIDATION_MODE` command. Use that table to inspect full error details (error text, file, row, column). ([Snowflake Documentation][2])\n",
    "\n",
    "**Important caveat:** `VALIDATION_MODE` does *not* support `COPY` statements that perform SQL transformations during the load (e.g., `COPY INTO table (SELECT ...)`). If you must transform during load, do a validation strategy using a raw landing table or ad-hoc queries — see next sections. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "# C — Step 2: Load the good rows — `ON_ERROR = 'CONTINUE'` (what it means)\n",
    "\n",
    "```sql\n",
    "COPY INTO retailx_orders\n",
    "  FROM @retailx_orders_stage\n",
    "  FILE_FORMAT = (FORMAT_NAME = 'retailx_csv_fmt')\n",
    "  PATTERN = '.*orders_20250828.*[.]csv'\n",
    "  ON_ERROR = 'CONTINUE';\n",
    "```\n",
    "\n",
    "**What `ON_ERROR = 'CONTINUE'` does**\n",
    "\n",
    "* If a row in a file causes an error (parsing, type conversion, null into non-nullable, etc.), Snowflake **skips that row** and continues loading the remaining rows from that file and the subsequent files. It does **not** abort the whole command. This is different from the default `ABORT_STATEMENT` (which stops at first error) and `SKIP_FILE` (which skips the entire file on the first error). Use `CONTINUE` when you want to ingest all good rows while still discovering bad rows. ([Snowflake Documentation][3])\n",
    "\n",
    "**After this run you will have:**\n",
    "\n",
    "* Good rows inserted into `retailx_orders`.\n",
    "* The load result (query) will report counts of rows loaded vs. errors.\n",
    "* Metadata about the load is available via `TABLE(VALIDATE(...))`, `COPY_HISTORY`, `LOAD_HISTORY`, or by capturing `LAST_QUERY_ID()` results. ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "# D — Step 3: Collect the error details after the LOAD\n",
    "\n",
    "You have two useful choices depending on whether you want *metadata about errors* or the *actual raw rows that failed*.\n",
    "\n",
    "## D.1 — Metadata / error reasons (fast, built-in)\n",
    "\n",
    "If you executed the `COPY` with `ON_ERROR='CONTINUE'`, capture all errors for that load run:\n",
    "\n",
    "```sql\n",
    "-- Immediately after your COPY command:\n",
    "CREATE OR REPLACE TABLE retailx_orders_load_errors AS\n",
    "SELECT * FROM TABLE(VALIDATE('retailx_orders', JOB_ID => LAST_QUERY_ID()));\n",
    "```\n",
    "\n",
    "`VALIDATE(table, JOB_ID => ...)` returns the errors encountered during the COPY job — one row per error with details (error message, file, line, column, etc.). Save it to a table for triage. ([Snowflake Documentation][5])\n",
    "\n",
    "## D.2 — Capture the actual **raw rows** that failed (recommended if you want to fix & re-load only those rows)\n",
    "\n",
    "`VALIDATE`/`RETURN_ERRORS` give you **error metadata**, but they don’t always return the exact raw CSV text in a convenient column. To get the raw problem rows, query the stage with `TRY_` functions to detect rows that would fail when cast to the target types — then save those rows to a table or directly export them to a stage file.\n",
    "\n",
    "Example: detect bad rows by using `TRY_CAST` / `TRY_TO_TIMESTAMP` (these return `NULL` instead of error):\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE retailx_orders_bad_raw AS\n",
    "SELECT\n",
    "  t.$1::STRING AS order_id_raw,\n",
    "  t.$2::STRING AS customer_id_raw,\n",
    "  t.$3::STRING AS created_at_raw,\n",
    "  t.$4::STRING AS total_usd_raw,\n",
    "  METADATA$FILENAME           AS source_file,\n",
    "  METADATA$FILE_ROW_NUMBER    AS source_row_num\n",
    "FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt') t\n",
    "WHERE TRY_CAST(t.$1 AS INTEGER) IS NULL                       -- order_id not int\n",
    "   OR TRY_TO_TIMESTAMP(t.$3, 'YYYY-MM-DD HH24:MI:SS') IS NULL -- bad date\n",
    "   OR TRY_CAST(t.$4 AS NUMBER) IS NULL;                       -- bad number\n",
    "```\n",
    "\n",
    "Notes:\n",
    "\n",
    "* When you query a stage directly, positional columns are `$1,$2,...`. You can reference `METADATA$FILENAME` and `METADATA$FILE_ROW_NUMBER` to know exactly which file/row the problem came from. ([Snowflake Documentation][6], [The Information Lab Nederland][7])\n",
    "\n",
    "### Export those bad rows to a file (optional)\n",
    "\n",
    "If you prefer to fix rows offline or send to a data-fixer team, write them to an internal stage:\n",
    "\n",
    "```sql\n",
    "-- create a named internal stage for error files (if needed)\n",
    "CREATE OR REPLACE STAGE retailx_error_stage;\n",
    "\n",
    "-- unload the bad rows (results) to files in that internal stage\n",
    "COPY INTO @retailx_error_stage/errors_\n",
    "FROM ( SELECT * FROM retailx_orders_bad_raw )\n",
    "FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ','  HEADER = TRUE);\n",
    "```\n",
    "\n",
    "Now you can `GET` these files from the internal stage, hand-fix them, re-stage them (to S3 or to the internal stage) and load them separately. `COPY INTO <location>` from a SELECT is supported. ([Snowflake Documentation][8])\n",
    "\n",
    "---\n",
    "\n",
    "# E — Step 4: Fix & re-load only the bad rows (safe ways)\n",
    "\n",
    "You have two strong options — pick based on scale and automation needs.\n",
    "\n",
    "## Option 1 — **Recommended for production**: Raw-landing → transform workflow (idempotent, easiest to reprocess)\n",
    "\n",
    "1. **Load everything** into a `raw` table with loose schema (all `VARIANT` or `VARCHAR`) so the COPY **never fails** due to type mismatch.\n",
    "\n",
    "   ```sql\n",
    "   CREATE OR REPLACE TABLE raw_orders_rawcols (\n",
    "     src_file STRING,\n",
    "     file_row_number NUMBER,\n",
    "     col1 STRING, col2 STRING, col3 STRING, col4 STRING,\n",
    "     ingested_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n",
    "   );\n",
    "   -- COPY INTO raw table (no type conversion) so errors are avoided or small:\n",
    "   COPY INTO raw_orders_rawcols\n",
    "     FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt')\n",
    "     FILE_FORMAT = (FORMAT_NAME='retailx_csv_fmt')\n",
    "     ON_ERROR = 'CONTINUE';\n",
    "   ```\n",
    "2. Use SQL (with `TRY_` functions) to validate/clean rows inside Snowflake, INSERT clean rows into `retailx_orders`, and write the unfixable rows into `retailx_orders_error` for manual remediation.\n",
    "\n",
    "   ```sql\n",
    "   INSERT INTO retailx_orders\n",
    "   SELECT\n",
    "     TRY_CAST(col1 AS INTEGER),\n",
    "     col2,\n",
    "     TRY_TO_TIMESTAMP(col3,'YYYY-MM-DD HH24:MI:SS'),\n",
    "     TRY_CAST(col4 AS NUMBER(10,2))\n",
    "   FROM raw_orders_rawcols\n",
    "   WHERE TRY_CAST(col1 AS INTEGER) IS NOT NULL\n",
    "     AND TRY_TO_TIMESTAMP(col3,'YYYY-MM-DD HH24:MI:SS') IS NOT NULL\n",
    "     AND TRY_CAST(col4 AS NUMBER) IS NOT NULL;\n",
    "\n",
    "   CREATE OR REPLACE TABLE retailx_orders_error AS\n",
    "   SELECT * FROM raw_orders_rawcols\n",
    "   WHERE NOT (TRY_CAST(col1 AS INTEGER) IS NOT NULL\n",
    "          AND TRY_TO_TIMESTAMP(col3,'YYYY-MM-DD HH24:MI:SS') IS NOT NULL\n",
    "          AND TRY_CAST(col4 AS NUMBER) IS NOT NULL);\n",
    "   ```\n",
    "\n",
    "**Why recommended:** no duplicate risk, easy reprocessing, easier to build automated cleaning transforms (SQL), and you keep raw immutable data for replay. (This is the most robust production pattern.)\n",
    "\n",
    "## Option 2 — Lightweight ad-hoc: fix raw bad files and re-stage\n",
    "\n",
    "1. Use the `retailx_orders_bad_raw` table from step D.2 or the exported CSV on `@retailx_error_stage`.\n",
    "2. Fix the CSV rows (either manually or with a script).\n",
    "3. Re-stage the corrected file with a new filename (so checksum changes), e.g. `orders_20250828_fixed.csv` in S3 or in internal stage.\n",
    "4. Run `COPY INTO retailx_orders FROM @retailx_orders_stage (FILES=('orders_20250828_fixed.csv'))` to load only that file.\n",
    "\n",
    "**Important:** do **not** re-run `COPY` on the original file without changing the filename OR without using `FORCE=TRUE` (but `FORCE=TRUE` will re-load the whole file and duplicate already-loaded good rows). So prefer creating a new corrected file or load fixed rows directly from an error table into the main table via `INSERT`. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "# F — Audit and monitoring (where to look for load details)\n",
    "\n",
    "* Use `TABLE(VALIDATE(table, JOB_ID => '<query_id>'))` or `TABLE(RESULT_SCAN(LAST_QUERY_ID()))` after validation loads to see row-level errors. ([Snowflake Documentation][5])\n",
    "* Use `INFORMATION_SCHEMA.LOAD_HISTORY` or the `COPY_HISTORY` table function to see file-level load results and counts for the last 14 days. Example:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM TABLE(COPY_HISTORY(DATEADD('day', -2, CURRENT_TIMESTAMP()), CURRENT_TIMESTAMP()))\n",
    "WHERE table_name = 'RETAILX_ORDERS'\n",
    "ORDER BY last_load_time DESC;\n",
    "```\n",
    "\n",
    "These show which files were processed, rows loaded, and error counts. ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "# G — Short FAQ (quick answers)\n",
    "\n",
    "Q — *Does `ON_ERROR = 'CONTINUE'` record the skipped row text?*\n",
    "A — It records error metadata (line/column/error) that you can get via `VALIDATE` or `VALIDATION_MODE`; but if you want the exact original row fields, best to query the stage (positional `$1,$2...`) with `TRY_` functions and capture the raw row into a table or stage. ([Snowflake Documentation][5])\n",
    "\n",
    "Q — *Can I automate this whole fix-and-reload?*\n",
    "A — Yes — wrap the steps in a stored procedure or Snowflake Task: 1) `VALIDATE` or `COPY` with `CONTINUE`, 2) `VALIDATE(...)` → store errors, 3) generate error file or error table, 4) run cleansing stored proc / external job to fix errors, 5) stage corrected files and `COPY` them in. You can also use Snowpipe for continuous loads and monitor error outputs similarly. ([Snowflake Documentation][9])\n",
    "\n",
    "Q — *Should I ever use `FORCE=TRUE` to reload the same file after fixing it?*\n",
    "A — Only if you’re sure you want to re-load **all** rows in that file (and deduplicate later). Prefer staging corrected rows as new files or using the raw-landing & SQL-cleanse approach to avoid duplicates. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "# H — Recommended production pattern (summary)\n",
    "\n",
    "* **Always** validate files first (especially the first time a feed runs). `VALIDATION_MODE='RETURN_ERRORS'` is low cost and prevents surprises. ([Snowflake Documentation][1])\n",
    "* **Prefer** landing raw data in a raw table (all strings/variants). Use idempotent transformation SQL to push clean data to final tables and write bad rows to an error table for human review. This is robust and easily automated.\n",
    "* If you must operate ad-hoc, **extract bad rows** using SELECT from stage with `TRY_` functions, write them to an internal stage or table, fix them, and reload only the corrected file(s).\n",
    "* Use `VALIDATE` / `RESULT_SCAN(LAST_QUERY_ID())` to capture error metadata and keep a persistent error log table for triage.\n",
    "\n",
    "---\n",
    "\n",
    "# References / docs (most relevant)\n",
    "\n",
    "* `COPY INTO <table>` (VALIDATION\\_MODE, ON\\_ERROR, PATTERN, PURGE). ([Snowflake Documentation][1])\n",
    "* `VALIDATE(table, JOB_ID => ...)` table function (returns all errors for a past COPY). ([Snowflake Documentation][5])\n",
    "* `RESULT_SCAN(LAST_QUERY_ID())` to capture resultsets of the last command. ([Snowflake Documentation][2])\n",
    "* `COPY_HISTORY` / load-history docs for auditing. ([Snowflake Documentation][4])\n",
    "* Querying staged file metadata (`METADATA$FILENAME`, `METADATA$FILE_ROW_NUMBER`). ([Snowflake Documentation][6])\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "[1]: https://docs.snowflake.com/en/sql-reference/sql/copy-into-table \"COPY INTO <table> | Snowflake Documentation\"\n",
    "[2]: https://docs.snowflake.com/en/sql-reference/functions/result_scan?utm_source=chatgpt.com \"RESULT_SCAN - Snowflake Documentation\"\n",
    "[3]: https://docs.snowflake.com/en/sql-reference/sql/copy-into-table?utm_source=chatgpt.com \"COPY INTO <table> | Snowflake Documentation\"\n",
    "[4]: https://docs.snowflake.com/en/sql-reference/functions/copy_history?utm_source=chatgpt.com \"COPY_HISTORY - Snowflake Documentation\"\n",
    "[5]: https://docs.snowflake.com/en/sql-reference/functions/validate \"VALIDATE | Snowflake Documentation\"\n",
    "[6]: https://docs.snowflake.com/en/user-guide/querying-stage?utm_source=chatgpt.com \"Querying Data in Staged Files - Snowflake Documentation\"\n",
    "[7]: https://www.theinformationlab.nl/2022/08/26/snowflake-skills-3-metadata/?utm_source=chatgpt.com \"Snowflake Skills #3 - Metadata - The Information Lab Nederland\"\n",
    "[8]: https://docs.snowflake.com/en/sql-reference/sql/copy-into-location?utm_source=chatgpt.com \"COPY INTO <location> - Snowflake Documentation\"\n",
    "[9]: https://docs.snowflake.com/en/user-guide/data-load-bulk-ts?utm_source=chatgpt.com \"Troubleshooting bulk data loads - Snowflake Documentation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f920ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
