{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f121870c",
   "metadata": {},
   "source": [
    "--- \n",
    "## — QAS scale factor (the “speed budget” knob)\n",
    "\n",
    "### What is it?\n",
    "\n",
    "Think of QAS as a pool of **serverless scan workers** Snowflake can borrow to turbo-scan and pre-filter data for your query. The **scale factor** is your *speed budget*: an **upper bound** on how much of that serverless horsepower a warehouse is allowed to lease, expressed as a multiplier of the warehouse’s own size/cost. Default is **8**; set **0** to remove the cap. You’re billed by the second, separately from the warehouse. ([Snowflake Documentation][1])\n",
    "\n",
    "> Example: a **MEDIUM** warehouse costs **4 credits/hour**. With a scale factor of **5**, QAS can spend up to an additional **20 credits/hour** *while it’s actively accelerating queries*. When no accelerated work is happening, that QAS spend is **0**. ([Snowflake Documentation][1])\n",
    "\n",
    "### The real purpose (what problem it solves)\n",
    "\n",
    "Some queries do huge scans but return a sliver of rows. Upsizing the warehouse speeds *everything*, even the lightweight queries. QAS instead **offloads the scan+filter heavy lifting** to serverless compute for only the queries that need it—reducing wall-clock time for the hogs and relieving pressure on your warehouse for everyone else. It often helps “outlier” queries so the rest of the workload runs smoother. ([Snowflake Documentation][2])\n",
    "\n",
    "### How it works (in plain English)\n",
    "\n",
    "When Snowflake detects an eligible scan with selective filters, it **splits the scan across many serverless workers**. Those workers read micro-partitions, apply the filters, and stream back only the rows your warehouse actually needs. The warehouse still does the remaining plan (joins/agg/sort), but the slowest part—**scanning and filtering**—is dramatically parallelized. QAS **uses only as many workers as needed and available**, bounded by your scale factor and service capacity. Results vary with availability; estimates assume the service can allocate the full amount. ([Snowflake Documentation][1])\n",
    "\n",
    "### What advantage does it bring?\n",
    "\n",
    "* **Faster long scans** without keeping a permanently bigger warehouse.\n",
    "* **Cost control**: you can cap spend with the scale factor; you pay per second only when QAS is actually used.\n",
    "* **Better “mixed workloads”**: outliers are offloaded so ordinary queries aren’t starved.\n",
    "* **Simple to try**: flip it on per warehouse; no code changes. ([Snowflake Documentation][2])\n",
    "\n",
    "### Quick enable (copy/paste)\n",
    "\n",
    "```sql\n",
    "-- Turn QAS on for a warehouse\n",
    "ALTER WAREHOUSE my_wh SET ENABLE_QUERY_ACCELERATION = TRUE;\n",
    "\n",
    "-- Give it a generous budget (remove cap)\n",
    "ALTER WAREHOUSE my_wh SET QUERY_ACCELERATION_MAX_SCALE_FACTOR = 0;\n",
    "\n",
    "-- Or set a sensible cap (e.g., 5x of the warehouse)\n",
    "ALTER WAREHOUSE my_wh SET QUERY_ACCELERATION_MAX_SCALE_FACTOR = 5;\n",
    "```\n",
    "\n",
    "(Enterprise Edition or higher required.) ([Snowflake Documentation][2])\n",
    "\n",
    "### A realistic scenario (feel the pain → see the fix)\n",
    "\n",
    "**Story:** Marketing fires a SQL that scans **12 TB** of events to answer “users who did A then B in a week.” On a MEDIUM warehouse, it takes \\~**30 minutes**. You enable QAS with **scale factor = 8**. The exact same query drops to \\~**7–10 minutes** because QAS parallelizes the **TableScan** nodes and ships back only the rows matching the selective filters. The warehouse finishes the joins/aggregations faster because it’s fed fewer rows. You didn’t touch the SQL, didn’t resize the warehouse, and teammates’ small queries stop feeling sluggish because the hog’s scan moved off the warehouse. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "# Part 2 — Queries that are eligible for QAS (and how to check)\n",
    "\n",
    "### The big idea\n",
    "\n",
    "QAS focuses on **scan and filter** stages. Eligible commands include **SELECT, INSERT, CTAS, and COPY INTO <table>**. Within a supported command, QAS might accelerate the whole statement or just a subquery/clause if that part is eligible. In practice, you’ll see QAS attached to **TableScan** operators in the Query Profile. ([Snowflake Documentation][1])\n",
    "\n",
    "> Note: Snowflake **expanded** INSERT support—previously only the scan part of `INSERT … SELECT` could be accelerated; now **all portions of eligible INSERTs** can be accelerated. ([Snowflake Documentation][3])\n",
    "\n",
    "### What makes a query eligible?\n",
    "\n",
    "A query (or a part of it) is typically eligible when **there’s enough parallelizable scan work** and **filters are selective**. Common reasons **not** eligible:\n",
    "\n",
    "* **Not enough partitions to scan** (too little scan work to offset QAS overhead).\n",
    "* **Low selectivity filters** or **high-cardinality GROUP BY** that kills the benefit.\n",
    "* **LIMIT without ORDER BY** (nondeterministic result handling).\n",
    "* Use of **nondeterministic functions** (e.g., `RANDOM()`, `SEQ`) in ways that block acceleration.\n",
    "  These rules evolve, but that’s the gist today. ([Snowflake Documentation][1])\n",
    "\n",
    "### How to check eligibility (two reliable tools)\n",
    "\n",
    "**1) Ask Snowflake about a specific past query (last 14 days):**\n",
    "\n",
    "```sql\n",
    "-- Returns JSON with status and estimated times at several scale factors\n",
    "SELECT PARSE_JSON(SYSTEM$ESTIMATE_QUERY_ACCELERATION('<query_id>'));\n",
    "```\n",
    "\n",
    "Look for `\"status\": \"eligible\"` and `\"estimatedQueryTimes\"`, plus `\"upperLimitScaleFactor\"`. If it says `\"ineligible\"`, QAS won’t help that query as written. (Works only for queries executed in the last **14 days**.) ([Snowflake Documentation][4])\n",
    "\n",
    "**2) Hunt across history for best candidates:**\n",
    "\n",
    "```sql\n",
    "-- Which queries would benefit most (by time eligible for acceleration)?\n",
    "SELECT query_id, eligible_query_acceleration_time\n",
    "FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ACCELERATION_ELIGIBLE\n",
    "WHERE start_time > DATEADD('day', -7, CURRENT_TIMESTAMP())\n",
    "ORDER BY eligible_query_acceleration_time DESC;\n",
    "\n",
    "-- Which warehouses have the most eligible time?\n",
    "SELECT warehouse_name, SUM(eligible_query_acceleration_time) AS total_eligible_time\n",
    "FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ACCELERATION_ELIGIBLE\n",
    "WHERE start_time > DATEADD('day', -7, CURRENT_TIMESTAMP())\n",
    "GROUP BY warehouse_name\n",
    "ORDER BY total_eligible_time DESC;\n",
    "\n",
    "-- What “upper limit” scale factor Snowflake would consider for this warehouse?\n",
    "SELECT MAX(upper_limit_scale_factor)\n",
    "FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ACCELERATION_ELIGIBLE\n",
    "WHERE warehouse_name = 'MY_WH'\n",
    "  AND start_time > DATEADD('day', -7, CURRENT_TIMESTAMP());\n",
    "```\n",
    "\n",
    "These views show *how much* of a query’s time is eligible and what **upper limit scale factor** the service would consider. ([Snowflake Documentation][1])\n",
    "\n",
    "### “Only fetch and filtering operations are executed on QAS” — let’s be precise\n",
    "\n",
    "* **Mostly true for SELECT/CTAS/COPY**: QAS accelerates **scan/filter** work (you’ll see **TableScan** nodes with “Query Acceleration” stats in the profile). Joins, aggregations, sorts are still executed by your warehouse. ([Snowflake Documentation][1])\n",
    "* **INSERT got broader**: as of a 2024 change, **all portions of eligible INSERT** statements can be accelerated, not just the scan part of `INSERT … SELECT`. That’s a nuance worth remembering. ([Snowflake Documentation][3])\n",
    "\n",
    "### How to confirm QAS actually helped a run (after you enable it)\n",
    "\n",
    "* **Query Profile → Overview**: look for “**Query Acceleration**” stats (e.g., *Partitions scanned by service*, *Scans selected for acceleration*).\n",
    "* **Account Usage → QUERY\\_HISTORY**: the columns\n",
    "  `QUERY_ACCELERATION_BYTES_SCANNED`,\n",
    "  `QUERY_ACCELERATION_PARTITIONS_SCANNED`, and\n",
    "  `QUERY_ACCELERATION_UPPER_LIMIT_SCALE_FACTOR`\n",
    "  will be > 0 for accelerated queries. (Note: bytes/partitions can appear *higher* because of intermediary results QAS creates—that’s expected.) ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "## Putting it together: a 10-minute, safe experiment\n",
    "\n",
    "1. **Find a candidate:**\n",
    "\n",
    "```sql\n",
    "SELECT query_id\n",
    "FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ACCELERATION_ELIGIBLE\n",
    "WHERE start_time > DATEADD('day', -7, CURRENT_TIMESTAMP())\n",
    "ORDER BY eligible_query_acceleration_time DESC\n",
    "LIMIT 1;\n",
    "```\n",
    "\n",
    "2. **Estimate the benefit:**\n",
    "\n",
    "```sql\n",
    "SELECT PARSE_JSON(SYSTEM$ESTIMATE_QUERY_ACCELERATION('<that_query_id>'));\n",
    "```\n",
    "\n",
    "3. **Enable QAS on your dev/test warehouse and run once more:**\n",
    "\n",
    "```sql\n",
    "ALTER WAREHOUSE dev_wh SET ENABLE_QUERY_ACCELERATION = TRUE, QUERY_ACCELERATION_MAX_SCALE_FACTOR = 5;\n",
    "```\n",
    "\n",
    "4. **Verify effect (either place):**\n",
    "\n",
    "* Query Profile → see “Query Acceleration” numbers.\n",
    "* `SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY` → the QAS columns > 0. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "## Gotchas & pro tips \n",
    "\n",
    "* **Not a concurrency feature:** Scale factor ≠ “more slots.” QAS speeds *parts of eligible queries*; **queues** still depend on warehouse size and multi-cluster settings. Use both if you need throughput *and* faster scans. ([Snowflake Documentation][1])\n",
    "* **Set a cap first:** Start with **2–5**, measure, then raise. **0** is for “go fast, we’ll pay what it takes”. ([Snowflake Documentation][1])\n",
    "* **Eligibility evolves:** Snowflake keeps expanding patterns it can accelerate; re-check periodically. ([Snowflake Documentation][1])\n",
    "* **Enterprise edition required.** ([Snowflake Documentation][2])\n",
    "\n",
    "---\n",
    "\n",
    "## Practice questions (to make sure it sticks)\n",
    "\n",
    "1. Explain—in one minute—**what the QAS scale factor** controls and how **cost** is bounded when set to 5 on a MEDIUM warehouse. (Give the math.) ([Snowflake Documentation][1])\n",
    "2. Name **four reasons** a query may be **ineligible** for QAS and how you’d rewrite or index/cluster to improve eligibility. ([Snowflake Documentation][1])\n",
    "3. Walk through how you would use **`SYSTEM$ESTIMATE_QUERY_ACCELERATION`** and **`QUERY_ACCELERATION_ELIGIBLE`** to pick a scale factor for a new warehouse. What does `\"upperLimitScaleFactor\"` mean? ([Snowflake Documentation][4])\n",
    "4. In the **Query Profile**, where do you see that QAS was used, and what metrics prove it? What does it mean if total bytes scanned are *higher* than without QAS? ([Snowflake Documentation][1])\n",
    "5. Clarify this statement: “QAS accelerates **only** scan/filter stages.” When is that accurate, and what changed for **INSERT** statements? ([Snowflake Documentation][1])\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cedd596",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Practice Questions for QAS**\n",
    "\n",
    "### **1. When would you enable Query Acceleration Service (QAS) for a warehouse?**\n",
    "\n",
    "👉 **Answer:**\n",
    "I would enable QAS when my queries are **long-running scan-heavy workloads** where users often experience queueing or slow response time.\n",
    "\n",
    "Example:\n",
    "Suppose my analytics team runs **ad-hoc queries** on a 50 TB sales table. A typical query might filter last 2 years of data with some joins. Even with a `LARGE` warehouse, the response time is 2–3 minutes. Here, scaling up vertically (to XL or 2XL) doesn’t help much, because Snowflake warehouse scaling increases compute power but **still distributes data scan across the same set of nodes**.\n",
    "\n",
    "By enabling QAS, Snowflake automatically creates **temporary micro-clusters** dedicated for data scan and filtering. The warehouse doesn’t get overloaded and queries complete faster.\n",
    "\n",
    "So the **trigger condition** is:\n",
    "\n",
    "* Heavy scans (billions of rows, wide fact tables)\n",
    "* When multi-cluster scaling doesn’t help because it only solves queueing, not execution time\n",
    "* When SLA-sensitive queries need faster performance without constantly running an oversized warehouse\n",
    "\n",
    "---\n",
    "\n",
    "### **2. How does QAS differ from Multi-cluster scaling?**\n",
    "\n",
    "👉 **Answer:**\n",
    "\n",
    "* **Multi-cluster scaling (horizontal scaling):**\n",
    "  Solves **queueing issues**. If 50 users submit queries at once, Snowflake spawns new clusters so queries run in parallel. But each query still takes the same amount of time.\n",
    "\n",
    "* **Query Acceleration Service (QAS):**\n",
    "  Solves **long-running query execution issues**. Instead of queueing, here a single query gets divided into smaller tasks. QAS spins up **short-lived compute resources** that execute scan and filtering in parallel with the warehouse, reducing latency.\n",
    "\n",
    "Think of it this way:\n",
    "\n",
    "* Multi-cluster = **more checkout counters at a supermarket** (reduces queue)\n",
    "* QAS = **more staff helping you inside the store to find products quickly** (reduces time per transaction).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What is the Query Acceleration Scale Factor?**\n",
    "\n",
    "👉 **Answer:**\n",
    "The **scale factor** is a **configuration parameter** that controls **how much additional compute QAS can use** relative to the warehouse size.\n",
    "\n",
    "* Example: Scale Factor = `8` on a `MEDIUM` warehouse\n",
    "  → Snowflake can use up to 8× more temporary QAS compute for acceleration.\n",
    "* This ensures you don’t accidentally spend unlimited credits.\n",
    "\n",
    "**Purpose:**\n",
    "It prevents runaway costs and allows you to cap QAS resources. Without it, a query might keep consuming QAS compute and blow up cost.\n",
    "\n",
    "**Real-life scenario:**\n",
    "Imagine a finance department running **complex historical queries** at month-end. Normally, these queries would take 30–40 minutes. With QAS scale factor = 8, the same queries finish in \\~8–10 minutes. Finance team delivers reports before the business meeting.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Which queries are eligible for QAS?**\n",
    "\n",
    "👉 **Answer:**\n",
    "Snowflake has strict eligibility rules. QAS is designed only for **scan and filter-heavy queries**. Specifically:\n",
    "\n",
    "✅ Eligible:\n",
    "\n",
    "* Queries where a **large amount of raw data must be scanned and filtered** before returning a small result set\n",
    "* Queries where warehouse compute becomes bottlenecked on scanning, not CPU for joins/aggregations\n",
    "\n",
    "❌ Not Eligible:\n",
    "\n",
    "* Queries that are **join-heavy** or **aggregation-heavy** (e.g., `GROUP BY`, `JOIN`, `WINDOW FUNCTIONS`)\n",
    "* Queries that are already short-running (<1 sec)\n",
    "\n",
    "**Checking eligibility in Snowflake:**\n",
    "\n",
    "* Use `SYSTEM$ESTIMATE_QUERY_ACCELERATION(query_id)` → tells how much faster a query would run with QAS\n",
    "* Use `QUERY_ACCELERATION_ELIGIBLE` in `QUERY_HISTORY` → shows whether a query qualified\n",
    "\n",
    "**Example:**\n",
    "Suppose I run:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM sales\n",
    "WHERE region = 'APAC' AND order_date > '2022-01-01';\n",
    "```\n",
    "\n",
    "* The query scans 20 TB of sales data but returns only 200 MB.\n",
    "* Eligible for QAS → filtering/scanning can be offloaded.\n",
    "\n",
    "But if I run:\n",
    "\n",
    "```sql\n",
    "SELECT region, SUM(revenue)\n",
    "FROM sales\n",
    "GROUP BY region;\n",
    "```\n",
    "\n",
    "* Heavy aggregation, not just scanning. QAS won’t help here.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. What parts of query execution are accelerated by QAS?**\n",
    "\n",
    "👉 **Answer:**\n",
    "QAS **only accelerates scan + filter operations**. That means:\n",
    "\n",
    "* Reading rows from storage\n",
    "* Applying WHERE clause filters\n",
    "* Projecting columns\n",
    "\n",
    "But **NOT accelerated**:\n",
    "\n",
    "* Joins\n",
    "* Aggregations (`SUM`, `COUNT`, `GROUP BY`)\n",
    "* Window functions\n",
    "* Sorting\n",
    "\n",
    "**Why?** Because QAS is built to parallelize I/O-heavy workloads. Once data is filtered down, the warehouse nodes still do the heavy lifting for joins and aggregations.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Query scans 50 TB → filters down to 500 GB → QAS parallelizes this scan\n",
    "* After filtering, warehouse does join/aggregate on the reduced dataset\n",
    "\n",
    "So QAS acts as a **turbocharger for data scanning**, not a replacement for the warehouse compute engine.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7bada",
   "metadata": {},
   "source": [
    "## HowQAS works under the hood?\n",
    "---\n",
    "\n",
    "# scene: “month-end crunch at Streamly”\n",
    "\n",
    "* table: `FACT_EVENTS` (petabyte-scale clickstream)\n",
    "* layout: natural Snowflake micro-partitions (created as data lands), plus light clustering on `(event_date, country)`\n",
    "* shape: \\~9 months of data, \\~35 billion rows, \\~600 TB compressed\n",
    "* typical query (slow one): product asks for “active users who did A then B within 15 minutes last 30 days, country = 'IN'”, feeding a dashboard tile\n",
    "\n",
    "```sql\n",
    "-- the slow query\n",
    "WITH a AS (\n",
    "  SELECT user_id, event_ts\n",
    "  FROM FACT_EVENTS\n",
    "  WHERE event_name = 'A'\n",
    "    AND event_date >= DATEADD(day, -30, CURRENT_DATE())\n",
    "    AND country = 'IN'\n",
    "),\n",
    "b AS (\n",
    "  SELECT user_id, event_ts\n",
    "  FROM FACT_EVENTS\n",
    "  WHERE event_name = 'B'\n",
    "    AND event_date >= DATEADD(day, -30, CURRENT_DATE())\n",
    "    AND country = 'IN'\n",
    ")\n",
    "SELECT COUNT(DISTINCT a.user_id)\n",
    "FROM a\n",
    "JOIN b\n",
    "  ON a.user_id = b.user_id\n",
    " AND b.event_ts BETWEEN a.event_ts AND a.event_ts + INTERVAL '15 minutes';\n",
    "```\n",
    "\n",
    "On a **MEDIUM** warehouse this takes \\~20–25 minutes when traffic spikes. You enable QAS with a modest cap:\n",
    "\n",
    "```sql\n",
    "ALTER WAREHOUSE ANALYTICS_WH\n",
    "  SET ENABLE_QUERY_ACCELERATION = TRUE,\n",
    "      QUERY_ACCELERATION_MAX_SCALE_FACTOR = 4;  -- “speed budget” cap\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# how QAS works — step by step (what actually happens)\n",
    "\n",
    "## 0) quick mental model\n",
    "\n",
    "* **warehouse** = the conductor and main band (does planning, joins, aggs, final stages).\n",
    "* **QAS** = a temporary brass section Snowflake spins up *only* to blast through **TableScan + WHERE** work, then disappears.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) compile & prune (warehouse)\n",
    "\n",
    "* the planner builds a graph like:\n",
    "\n",
    "```\n",
    "TableScan(FACT_EVENTS) → Filter (date, country, event_name) → Project (user_id, event_ts)\n",
    "            ↘ (same again for subquery b)\n",
    "                    → Join on user_id/time-window\n",
    "                    → Aggregate COUNT DISTINCT\n",
    "```\n",
    "\n",
    "* **metadata pruning** removes micro-partitions that are obviously irrelevant (older than 30 days, wrong country, wrong event). this already saves a ton.\n",
    "* what’s left is still **a very large set of micro-partitions** (MPs) because India traffic is huge—classic **data skew**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) eligibility check (warehouse)\n",
    "\n",
    "* the planner marks each **TableScan** with “acceleration potential”:\n",
    "\n",
    "  * big remaining scan? ✅\n",
    "  * selective filters? (`country='IN'`, `event='A'`/`'B'`, `event_date last 30 days`) ✅\n",
    "  * enough MPs to parallelize? ✅\n",
    "    → **eligible**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) choose a “speed budget” (warehouse + QAS control plane)\n",
    "\n",
    "* the service considers:\n",
    "\n",
    "  * your **scale factor cap** (=4)\n",
    "  * an internal **upper-limit** for this query (based on partitioning & selectivity)\n",
    "  * current service availability\n",
    "* result is a target parallelism for helper workers (say **\\~3.5×** the warehouse, bounded at **4×**).\n",
    "\n",
    "> this doesn’t make the warehouse “stronger”; it just adds **more parallel scan+filter workers** elsewhere so your band isn’t stuck reading the whole library alone.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) slice the remaining scan into **subtasks** (QAS)\n",
    "\n",
    "* QAS groups the eligible micro-partitions into **scan chunks**.\n",
    "* think of micro-partitions like pages in many boxes:\n",
    "\n",
    "```\n",
    "Remaining MPs after pruning (illustrative):\n",
    "\n",
    "[MP_101 ... MP_150]  (event_date D-30..D-25, country IN)\n",
    "[MP_201 ... MP_480]  (D-24..D-10, country IN)  <-- heavy zone (campaign spike)\n",
    "[MP_481 ... MP_620]  (D-9..D,    country IN)\n",
    "```\n",
    "\n",
    "* QAS carves these into **balanced bundles** (subtasks). heavy zones get split **more finely** to avoid stragglers:\n",
    "\n",
    "```\n",
    "Subtasks:\n",
    "T1: MP_101..115\n",
    "T2: MP_116..130\n",
    "T3: MP_131..150\n",
    "T4: MP_201..230\n",
    "T5: MP_231..260\n",
    "T6: MP_261..290\n",
    "...\n",
    "T18: MP_451..480\n",
    "T19: MP_481..545\n",
    "T20: MP_546..620\n",
    "```\n",
    "\n",
    "* subtasks are sized so they’re **roughly equal wall time**, not equal count. if QAS notices a task lagging, it **splits it again** and hands the remainder to another helper (anti-straggler).\n",
    "\n",
    "---\n",
    "\n",
    "## 5) offload & stream back (QAS workers)\n",
    "\n",
    "for each subtask `Ti`, a serverless worker:\n",
    "\n",
    "1. **reads** the assigned micro-partitions directly from storage.\n",
    "2. **applies your WHERE filters** (`event_name`, `country`, `date`).\n",
    "3. **projects** only needed columns (`user_id`, `event_ts`).\n",
    "4. **streams** the filtered rows back toward the warehouse.\n",
    "\n",
    "ASCII view:\n",
    "\n",
    "```\n",
    "       QAS worker A  ← T4, T5, T6 (partial)\n",
    "       QAS worker B  ← T7, T8\n",
    "Warehouse  ←←← filtered rows (user_id, event_ts) ← QAS worker C  ← T9\n",
    "       QAS worker D  ← T10 (heavy → split into T10a/T10b on the fly)\n",
    "```\n",
    "\n",
    "* **no joins/aggregations** happen in QAS; it’s just turbo-scanning and filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) warehouse keeps the “brain” work\n",
    "\n",
    "* while QAS streams rows, the warehouse:\n",
    "\n",
    "  * buffers them,\n",
    "  * executes the **JOIN** (time-window) between `a` and `b`,\n",
    "  * runs the **COUNT DISTINCT**,\n",
    "  * returns results.\n",
    "\n",
    "The **join/agg stages run earlier and faster** because rows arrive **pre-filtered** and **much sooner** than if the warehouse had to do all scans itself.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) dynamic balancing & straggler control (during the run)\n",
    "\n",
    "* if worker **D** is stuck on a fat subtask (e.g., a day with a cricket final!), QAS splits it:\n",
    "\n",
    "  * `T10` → `T10a` + `T10b`\n",
    "  * assigns `T10b` to another idle worker\n",
    "* this keeps the **longest tail** short and avoids the “one last partition” syndrome.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) wrap-up & teardown\n",
    "\n",
    "* when all scan subtasks finish, QAS workers vanish.\n",
    "* you pay **only for seconds used**.\n",
    "* the Query Profile now shows a **“Query Acceleration”** section on the `TableScan` nodes (e.g., *Partitions scanned by service*, *Bytes scanned by service*).\n",
    "\n",
    "---\n",
    "\n",
    "# the same story as a numbered flow (quick recap)\n",
    "\n",
    "1. query arrives → plan built\n",
    "2. metadata pruning removes obvious MPs\n",
    "3. planner flags TableScan as **eligible**\n",
    "4. QAS decides **parallelism** bounded by your **scale factor**\n",
    "5. QAS splits remaining MPs into **balanced subtasks**\n",
    "6. serverless workers **read + filter + project** and **stream back**\n",
    "7. warehouse **joins/aggregates** on a much smaller, earlier stream\n",
    "8. QAS **tears down**, you see acceleration stats & separate serverless cost\n",
    "\n",
    "---\n",
    "\n",
    "# tiny lab you can run (safe & concrete)\n",
    "\n",
    "> goal: feel QAS decisions and see subtasks in the profile (you’ll see acceleration on the `TableScan`).\n",
    "\n",
    "```sql\n",
    "-- 1) pick a warehouse & turn on QAS with a safe cap\n",
    "ALTER WAREHOUSE ANALYTICS_WH\n",
    "  SET ENABLE_QUERY_ACCELERATION = TRUE,\n",
    "      QUERY_ACCELERATION_MAX_SCALE_FACTOR = 3;\n",
    "\n",
    "-- 2) run your slow query once; note query_id\n",
    "-- (from history or UI)\n",
    "\n",
    "-- 3) ask Snowflake how much QAS *would* help that exact run\n",
    "SELECT PARSE_JSON(SYSTEM$ESTIMATE_QUERY_ACCELERATION('<your_query_id>')) AS est;\n",
    "\n",
    "-- look for:\n",
    "-- est:status = 'eligible'\n",
    "-- est:originalQueryTime vs est:estimatedQueryTimes: { \"1\": ..., \"2\": ..., \"3\": ... }\n",
    "-- est:upperLimitScaleFactor\n",
    "\n",
    "-- 4) re-run the query and open Query Profile\n",
    "-- In the TableScan node(s), check:\n",
    "-- - Scans selected for acceleration\n",
    "-- - Partitions scanned by service\n",
    "-- - Bytes scanned by service\n",
    "\n",
    "-- 5) optional: see numbers in history\n",
    "SELECT query_id,\n",
    "       query_acceleration_partitions_scanned,\n",
    "       query_acceleration_bytes_scanned,\n",
    "       query_acceleration_upper_limit_scale_factor\n",
    "FROM   SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\n",
    "WHERE  query_id = '<your_query_id>';\n",
    "```\n",
    "\n",
    "**What you’ll observe:**\n",
    "\n",
    "* with QAS **off**: TableScan duration dominates.\n",
    "* with QAS **on**: TableScan duration shrinks; you’ll see **partitions/bytes “by service”** > 0, and overall elapsed time drops. on busy days, you’ll notice **more subtasks** (more partitions “by service”).\n",
    "\n",
    "---\n",
    "\n",
    "# important clarifications (common traps)\n",
    "\n",
    "* “QAS makes my warehouse faster.”\n",
    "  → not exactly. the warehouse doesn’t get stronger; **scan work moves off** to serverless helpers, so **end-to-end time** improves and the warehouse has more headroom for joins/agg.\n",
    "\n",
    "* “QAS is for queues.”\n",
    "  → queues are about **concurrency**; fix with **multi-cluster**. QAS is about **long scan/filters**.\n",
    "\n",
    "* “Only fetch/filter run on QAS.”\n",
    "  → correct for SELECT/CTAS/COPY: it’s **TableScan + WHERE + projection**. the **join/agg** stages are warehouse work.\n",
    "\n",
    "---\n",
    "\n",
    "# a second mini-scenario (to cement it)\n",
    "\n",
    "You also run:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  DATE_TRUNC('day', event_ts) AS d,\n",
    "  COUNT(*) AS clicks\n",
    "FROM FACT_EVENTS\n",
    "WHERE event_date >= DATEADD(day, -7, CURRENT_DATE())\n",
    "  AND country IN ('IN','BD','PK')\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "```\n",
    "\n",
    "* **why QAS helps**: huge 7-day scan; filters are selective; aggregation is cheap once rows are filtered.\n",
    "* **how subtasks form**: QAS splits the remaining MPs by day & country hot spots (e.g., Sunday traffic), balances them, streams filtered rows; warehouse groups by day quickly.\n",
    "* **what you see**: TableScan shows many partitions “by service”; total time shrinks from, say, 6m → 2m on the same warehouse.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50ed76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
