{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8678b77f",
   "metadata": {},
   "source": [
    "# Snowflake Stages ‚Äî the practical, story-driven deep dive (with commands you‚Äôll actually use)\n",
    "\n",
    "Imagine you just joined a fintech where raw files land in three places: a vendor‚Äôs S3 bucket, your analysts‚Äô laptops, and an internal app that exports JSON. Your job is to make all three reliable and repeatable. In Snowflake, **stages** are your ‚Äúloading docks‚Äù for files; **file formats** are the ‚Äúinstructions‚Äù for how to read/write those files. Put them together and your pipelines become boring‚Äîin the best way.\n",
    "\n",
    "Below is a step-by-step guide that covers the fundamentals, goes deep where it matters, fixes common misconceptions, and gives you runnable examples.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) What a Stage is (and isn‚Äôt)\n",
    "\n",
    "* A **stage** is a Snowflake object that points to a location to **load from** or **unload to** (internal Snowflake storage or an external bucket). You reference it with `@...` in SQL. \n",
    "* Types you‚Äôll use:\n",
    "\n",
    "  * **User stage** `@~` (personal scratch space).\n",
    "  * **Table stage** `@%table_name` (tightly coupled to one table).\n",
    "  * **Named stage** `@db.schema.stage` (shared, governed, best for production). \n",
    "* Internal vs External:\n",
    "\n",
    "  * **Internal stage**: storage fully managed by Snowflake.\n",
    "  * **External stage**: pointer to S3 / Azure Blob / GCS, usually via a **storage integration** (preferred over embedding keys). \n",
    "> ‚úÖ Mental model\n",
    "> Stage = **where** files live + some defaults.\n",
    "> File format = **how** to interpret files (CSV/JSON/Parquet options, etc.). (Details in ¬ß4‚Äì5.)\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Fixing a common naming misconception\n",
    "\n",
    "You wrote:\n",
    "`DESC stage [db_name].external_stages.[stage_name] / [db_name].internal_stages.[stage_name]`\n",
    "\n",
    "In Snowflake, a fully-qualified stage name is **`<db>.<schema>.<stage_name>`**. Schemas like `external_stages` or `internal_stages` are just *conventions* you might adopt‚Äînot special namespaces. So you‚Äôd run, for example:\n",
    "\n",
    "```sql\n",
    "DESC STAGE analytics_stg.raw_landing.vendor_drop;\n",
    "```\n",
    "\n",
    "This works the same whether `vendor_drop` is internal or external. \n",
    "\n",
    "---\n",
    "\n",
    "## 3) The handful of stage commands you‚Äôll use daily\n",
    "\n",
    "### 3.1 Describe a stage (what‚Äôs configured there?)\n",
    "\n",
    "```sql\n",
    "DESC STAGE analytics_stg.raw_landing.vendor_drop;\n",
    "```\n",
    "\n",
    "This returns key properties like `URL` (for external), `STAGE_TYPE` (INTERNAL/EXTERNAL), any embedded `FILE_FORMAT` defaults, `ENCRYPTION`, `STORAGE_INTEGRATION`, and directory settings.\n",
    "\n",
    "### 3.2 Alter a stage (change URL, file format, integration, directory, etc.)\n",
    "\n",
    "```sql\n",
    "-- Point an external stage to a new prefix and switch to storage integration\n",
    "ALTER STAGE analytics_stg.raw_landing.vendor_drop\n",
    "  SET URL='s3://partner-bucket/new_prefix/'\n",
    "    STORAGE_INTEGRATION = partner_si;\n",
    "\n",
    "-- Attach or change default parsing rules for files that use this stage\n",
    "ALTER STAGE analytics_stg.raw_landing.vendor_drop\n",
    "  SET FILE_FORMAT = ( TYPE = CSV FIELD_DELIMITER='|' SKIP_HEADER=1 );\n",
    "\n",
    "-- Enable/disable stage directory table metadata (useful for listing/querying files)\n",
    "ALTER STAGE analytics_stg.raw_landing.vendor_drop\n",
    "  SET DIRECTORY = ( ENABLE = TRUE );\n",
    "```\n",
    "\n",
    "Valid attributes include `URL`, `CREDENTIALS` (legacy), `STORAGE_INTEGRATION` (recommended), `FILE_FORMAT`, `ENCRYPTION`, `DIRECTORY`, and more.\n",
    "\n",
    "### 3.3 List files in a stage\n",
    "\n",
    "```sql\n",
    "LIST @analytics_stg.raw_landing.vendor_drop;         -- named stage\n",
    "LIST @~;                                             -- current user's stage\n",
    "LIST @%orders;                                       -- table stage for ORDERS\n",
    "```\n",
    "\n",
    "Handy for sanity checks and scripting. Tip: wrap names with spaces like `LIST '@%\"Cars (Sedan)\"';`. \n",
    "\n",
    "### 3.4 Query staged files *before* loading (great for debugging)\n",
    "\n",
    "```sql\n",
    "SELECT $1, $2::NUMBER, $3::TIMESTAMP_NTZ\n",
    "FROM @analytics_stg.raw_landing.vendor_drop/orders/\n",
    "  ( FILE_FORMAT => 'ff_csv_pipe' );\n",
    "```\n",
    "\n",
    "Use this to preview parsing and types. \n",
    "\n",
    "---\n",
    "\n",
    "## 4) File formats ‚Äî your parsing/playback settings\n",
    "\n",
    "A **file format** tells Snowflake how to read/write a file type. You can define them **inline** (e.g., `FILE_FORMAT=(TYPE=CSV ...)`) or as **named objects** you reuse. Supported types: CSV, JSON, AVRO, ORC, PARQUET, XML. [8])\n",
    "\n",
    "### 4.1 Create/describe a file format\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT util.ff_csv_pipe\n",
    "  TYPE = CSV\n",
    "  FIELD_DELIMITER = '|'\n",
    "  SKIP_HEADER = 1\n",
    "  EMPTY_FIELD_AS_NULL = TRUE\n",
    "  NULL_IF = ('\\\\N','NULL')\n",
    "  TIMESTAMP_FORMAT = 'AUTO';\n",
    "\n",
    "DESC FILE FORMAT util.ff_csv_pipe;\n",
    "```\n",
    "\n",
    "`DESC FILE FORMAT` shows all effective options‚Äîgreat for audits and handoffs.\n",
    "\n",
    "### 4.2 High-value CSV options (what you‚Äôll tune most)\n",
    "\n",
    "* `FIELD_DELIMITER`, `SKIP_HEADER`, `FIELD_OPTIONALLY_ENCLOSED_BY`, `ESCAPE_UNENCLOSED_FIELD`, `TRIM_SPACE`, `NULL_IF`, `EMPTY_FIELD_AS_NULL`, `DATE/TIME/TIMESTAMP_FORMAT`, `ENCODING`, `COMPRESSION`.\n",
    "* Watchouts:\n",
    "\n",
    "  * `PARSE_HEADER=TRUE` vs `SKIP_HEADER` can‚Äôt be combined as you might expect‚Äîpick one pattern. [9], [Medium][10])\n",
    "\n",
    "### 4.3 JSON essentials\n",
    "\n",
    "* `STRIP_OUTER_ARRAY` (explode `[...]` into multiple rows), `ALLOW_DUPLICATE`, `STRIP_NULL_VALUES`, `IGNORE_UTF8_ERRORS`. These often fix ‚Äúall data in one row‚Äù or encoding issues.\n",
    "\n",
    "### 4.4 Parquet tips\n",
    "\n",
    "* Usually fastest to ingest; Snowflake‚Äôs newer vectorized scanner improved Parquet ingest efficiency notably. Keep the defaults unless you *must* tweak. \n",
    "\n",
    "---\n",
    "\n",
    "## 5) Stage object properties ‚Äî what matters most (and why)\n",
    "\n",
    "When you `DESC STAGE`, focus on:\n",
    "\n",
    "* `STAGE_TYPE` (INTERNAL/EXTERNAL)\n",
    "* `STAGE_LOCATION` (`URL` for external; blank for internal)\n",
    "* `STORAGE_INTEGRATION` (preferred over embedding `CREDENTIALS`)\n",
    "* `FILE_FORMAT` (defaults applied when you don‚Äôt specify one in `COPY`)\n",
    "* `ENCRYPTION` (how the cloud provider encrypts at rest / KMS settings)\n",
    "* `DIRECTORY` (whether a **stage directory table** is enabled; lets you `SELECT * FROM DIRECTORY(@stage)` to list paths, sizes, timestamps). [3])\n",
    "\n",
    "> **Directory tables, in practice**\n",
    "> Enable with `DIRECTORY=(ENABLE=TRUE)`, then:\n",
    ">\n",
    "> ```sql\n",
    "> SELECT * FROM DIRECTORY(@analytics_stg.raw_landing.vendor_drop);\n",
    "> ```\n",
    ">\n",
    "> Useful for audits, ‚Äúwhat‚Äôs landed?‚Äù monitoring, and building idempotent loaders. Works with both internal and external stages.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) File format vs Stage ‚Äî how they differ\n",
    "\n",
    "| Aspect     | Stage                                                  | File format                     |\n",
    "| ---------- | ------------------------------------------------------ | ------------------------------- |\n",
    "| Purpose    | **Where** files are                                    | **How** to parse/write files    |\n",
    "| Scope      | `@...` locations (internal or external)                | CSV/JSON/Parquet/‚Ä¶ options      |\n",
    "| Security   | `USAGE/READ/WRITE` grants                              | `USAGE` grant                   |\n",
    "| Defaults   | Can embed a `FILE_FORMAT` (and encryption)             | No location; only parsing rules |\n",
    "| Precedence | `COPY` option > Stage‚Äôs `FILE_FORMAT` > Table defaults | N/A                             |\n",
    "\n",
    "Privileges on stages: grant **USAGE** to reference, **READ** to download (GET), **WRITE** to upload/remove (PUT/REMOVE). \n",
    "\n",
    "---\n",
    "\n",
    "## 7) ‚ÄúWhy can‚Äôt I run PUT/GET in the web UI?‚Äù\n",
    "\n",
    "* **Correct:** You cannot execute `PUT`/`GET` from Snowsight worksheets; they require client access to your local filesystem. Use **SnowSQL**, JDBC/ODBC, or Python connector, or use Snowsight‚Äôs **UI upload** feature (which uploads to **named internal stages only**). [16],\n",
    "\n",
    "> Options:\n",
    ">\n",
    "> * Local ‚Üí Internal stage: **SnowSQL `PUT`** (or connectors).\n",
    "> * Browser upload ‚Üí Named **internal** stage: Snowsight ‚ÄúStages‚Äù UI.\n",
    "> * External bucket ‚Üí External stage: create with **`STORAGE_INTEGRATION`**, then `COPY INTO ...`.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) End-to-end scenarios you‚Äôll actually encounter\n",
    "\n",
    "### Scenario A ‚Äî ‚ÄúI have CSVs on my laptop; load to a table‚Äù\n",
    "\n",
    "1. Create parsing rules:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT util.ff_csv_pipe\n",
    "  TYPE=CSV FIELD_DELIMITER='|' SKIP_HEADER=1 TRIM_SPACE=TRUE;\n",
    "```\n",
    "\n",
    "2. Create a **named internal** stage and attach the default file format:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE analytics_stg.int_landing\n",
    "  FILE_FORMAT = util.ff_csv_pipe;\n",
    "```\n",
    "\n",
    "3. Upload files (SnowSQL on your machine):\n",
    "\n",
    "```bash\n",
    "snowsql -q \"PUT file:///C:/drops/*.csv @analytics_stg.int_landing AUTO_COMPRESS=TRUE OVERWRITE=TRUE;\"\n",
    "```\n",
    "\n",
    "4. Validate and load safely:\n",
    "\n",
    "```sql\n",
    "-- Dry run: show parsing errors without loading rows\n",
    "COPY INTO staging.orders_raw\n",
    "FROM @analytics_stg.int_landing\n",
    "  VALIDATION_MODE = RETURN_ALL_ERRORS;\n",
    "\n",
    "-- Real load\n",
    "COPY INTO staging.orders_raw\n",
    "FROM @analytics_stg.int_landing\n",
    "  ON_ERROR = 'ABORT_STATEMENT'  -- or CONTINUE after you‚Äôre confident\n",
    "  PURGE = TRUE;                 -- delete staged files after load\n",
    "```\n",
    "\n",
    "(Why SnowSQL? Because worksheets can‚Äôt run `PUT`.) [16])\n",
    "\n",
    "### Scenario B ‚Äî ‚ÄúPartner drops Parquet into S3; we load hourly‚Äù\n",
    "\n",
    "1. Create a storage integration and **external** stage:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE analytics_stg.partner_s3\n",
    "  URL='s3://partner-bucket/prod/'\n",
    "  STORAGE_INTEGRATION=partner_si\n",
    "  DIRECTORY=(ENABLE=TRUE);\n",
    "```\n",
    "\n",
    "2. Load with a simple `COPY` (Parquet is inferred efficiently):\n",
    "\n",
    "```sql\n",
    "COPY INTO bronze.partner_events\n",
    "FROM @analytics_stg.partner_s3/events/\n",
    "FILE_FORMAT=(TYPE=PARQUET);\n",
    "```\n",
    "\n",
    "3. Monitor arrivals:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM DIRECTORY(@analytics_stg.partner_s3) ORDER BY last_modified;\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Scenario C ‚Äî ‚ÄúPreview JSON before loading‚Äù\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  $1:id::string                AS id,\n",
    "  TO_TIMESTAMP_NTZ($1:ts)      AS ts,\n",
    "  $1:payload                   AS payload\n",
    "FROM @analytics_stg.int_landing/json/\n",
    "  ( FILE_FORMAT => (TYPE=JSON STRIP_OUTER_ARRAY=TRUE) )\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "If you see ‚Äúeverything in one row‚Äù, add `STRIP_OUTER_ARRAY=TRUE`. \n",
    "\n",
    "### Scenario D ‚Äî ‚ÄúUnload a filtered dataset to an external lake‚Äù\n",
    "\n",
    "```sql\n",
    "COPY INTO @analytics_stg.partner_s3/exports/dt=2025-08-23/\n",
    "FROM ( SELECT * FROM mart.daily_sales WHERE sales_date = '2025-08-23' )\n",
    "FILE_FORMAT=(TYPE=CSV COMPRESSION=GZIP)\n",
    "HEADER=TRUE OVERWRITE=TRUE;\n",
    "```\n",
    "\n",
    "Now downstream tools can pick up partitioned files from your S3 prefix. \n",
    "\n",
    "---\n",
    "\n",
    "## 9) Operational & security best practices\n",
    "\n",
    "* **Prefer `STORAGE_INTEGRATION`** over embedding cloud keys in the stage. Easier rotation and tighter IAM.\n",
    "* **Govern access**: `USAGE` to see a stage, `READ` to GET/list, `WRITE` to PUT/REMOVE. Least privilege for roles. \n",
    "* **Idempotency**: Snowflake tracks loaded files by name + path. Keep stable file names or use `PURGE=TRUE` after load. Use `VALIDATION_MODE` first. [19])\n",
    "* **Compression**: Keep files compressed; Snowflake auto-detects codec; set `COMPRESSION` if needed in file format. \n",
    "* **Headers**: Decide between `PARSE_HEADER=TRUE` or `SKIP_HEADER=1`‚Äîdon‚Äôt mix patterns arbitrarily.\n",
    "* **Troubleshoot JSON encodings** with `IGNORE_UTF8_ERRORS` carefully; fix source encoding where possible.\n",
    "---\n",
    "\n",
    "## 10) Must-answer questions to test yourself\n",
    "\n",
    "1. What are the differences among **user**, **table**, and **named** stages, and when would you choose each? \n",
    "2. Contrast **internal** vs **external** stages. Why prefer **storage integrations**? \n",
    "3. Which properties does `DESC STAGE` expose, and how do they impact a `COPY`? (Think: `FILE_FORMAT`, `URL`, `STORAGE_INTEGRATION`, `DIRECTORY`.) \n",
    "4. Show three ways to specify parsing rules: stage-level `FILE_FORMAT`, named file format, inline in `COPY`. Which takes precedence? (Inline `COPY` overrides stage defaults.) \n",
    "5. What are the most important **CSV** and **JSON** file format options you‚Äôve tuned, and why? (Delimiter, header handling, quoting, trimming, `STRIP_OUTER_ARRAY`, encoding.)\n",
    "6. How do you **list** files, **query** staged data, and **monitor** arrivals via a **directory table**? \n",
    "7. Why can‚Äôt you run `PUT/GET` from worksheets, and what are your alternatives? \n",
    "8. Which **grants** do you apply to make a stage safely shareable across teams? (USAGE/READ/WRITE.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba6a812",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1) What are the differences among **user**, **table**, and **named** stages, and when would you choose each?\n",
    "\n",
    "* **User stage (`@~`)**\n",
    "\n",
    "  * Automatically available to every user.\n",
    "  * Temporary ‚Äúpersonal‚Äù upload space, useful for ad-hoc testing.\n",
    "  * Example: Analyst wants to quickly load a CSV from their laptop ‚Üí `PUT file.csv @~;`.\n",
    "\n",
    "* **Table stage (`@%table`)**\n",
    "\n",
    "  * Tied to one specific table.\n",
    "  * Files here are usually meant to be loaded **only into that table**.\n",
    "  * Example: Developers staging files for `ORDERS` table ‚Üí `PUT orders.csv @%orders;`.\n",
    "\n",
    "* **Named stage (`@db.schema.stage`)**\n",
    "\n",
    "  * Explicitly created object, reusable across tables/users.\n",
    "  * Allows attaching defaults (file formats, directory tables, storage integration).\n",
    "  * Best for production pipelines with multiple consumers.\n",
    "  * Example: `CREATE STAGE analytics_stg.vendor_landing ...;`.\n",
    "\n",
    "üëâ **When to use?**\n",
    "\n",
    "* Quick one-off: user stage.\n",
    "* One table, one purpose: table stage.\n",
    "* Enterprise pipeline: named stage.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Contrast **internal** vs **external** stages. Why prefer **storage integrations**?\n",
    "\n",
    "* **Internal stage**\n",
    "\n",
    "  * Files stored inside Snowflake-managed cloud storage.\n",
    "  * Good for sensitive data, simplicity, and small/medium file transfers.\n",
    "  * Example: uploading files from analyst laptops.\n",
    "\n",
    "* **External stage**\n",
    "\n",
    "  * A pointer to cloud object storage (S3, Azure Blob, GCS).\n",
    "  * Best for large-scale ingestion or when data already lives in the lake.\n",
    "  * Example: loading terabytes of parquet data daily from S3.\n",
    "\n",
    "* **Storage Integration (best practice)**\n",
    "\n",
    "  * An IAM-like object in Snowflake that securely links Snowflake to your cloud provider.\n",
    "  * Prevents embedding cloud credentials in the stage definition.\n",
    "  * Example: `CREATE STORAGE INTEGRATION partner_si;` then `CREATE STAGE vendor_stage STORAGE_INTEGRATION=partner_si;`.\n",
    "\n",
    "üëâ **Why prefer storage integrations?**\n",
    "\n",
    "* Centralized security, easier credential rotation, no secrets leaked into SQL.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Which properties does `DESC STAGE` expose, and how do they impact a `COPY`?\n",
    "\n",
    "When you `DESC STAGE`, you‚Äôll see:\n",
    "\n",
    "* `STAGE_TYPE` ‚Üí INTERNAL / EXTERNAL\n",
    "* `STAGE_LOCATION` or `URL` ‚Üí where files live\n",
    "* `STORAGE_INTEGRATION` / `CREDENTIALS` ‚Üí how to authenticate (important for external stages)\n",
    "* `FILE_FORMAT` ‚Üí default parsing rules if none are specified in `COPY`\n",
    "* `ENCRYPTION` ‚Üí encryption details (important for compliance)\n",
    "* `DIRECTORY` ‚Üí whether you can query stage files as a table (`SELECT * FROM DIRECTORY(@stage)`)\n",
    "\n",
    "üëâ Impact: If you run `COPY INTO` without specifying `FILE_FORMAT`, Snowflake falls back to the stage‚Äôs default. Also, if `DIRECTORY=TRUE`, you can build incremental loaders by querying which files have arrived.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Show three ways to specify parsing rules: stage-level `FILE_FORMAT`, named file format, inline in `COPY`. Which takes precedence?\n",
    "\n",
    "1. **Stage-level file format**\n",
    "\n",
    "   ```sql\n",
    "   CREATE STAGE s1 FILE_FORMAT = (TYPE=CSV SKIP_HEADER=1);\n",
    "   ```\n",
    "\n",
    "   ‚Üí Used automatically if you don‚Äôt override later.\n",
    "\n",
    "2. **Named file format object**\n",
    "\n",
    "   ```sql\n",
    "   CREATE FILE FORMAT ff_csv TYPE=CSV FIELD_DELIMITER='|';\n",
    "   COPY INTO mytable FROM @s1 FILE_FORMAT=ff_csv;\n",
    "   ```\n",
    "\n",
    "3. **Inline in `COPY`**\n",
    "\n",
    "   ```sql\n",
    "   COPY INTO mytable\n",
    "   FROM @s1 FILE_FORMAT=(TYPE=CSV FIELD_DELIMITER=',');\n",
    "   ```\n",
    "\n",
    "üëâ **Precedence order:**\n",
    "Inline in `COPY` > Stage default > Table default.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) What are the most important **CSV** and **JSON** file format options you‚Äôve tuned, and why?\n",
    "\n",
    "* **CSV**\n",
    "\n",
    "  * `FIELD_DELIMITER` ‚Üí defines separation (`','`, `'|'`, `'\\t'`).\n",
    "  * `SKIP_HEADER` or `PARSE_HEADER` ‚Üí handle header rows.\n",
    "  * `FIELD_OPTIONALLY_ENCLOSED_BY` ‚Üí handle quoted text like `\"Smith, John\"`.\n",
    "  * `NULL_IF` and `EMPTY_FIELD_AS_NULL` ‚Üí decide how blanks are treated.\n",
    "  * `TRIM_SPACE` ‚Üí common when source files have padded text.\n",
    "\n",
    "* **JSON**\n",
    "\n",
    "  * `STRIP_OUTER_ARRAY` ‚Üí split `[...]` arrays into multiple rows.\n",
    "  * `ALLOW_DUPLICATE` / `STRIP_NULL_VALUES` ‚Üí manage messy JSON.\n",
    "  * `IGNORE_UTF8_ERRORS` ‚Üí handle bad encodings.\n",
    "\n",
    "üëâ These fix the ‚Äúwrong column counts‚Äù or ‚Äúall data in one row‚Äù headaches you‚Äôll face in real projects.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) How do you **list** files, **query** staged data, and **monitor** arrivals via a **directory table**?\n",
    "\n",
    "* **List files**\n",
    "\n",
    "  ```sql\n",
    "  LIST @my_stage/path/;\n",
    "  ```\n",
    "\n",
    "* **Query staged data directly**\n",
    "\n",
    "  ```sql\n",
    "  SELECT $1, $2::NUMBER\n",
    "  FROM @my_stage/data/ (FILE_FORMAT => ff_csv);\n",
    "  ```\n",
    "\n",
    "* **Directory table (if `DIRECTORY=TRUE`)**\n",
    "\n",
    "  ```sql\n",
    "  SELECT * FROM DIRECTORY(@my_stage);\n",
    "  ```\n",
    "\n",
    "  ‚Üí Returns file names, sizes, last modified times. Useful for pipelines (‚Äúhas today‚Äôs file arrived?‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Why can‚Äôt you run `PUT/GET` from worksheets, and what are your alternatives?\n",
    "\n",
    "* **Why not?**\n",
    "\n",
    "  * Snowflake worksheets (Snowsight UI) run in the cloud; they don‚Äôt have access to your local filesystem.\n",
    "  * `PUT`/`GET` require a client that can access local files.\n",
    "\n",
    "* **Alternatives**\n",
    "\n",
    "  * Use **SnowSQL CLI** (`PUT local.csv @mystage; GET @mystage file://local/`).\n",
    "  * Use connectors (Python, JDBC, etc.).\n",
    "  * Or Snowsight‚Äôs **upload UI**, which only supports **named internal stages**.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Which **grants** do you apply to make a stage safely shareable across teams?\n",
    "\n",
    "* `USAGE` ‚Üí lets a role see the stage exists.\n",
    "* `READ` ‚Üí allows `LIST` and `COPY INTO <table> FROM @stage`.\n",
    "* `WRITE` ‚Üí allows `PUT`, `REMOVE`.\n",
    "\n",
    "üëâ Typical pattern: Analysts get **READ**, Engineers get **READ + WRITE**, Admins get full control.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2a44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
