{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0933281b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Snowflake Stages: What, Why, and When (with code)\n",
    "\n",
    "## A 10-second mental model\n",
    "\n",
    "A **stage** is just a **file parking lot** that Snowflake can read from or write to.\n",
    "You load files **into a stage** → then **COPY** them into tables (or unload back to a stage).\n",
    "\n",
    "There are four you will actually touch:\n",
    "\n",
    "1. **User stage** – your personal scratch area (`@~`)\n",
    "2. **Table stage** – a parking lot tied to one table (`@%table_name`)\n",
    "3. **Named internal stage** – a reusable Snowflake-managed area (`@stage_name`)\n",
    "4. **Named external stage** – a pointer to S3/Azure Blob/GCS (`@ext_stage`)\n",
    "\n",
    "### When to use which (purpose in one line)\n",
    "\n",
    "* **User stage:** ad-hoc, one-off loads by a person.\n",
    "* **Table stage:** tightly-coupled loads to a single table; great for simple, reliable pipelines.\n",
    "* **Named internal stage:** shared team landing zone inside Snowflake; repeatable jobs.\n",
    "* **Named external stage:** enterprise data lake integration; no duplication of storage.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) User stage (`@~`) — “my scratch pad”\n",
    "\n",
    "**Purpose:** quick, personal experiments or small analyst uploads.\n",
    "**Who owns storage?** Snowflake (internal).\n",
    "**Access:** only you (unless you purposefully share data later).\n",
    "\n",
    "### Typical flow (CSV example)\n",
    "\n",
    "```sql\n",
    "-- From SnowSQL or Worksheet:\n",
    "-- 1) Upload file from your laptop to your user stage\n",
    "PUT file://C:\\data\\leads_2025_08_21.csv @~ AUTO_COMPRESS=TRUE;\n",
    "\n",
    "-- 2) Inspect what’s there\n",
    "LIST @~;\n",
    "\n",
    "-- 3) Create a table to load into\n",
    "CREATE OR REPLACE TABLE leads_raw (\n",
    "  lead_id NUMBER,\n",
    "  full_name STRING,\n",
    "  email STRING,\n",
    "  created_at TIMESTAMP_NTZ\n",
    ");\n",
    "\n",
    "-- 4) Load\n",
    "COPY INTO leads_raw\n",
    "FROM @~\n",
    "FILE_FORMAT = (TYPE=CSV SKIP_HEADER=1 FIELD_OPTIONALLY_ENCLOSED_BY='\"')\n",
    "PATTERN='.*leads_2025_08_21.*[.]csv[.]gz';\n",
    "\n",
    "-- 5) Validate count\n",
    "SELECT COUNT(*) FROM leads_raw;\n",
    "```\n",
    "\n",
    "### When it shines\n",
    "\n",
    "* An analyst needs to upload one Excel/CSV and test logic today.\n",
    "* You don’t want to create infra or share anything yet.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Table stage (`@%table`) — “the table’s own dock”\n",
    "\n",
    "**Purpose:** always load the **same** table from a small set of files; keep things simple.\n",
    "**Who owns storage?** Snowflake (internal).\n",
    "**Access:** controlled by table permissions.\n",
    "\n",
    "### Typical flow (JSON → VARIANT)\n",
    "\n",
    "```sql\n",
    "-- 1) Create target table\n",
    "CREATE OR REPLACE TABLE orders_raw (data VARIANT);\n",
    "\n",
    "-- 2) Upload JSON to the table's stage\n",
    "PUT file://C:\\data\\orders*.json @%orders_raw AUTO_COMPRESS=TRUE;\n",
    "\n",
    "-- 3) Load using the table’s stage\n",
    "COPY INTO orders_raw\n",
    "FROM @%orders_raw\n",
    "FILE_FORMAT=(TYPE=JSON);\n",
    "\n",
    "-- 4) Query\n",
    "SELECT data:\"order_id\"::string AS order_id,\n",
    "       data:\"amount\"::number  AS amount\n",
    "FROM orders_raw\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🧩 What is `VARIANT` in Snowflake?\n",
    "\n",
    "* `VARIANT` is a **semi-structured data type**.\n",
    "* It can store **JSON, Avro, ORC, Parquet, XML** in a flexible way.\n",
    "* You don’t need to predefine all fields/columns upfront.\n",
    "\n",
    "Example:\n",
    "\n",
    "```sql\n",
    "INSERT INTO orders_raw VALUES\n",
    "    (PARSE_JSON('{\"order_id\": 101, \"customer\": \"Alice\", \"items\": [\"Book\", \"Pen\"], \"amount\": 23.5}')),\n",
    "    (PARSE_JSON('{\"order_id\": 102, \"customer\": \"Bob\", \"amount\": 45.0, \"status\": \"shipped\"}'));\n",
    "```\n",
    "\n",
    "Now the table looks like:\n",
    "\n",
    "| data                                                                       |\n",
    "| -------------------------------------------------------------------------- |\n",
    "| {\"order\\_id\":101,\"customer\":\"Alice\",\"items\":\\[\"Book\",\"Pen\"],\"amount\":23.5} |\n",
    "| {\"order\\_id\":102,\"customer\":\"Bob\",\"amount\":45.0,\"status\":\"shipped\"}        |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Querying inside `VARIANT`\n",
    "\n",
    "You can use **dot notation** to extract fields from `VARIANT`:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    data:order_id::INT AS order_id,\n",
    "    data:customer::STRING AS customer,\n",
    "    data:amount::FLOAT AS amount\n",
    "FROM orders_raw;\n",
    "```\n",
    "\n",
    "👉 Output:\n",
    "\n",
    "| order\\_id | customer | amount |\n",
    "| --------- | -------- | ------ |\n",
    "| 101       | Alice    | 23.5   |\n",
    "| 102       | Bob      | 45.0   |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Purpose of this Design\n",
    "\n",
    "* This design is common in **data ingestion pipelines**.\n",
    "* `orders_raw` is like a **landing zone** for raw JSON data.\n",
    "* Later, you can transform it into structured tables (like `orders`, `customers`, etc.) using `INSERT INTO ... SELECT` or `COPY INTO`.\n",
    "\n",
    "---\n",
    "\n",
    "⚡ So, to summarize:\n",
    "\n",
    "`CREATE OR REPLACE TABLE orders_raw (data VARIANT);`\n",
    "➡ Creates a **raw data table** with one column `data` that can hold **flexible JSON-like data**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 3) Named **internal** stage (`@stage_name`) — “shared team landing zone”\n",
    "\n",
    "**Purpose:** repeatable pipelines within Snowflake, shared across jobs and roles.\n",
    "**Who owns storage?** Snowflake (internal).\n",
    "**Access:** via grants on the stage (and its database/schema).\n",
    "\n",
    "### Create once, reuse everywhere\n",
    "\n",
    "```sql\n",
    "-- 1) (Optional) define a reusable file format\n",
    "CREATE OR REPLACE FILE FORMAT ff_csv_std\n",
    "  TYPE=CSV SKIP_HEADER=1 FIELD_OPTIONALLY_ENCLOSED_BY='\"';\n",
    "\n",
    "-- 2) Create the stage and bind the file format\n",
    "CREATE OR REPLACE STAGE stg_marketing_in\n",
    "  FILE_FORMAT = ff_csv_std\n",
    "  COMMENT = 'Landing zone for Marketing CSV drops';\n",
    "\n",
    "-- 3) Upload files to the named stage\n",
    "PUT file://C:\\data\\campaigns_*.csv @stg_marketing_in AUTO_COMPRESS=TRUE;\n",
    "\n",
    "-- 4) Load into target table(s)\n",
    "CREATE OR REPLACE TABLE campaigns_raw (\n",
    "  campaign_id NUMBER, name STRING, channel STRING, spend NUMBER, dt DATE\n",
    ");\n",
    "\n",
    "COPY INTO campaigns_raw\n",
    "FROM @stg_marketing_in\n",
    "-- you can keep FILE_FORMAT off here since stg has one; or override:\n",
    "FILE_FORMAT = (FORMAT_NAME = ff_csv_std)\n",
    "PATTERN='.*campaigns_.*[.]csv[.]gz'\n",
    "ON_ERROR='CONTINUE';\n",
    "\n",
    "-- 5) Housekeeping (optional)\n",
    "LIST @stg_marketing_in;\n",
    "REMOVE @stg_marketing_in PATTERN='.*2024.*';  -- delete old staged files\n",
    "```\n",
    "\n",
    "### Why use it\n",
    "\n",
    "* One stage feeds multiple tables/pipelines.\n",
    "* Centralize file format policies and governance.\n",
    "* Easy to delegate access to teams via roles/grants.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Named **external** stage (`@ext_stage`) — “data lake pointer”\n",
    "\n",
    "**Purpose:** read from or write to your **own** cloud storage (S3/Blob/GCS) without duplicating data.\n",
    "**Who owns storage?** You (your cloud account).\n",
    "**Access:** **recommended** via a **Storage Integration** (secure, keyless).\n",
    "\n",
    "> Correcting a common misconception: you always create a **stage object**.\n",
    "> The difference is *how* you authenticate it:\n",
    ">\n",
    "> * **Inline credentials** in the stage (not recommended, okay for experiments)\n",
    "> * **Storage integration** (recommended, secure, production-ready)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Inline Credentials in the Stage (NOT recommended — only okay for experiments)**\n",
    "\n",
    "When you create an **external stage** in Snowflake (pointing to AWS S3, Azure Blob, or GCP bucket), you need to tell Snowflake **how to access that storage**.\n",
    "\n",
    "One way is:\n",
    "👉 directly embedding the **access credentials** (keys, secrets, tokens) inside the stage definition.\n",
    "\n",
    "### Example (AWS S3 with inline credentials):\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE my_s3_stage\n",
    "  URL='s3://my-bucket-name/my-path/'\n",
    "  CREDENTIALS=(AWS_KEY_ID='AKIAxxxx' AWS_SECRET_KEY='xxxxxxx');\n",
    "```\n",
    "\n",
    "### What’s happening:\n",
    "\n",
    "* The **AWS access key ID** and **secret key** are **hardcoded** in the stage.\n",
    "* Every time Snowflake accesses this stage (e.g., `COPY INTO`), it uses those credentials.\n",
    "\n",
    "### Nitty gritty downsides:\n",
    "\n",
    "1. **Security Risk** – Credentials are stored in Snowflake metadata (even though they are encrypted). If leaked or misused, your cloud storage is exposed.\n",
    "2. **Rotation nightmare** – If keys expire or are rotated, you must manually update all stages where you used them.\n",
    "3. **Compliance issue** – Hardcoding secrets is against security best practices (SOC2, HIPAA, PCI DSS would flag this).\n",
    "4. **Audit trail weakness** – You can’t track who’s using the underlying bucket easily.\n",
    "\n",
    "✅ Okay for:\n",
    "\n",
    "* Quick experiments\n",
    "* Proof-of-concept\n",
    "  ❌ Not okay for:\n",
    "* Production\n",
    "* Multi-team environments\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Storage Integration (Recommended, Secure, Production-ready)**\n",
    "\n",
    "Instead of embedding credentials, you let **Snowflake assume a role in your cloud provider account**. This is done via a **storage integration object**.\n",
    "\n",
    "Snowflake → assumes a **role with least-privilege access** → accesses the external bucket securely.\n",
    "\n",
    "### Example (AWS S3 with storage integration):\n",
    "\n",
    "**Step 1: Create a storage integration**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STORAGE INTEGRATION my_s3_integration\n",
    "  TYPE = EXTERNAL_STAGE\n",
    "  STORAGE_PROVIDER = S3\n",
    "  ENABLED = TRUE\n",
    "  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::123456789012:role/snowflake_role'\n",
    "  STORAGE_ALLOWED_LOCATIONS = ('s3://my-bucket-name/my-path/');\n",
    "```\n",
    "\n",
    "**Step 2: Create a stage that uses the integration**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE my_s3_stage\n",
    "  URL='s3://my-bucket-name/my-path/'\n",
    "  STORAGE_INTEGRATION = my_s3_integration;\n",
    "```\n",
    "\n",
    "### What’s happening:\n",
    "\n",
    "* No keys or secrets are stored in Snowflake.\n",
    "* Snowflake is granted permission to **assume an AWS IAM role**.\n",
    "* IAM policies control what Snowflake can do (e.g., only `READ` access).\n",
    "* Rotations, security, and revocations are managed **on the cloud side** (AWS/Azure/GCP IAM), not inside Snowflake.\n",
    "\n",
    "### Nitty gritty benefits:\n",
    "\n",
    "1. **Security best practice** – No secrets hardcoded in Snowflake.\n",
    "2. **Key rotation handled automatically** – IAM roles rotate securely behind the scenes.\n",
    "3. **Principle of Least Privilege** – Only the bucket/path you allow can be accessed.\n",
    "4. **Auditability** – Cloud provider logs all access via the IAM role, so you can trace.\n",
    "5. **Scalability** – Same integration can be used across multiple stages/projects.\n",
    "\n",
    "✅ Always use this for:\n",
    "\n",
    "* Production workloads\n",
    "* Sensitive data\n",
    "* Multi-team usage\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### 4A) Secure (recommended) — AWS S3 with Storage Integration\n",
    "\n",
    "```sql\n",
    "-- 1) Admin: create a storage integration (one time)\n",
    "CREATE OR REPLACE STORAGE INTEGRATION si_s3_data_lake\n",
    "  TYPE = EXTERNAL_STAGE\n",
    "  STORAGE_PROVIDER = S3\n",
    "  ENABLED = TRUE\n",
    "  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::123456789012:role/snowflake-access-role'\n",
    "  STORAGE_ALLOWED_LOCATIONS = ('s3://company-datalake/raw/');\n",
    "\n",
    "-- 2) Create the external stage that uses the integration\n",
    "CREATE OR REPLACE STAGE ext_raw_clicks\n",
    "  URL='s3://company-datalake/raw/clicks/'\n",
    "  STORAGE_INTEGRATION = si_s3_data_lake\n",
    "  FILE_FORMAT = (TYPE=PARQUET);\n",
    "\n",
    "-- 3) Load Parquet directly (column names auto-map if they match)\n",
    "CREATE OR REPLACE TABLE clicks_raw\n",
    "  (user_id STRING, ts TIMESTAMP_NTZ, url STRING, referrer STRING);\n",
    "\n",
    "COPY INTO clicks_raw\n",
    "FROM @ext_raw_clicks\n",
    "FILE_FORMAT=(TYPE=PARQUET)\n",
    "PATTERN='.*dt=2025-08-2[0-2].*'; -- load a date range partition, for example\n",
    "```\n",
    "\n",
    "#### Unload to S3 (export)\n",
    "\n",
    "```sql\n",
    "COPY INTO @ext_raw_clicks/unloads/dt=2025-08-22/\n",
    "FROM ( SELECT * FROM clicks_raw WHERE ts::date='2025-08-22' )\n",
    "FILE_FORMAT=(TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"')\n",
    "HEADER=TRUE OVERWRITE=TRUE;\n",
    "```\n",
    "\n",
    "### 4B) Inline credentials (quick test only; not secure)\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE ext_quick_test\n",
    "  URL='s3://mahbub-temp-bucket/dropzone/'\n",
    "  CREDENTIALS=(AWS_KEY_ID='AKIA...' AWS_SECRET_KEY='abcd...')\n",
    "  FILE_FORMAT=(TYPE=CSV SKIP_HEADER=1);\n",
    "```\n",
    "\n",
    "> Use this only in non-prod demos. Prefer **storage integrations** in real life.\n",
    "\n",
    "---\n",
    "\n",
    "## COPY patterns you’ll use all the time\n",
    "\n",
    "### CSV → table (with validations)\n",
    "\n",
    "```sql\n",
    "-- Preview errors first\n",
    "COPY INTO my_table\n",
    "FROM @stg_marketing_in\n",
    "FILE_FORMAT=(FORMAT_NAME=ff_csv_std)\n",
    "VALIDATION_MODE='RETURN_ERRORS';\n",
    "\n",
    "-- Load with idempotence helpers\n",
    "COPY INTO my_table\n",
    "FROM @stg_marketing_in\n",
    "FILE_FORMAT=(FORMAT_NAME=ff_csv_std)\n",
    "PATTERN='.*campaigns_2025_08_22.*[.]csv[.]gz'\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "FORCE=FALSE;  -- don’t reload files Snowflake marked as loaded\n",
    "```\n",
    "\n",
    "### JSON → typed columns (using VARIANT paths)\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE events_raw (\n",
    "  event_time TIMESTAMP_NTZ,\n",
    "  user_id STRING,\n",
    "  payload VARIANT\n",
    ");\n",
    "\n",
    "COPY INTO events_raw(payload)\n",
    "FROM @ext_raw_clicks\n",
    "FILE_FORMAT=(TYPE=JSON);\n",
    "\n",
    "-- Project out fields\n",
    "CREATE OR REPLACE VIEW events_v AS\n",
    "SELECT\n",
    "  TO_TIMESTAMP_NTZ(payload:\"ts\") AS event_time,\n",
    "  payload:\"user\"::string        AS user_id,\n",
    "  payload                       AS payload\n",
    "FROM events_raw;\n",
    "```\n",
    "\n",
    "### Parquet → automatic mapping or SELECT-transform\n",
    "\n",
    "```sql\n",
    "-- Direct\n",
    "COPY INTO sales_raw\n",
    "FROM @ext_s3_sales\n",
    "FILE_FORMAT=(TYPE=PARQUET);\n",
    "\n",
    "-- Or transform as you load\n",
    "COPY INTO sales_curated\n",
    "FROM (\n",
    "  SELECT\n",
    "    $1:user_id::string    AS user_id,\n",
    "    $1:amount::number(10,2) AS amount,\n",
    "    $1:ts::timestamp_ntz  AS ts\n",
    "  FROM @ext_s3_sales (FILE_FORMAT => 'PARQUET')\n",
    ")\n",
    "FILE_FORMAT=(TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"'); -- internal load format\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Useful stage/file operations you’ll actually use\n",
    "\n",
    "```sql\n",
    "-- See files\n",
    "LIST @stg_marketing_in;\n",
    "\n",
    "-- Remove specific files from a stage\n",
    "REMOVE @stg_marketing_in PATTERN='.*old_.*';\n",
    "\n",
    "-- Peek at staged file contents (semi-structured)\n",
    "SELECT\n",
    "  METADATA$FILENAME,\n",
    "  METADATA$FILE_ROW_NUMBER,\n",
    "  t.$1, t.$2, t.$3\n",
    "FROM @stg_marketing_in (FILE_FORMAT => ff_csv_std) t\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Governance & best practices that save you later\n",
    "\n",
    "* **Prefer storage integrations** for external stages (no hardcoded keys; auditable IAM).\n",
    "* **Bind a FILE FORMAT to the stage** so COPY jobs don’t repeat parsing options.\n",
    "* **Separate concerns:**\n",
    "\n",
    "  * *Raw landing* stage (immutable)\n",
    "  * *Work* stage (transforms/unloads)\n",
    "* **Idempotency:** rely on Snowflake’s **load history** (don’t set `FORCE=TRUE` unless you mean to reload).\n",
    "* **Partition-friendly patterns:** use `PATTERN` to load by date/hour prefixes.\n",
    "* **Access control:** grant stage usage to roles that need it; control who can **PUT/GET** vs **COPY**.\n",
    "* **Snowpipe:** uses a **stage** as the watched location; adding files → triggers ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "## Real scenarios (so it sticks)\n",
    "\n",
    "### Scenario A — Analyst upload (user stage)\n",
    "\n",
    "Mahbub gets a one-off CSV from Finance. He `PUT`s it to `@~`, runs `COPY INTO finance_leads_raw`, shares the resulting table. No infra, done in minutes.\n",
    "\n",
    "### Scenario B — Simple, reliable table feed (table stage)\n",
    "\n",
    "Your nightly ERP extract always feeds `orders_raw`. Dump files to `@%orders_raw` and run one `COPY`. No confusion about where files go.\n",
    "\n",
    "### Scenario C — Team landing (named internal stage)\n",
    "\n",
    "Marketing drops many CSVs daily. Devs and analysts share `@stg_marketing_in` with a fixed file format. One job loads “yesterday’s” files by regex pattern.\n",
    "\n",
    "### Scenario D — Lakehouse (named external stage)\n",
    "\n",
    "Clickstream logs live in `s3://company-datalake/raw/clicks/`. You create `ext_raw_clicks` with a storage integration and load Parquet directly; you also **unload** curated datasets back to S3 for ML teams.\n",
    "\n",
    "---\n",
    "\n",
    "## What you might have assumed (and the precise truth)\n",
    "\n",
    "* “External staging = stage object connection (not secure).”\n",
    "  ➜ **Refined:** You always create a **stage object**; the *authentication method* can be **inline creds** (ok for tests) or a **storage integration** (secure, recommended).\n",
    "\n",
    "* “Internal staging needs an integration.”\n",
    "  ➜ **No.** Internal stages are Snowflake-managed; **no storage integration needed**.\n",
    "\n",
    "* “Stages are like schemas/tables.”\n",
    "  ➜ **No.** Stages are **file locations**. Tables store rows; stages store files.\n",
    "\n",
    "---\n",
    "\n",
    "## Must-answer questions (to prove you’re ready)\n",
    "\n",
    "1. When would you choose **user**, **table**, **named internal**, or **named external** stages? Give one concrete example each.\n",
    "2. Show code to securely read Parquet from S3 using a **storage integration**.\n",
    "3. How do you avoid re-loading files you already loaded yesterday?\n",
    "4. How do you validate a load before committing it?\n",
    "5. What’s the difference between loading JSON into **VARIANT** vs. projecting JSON fields during `COPY`?\n",
    "6. How do you unload a filtered subset of data back to S3 with headers?\n",
    "7. If two teams need separate access to the same bucket prefix, how would you design **stages, roles, and patterns**?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cbb3e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. First, clear the biggest confusion: *Staging Area ≠ Data Warehouse Staging*\n",
    "\n",
    "Many people (and even junior engineers) confuse the two.\n",
    "\n",
    "* **Data Warehouse Staging Area (traditional concept)**\n",
    "  In on-prem or legacy warehouses, the “staging schema” or “staging area” meant:\n",
    "\n",
    "  > A temporary schema/table inside the warehouse where you land raw data before transformation.\n",
    "  > Example: You extract CSV files from source systems, dump them into a \"staging schema\", and then ETL into core fact/dimension tables.\n",
    "\n",
    "* **Snowflake Staging Area (actual Snowflake term)**\n",
    "  In Snowflake, **Stages** are *storage locations* (think “parking lots” for files) where you place raw data files (CSV, JSON, Parquet, etc.) *before* loading them into tables.\n",
    "\n",
    "  * This “parking lot” can live either inside Snowflake (internal stage) or outside Snowflake (external stage in S3, Azure Blob, GCS).\n",
    "  * The key is: Stages are **not tables**; they’re file-holding areas.\n",
    "\n",
    "👉 So, in Snowflake, when someone says **Stage**, think **blob storage for files**, not database schema.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Internal vs. External Stages\n",
    "\n",
    "Now, let’s separate the **Snowflake-managed vs. Customer-managed** parking lots:\n",
    "\n",
    "### (a) **Internal Stage**\n",
    "\n",
    "* Managed by Snowflake itself.\n",
    "* Files are stored in Snowflake’s cloud storage (hidden from you).\n",
    "* You don’t worry about infra, IAM, keys, or permissions — Snowflake secures it for you.\n",
    "* Perfect for **quick prototyping** or **smaller datasets**.\n",
    "* Downsides: Not cost-efficient for very large enterprise pipelines (you’ll end up duplicating storage).\n",
    "\n",
    "👉 **Scenario**:\n",
    "You’re a small analytics team. Marketing team sends a CSV file of leads daily. You just `PUT` the file into your Snowflake internal stage and load it with `COPY INTO`. Easy, no infra setup.\n",
    "\n",
    "---\n",
    "\n",
    "### (b) **External Stage**\n",
    "\n",
    "* Files remain in your cloud storage (S3, Azure Blob, GCS).\n",
    "* Snowflake doesn’t duplicate the storage; it just creates a pointer (stage object) to those files.\n",
    "* You control the bucket, retention, lifecycle policies, and costs.\n",
    "* Perfect for **big data lakes** or **multi-system sharing**.\n",
    "\n",
    "👉 **Scenario**:\n",
    "Your company already stores terabytes of clickstream logs in AWS S3. You don’t want to duplicate that storage in Snowflake. Instead, you create an external stage pointing to the S3 bucket. Snowflake can then load (or even directly query via external tables) the data.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Secure vs. Non-Secure Connection\n",
    "\n",
    "This is where many students (and even engineers) get confused.\n",
    "\n",
    "* **External Stage (Non-Secure)**\n",
    "  You can connect directly to S3/Blob/GCS by embedding access keys in the stage definition.\n",
    "  Example (S3):\n",
    "\n",
    "  ```sql\n",
    "  CREATE STAGE my_s3_stage \n",
    "    URL='s3://mybucket/data/'\n",
    "    CREDENTIALS=(AWS_KEY_ID='xxx' AWS_SECRET_KEY='yyy');\n",
    "  ```\n",
    "\n",
    "  🚨 Problem: Keys are hardcoded in Snowflake. Risky for production.\n",
    "\n",
    "* **External Stage (Secure with Integration Object)**\n",
    "  Instead of hardcoding credentials, you create a **Storage Integration** object in Snowflake.\n",
    "\n",
    "  * This is a secure handshake between Snowflake and your cloud provider IAM.\n",
    "  * No secrets stored in Snowflake; access is role-based and token-based.\n",
    "    Example (S3 with storage integration):\n",
    "\n",
    "  ```sql\n",
    "  CREATE STORAGE INTEGRATION my_s3_integration\n",
    "    TYPE=EXTERNAL_STAGE\n",
    "    STORAGE_PROVIDER = S3\n",
    "    ENABLED = TRUE\n",
    "    STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::123456789012:role/mySnowflakeRole'\n",
    "    STORAGE_ALLOWED_LOCATIONS = ('s3://mybucket/data/');\n",
    "  ```\n",
    "\n",
    "👉 Think of **Integration** like a **trusted gate pass** Snowflake uses to enter your cloud storage dock.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. How Files Move into Stages\n",
    "\n",
    "This is where **operations** come in:\n",
    "\n",
    "1. **Upload files (to internal stage):**\n",
    "\n",
    "   ```sql\n",
    "   PUT file://local/path/mydata.csv @~;\n",
    "   ```\n",
    "\n",
    "   (`@~` = your user stage)\n",
    "\n",
    "2. **List files:**\n",
    "\n",
    "   ```sql\n",
    "   LIST @~;\n",
    "   ```\n",
    "\n",
    "3. **Copy files into a table:**\n",
    "\n",
    "   ```sql\n",
    "   COPY INTO my_table\n",
    "   FROM @~ \n",
    "   FILE_FORMAT = (TYPE = CSV SKIP_HEADER = 1);\n",
    "   ```\n",
    "\n",
    "4. **Unload data (reverse direction):**\n",
    "\n",
    "   ```sql\n",
    "   COPY INTO @my_stage/unload/ \n",
    "   FROM my_table \n",
    "   FILE_FORMAT = (TYPE = CSV);\n",
    "   ```\n",
    "\n",
    "So, stages aren’t just for loading — you can also use them for **unloading/exporting** data.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 8. Must-Know Questions to Test Understanding\n",
    "\n",
    "Here are the **questions you should always be ready to answer**:\n",
    "\n",
    "1. What is the difference between a traditional DWH staging schema and a Snowflake Stage?\n",
    "2. Explain the three types of stages in Snowflake (user, table, named) with use cases.\n",
    "3. How do internal stages differ from external stages in terms of management, security, and cost?\n",
    "4. What’s the difference between defining an external stage with credentials vs. using a storage integration object?\n",
    "5. Can you unload/export data from Snowflake into a stage? How?\n",
    "6. Why might you use a table stage vs. a named stage?\n",
    "7. What’s the lifecycle of files in an internal stage? How long do they persist?\n",
    "   (Hint: Until explicitly removed, default retention 7 days for dropped tables’ stages).\n",
    "8. If your company already stores petabytes of logs in S3, would you use an internal or external stage? Why?\n",
    "9. How do you ensure security and governance when multiple teams use the same stage?\n",
    "10. Can stages be used with Snowpipe for continuous ingestion? (Yes, they are the entry point for Snowpipe.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3df72",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ❓ Must-Answer Questions on Snowflake Stages\n",
    "\n",
    "---\n",
    "\n",
    "## 1) When would you choose **user**, **table**, **named internal**, or **named external** stages?\n",
    "\n",
    "👉 Answer with purpose + scenario\n",
    "\n",
    "* **User stage (`@~`)**\n",
    "  *Purpose:* personal scratchpad for one-off loads.\n",
    "  *Scenario:* Analyst Mahbub gets a CSV of leads and wants to load it for quick testing. He uses `PUT` into `@~` and runs a `COPY INTO` without involving the engineering team.\n",
    "\n",
    "  ```sql\n",
    "  PUT file://C:\\data\\leads.csv @~ AUTO_COMPRESS=TRUE;\n",
    "  COPY INTO leads_raw FROM @~ FILE_FORMAT=(TYPE=CSV SKIP_HEADER=1);\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "* **Table stage (`@%table_name`)**\n",
    "  *Purpose:* data always lands in **one table**.\n",
    "  *Scenario:* ERP daily export always goes to `orders_raw`. Instead of remembering a named stage, you always load files to `@%orders_raw`.\n",
    "\n",
    "  ```sql\n",
    "  PUT file://C:\\data\\orders_20250822.csv @%orders_raw AUTO_COMPRESS=TRUE;\n",
    "  COPY INTO orders_raw FROM @%orders_raw FILE_FORMAT=(TYPE=CSV SKIP_HEADER=1);\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "* **Named internal stage (`@stage_name`)**\n",
    "  *Purpose:* shared landing zone inside Snowflake; good for team pipelines.\n",
    "  *Scenario:* Marketing sends campaign CSVs daily. You create `stg_marketing_in` with a standard CSV file format. Both ETL jobs and analysts reuse it.\n",
    "\n",
    "  ```sql\n",
    "  CREATE OR REPLACE FILE FORMAT ff_csv TYPE=CSV SKIP_HEADER=1;\n",
    "  CREATE OR REPLACE STAGE stg_marketing_in FILE_FORMAT=ff_csv;\n",
    "\n",
    "  PUT file://C:\\data\\campaigns_20250822.csv @stg_marketing_in AUTO_COMPRESS=TRUE;\n",
    "  COPY INTO campaigns_raw FROM @stg_marketing_in;\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "* **Named external stage (`@ext_stage`)**\n",
    "  *Purpose:* enterprise-scale integration with cloud storage (S3/Blob/GCS). No file duplication.\n",
    "  *Scenario:* Petabytes of clickstream logs are in `s3://company-datalake/raw/clicks/`. You create `ext_raw_clicks` with a storage integration and query/load Parquet directly.\n",
    "\n",
    "  ```sql\n",
    "  CREATE OR REPLACE STORAGE INTEGRATION si_s3\n",
    "    TYPE=EXTERNAL_STAGE\n",
    "    STORAGE_PROVIDER=S3\n",
    "    ENABLED=TRUE\n",
    "    STORAGE_AWS_ROLE_ARN='arn:aws:iam::123456789012:role/snowflake-access-role'\n",
    "    STORAGE_ALLOWED_LOCATIONS=('s3://company-datalake/raw/clicks/');\n",
    "\n",
    "  CREATE OR REPLACE STAGE ext_raw_clicks\n",
    "    URL='s3://company-datalake/raw/clicks/'\n",
    "    STORAGE_INTEGRATION=si_s3\n",
    "    FILE_FORMAT=(TYPE=PARQUET);\n",
    "\n",
    "  COPY INTO clicks_raw FROM @ext_raw_clicks FILE_FORMAT=(TYPE=PARQUET);\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Show code to securely read Parquet from S3 using a **storage integration**\n",
    "\n",
    "👉 Already covered above, but here’s a clean version:\n",
    "\n",
    "```sql\n",
    "-- Step 1: Create a storage integration\n",
    "CREATE OR REPLACE STORAGE INTEGRATION si_s3_data\n",
    "  TYPE=EXTERNAL_STAGE\n",
    "  STORAGE_PROVIDER=S3\n",
    "  ENABLED=TRUE\n",
    "  STORAGE_AWS_ROLE_ARN='arn:aws:iam::123456789012:role/snowflake-access-role'\n",
    "  STORAGE_ALLOWED_LOCATIONS=('s3://company-datalake/parquet/');\n",
    "\n",
    "-- Step 2: Create external stage pointing to S3\n",
    "CREATE OR REPLACE STAGE ext_parquet_data\n",
    "  URL='s3://company-datalake/parquet/'\n",
    "  STORAGE_INTEGRATION=si_s3_data\n",
    "  FILE_FORMAT=(TYPE=PARQUET);\n",
    "\n",
    "-- Step 3: Load Parquet into table\n",
    "CREATE OR REPLACE TABLE sales_raw (\n",
    "  user_id STRING, amount NUMBER(10,2), ts TIMESTAMP_NTZ\n",
    ");\n",
    "\n",
    "COPY INTO sales_raw\n",
    "FROM @ext_parquet_data\n",
    "FILE_FORMAT=(TYPE=PARQUET);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) How do you avoid re-loading files you already loaded yesterday?\n",
    "\n",
    "👉 Use Snowflake’s **load history** (metadata).\n",
    "\n",
    "* By default, `COPY INTO` tracks loaded files in the **metadata table**. If a file with the same name is seen again, Snowflake **skips it** unless you set `FORCE=TRUE`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_raw\n",
    "FROM @stg_sales\n",
    "FILE_FORMAT=(TYPE=CSV SKIP_HEADER=1)\n",
    "ON_ERROR='CONTINUE'\n",
    "FORCE=FALSE;  -- avoids duplicates\n",
    "```\n",
    "\n",
    "* To check load history:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM INFORMATION_SCHEMA.LOAD_HISTORY WHERE TABLE_NAME='SALES_RAW';\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4) How do you validate a load before committing it?\n",
    "\n",
    "👉 Use `VALIDATION_MODE`.\n",
    "\n",
    "```sql\n",
    "-- Return up to 100 errors without loading\n",
    "COPY INTO sales_raw\n",
    "FROM @stg_sales\n",
    "FILE_FORMAT=(TYPE=CSV SKIP_HEADER=1)\n",
    "VALIDATION_MODE='RETURN_ERRORS';\n",
    "\n",
    "-- Check which files would be loaded\n",
    "COPY INTO sales_raw\n",
    "FROM @stg_sales\n",
    "FILE_FORMAT=(TYPE=CSV SKIP_HEADER=1)\n",
    "VALIDATION_MODE='RETURN_ALL_ERRORS';\n",
    "```\n",
    "\n",
    "This helps debug data formatting issues before loading into a table.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Difference between loading JSON into **VARIANT** vs. projecting JSON fields during `COPY`\n",
    "\n",
    "* **Load JSON as VARIANT** (raw storage):\n",
    "  Store the whole document as-is, flexible for schema changes. Query later.\n",
    "\n",
    "  ```sql\n",
    "  CREATE TABLE events_raw (data VARIANT);\n",
    "  COPY INTO events_raw FROM @stg_json FILE_FORMAT=(TYPE=JSON);\n",
    "  ```\n",
    "\n",
    "  Then query:\n",
    "\n",
    "  ```sql\n",
    "  SELECT data:\"user\"::string, data:\"event_type\"::string FROM events_raw;\n",
    "  ```\n",
    "\n",
    "* **Project JSON fields during load** (structured storage):\n",
    "  Extract and cast during `COPY`, store as typed columns. Faster queries, stricter schema.\n",
    "\n",
    "  ```sql\n",
    "  CREATE TABLE events_clean (user_id STRING, event_type STRING, ts TIMESTAMP_NTZ);\n",
    "\n",
    "  COPY INTO events_clean\n",
    "  FROM (\n",
    "    SELECT\n",
    "      $1:\"user\"::string,\n",
    "      $1:\"event_type\"::string,\n",
    "      TO_TIMESTAMP_NTZ($1:\"ts\") \n",
    "    FROM @stg_json (FILE_FORMAT => 'json_format')\n",
    "  );\n",
    "  ```\n",
    "\n",
    "👉 Use **VARIANT** if schema is fluid; **columns** if schema is fixed.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) How do you unload a filtered subset of data back to S3 with headers?\n",
    "\n",
    "```sql\n",
    "COPY INTO @ext_parquet_data/unloads/2025-08-22/\n",
    "FROM (\n",
    "  SELECT user_id, amount, ts\n",
    "  FROM sales_raw\n",
    "  WHERE ts::date='2025-08-22'\n",
    ")\n",
    "FILE_FORMAT=(TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"')\n",
    "HEADER=TRUE OVERWRITE=TRUE;\n",
    "```\n",
    "\n",
    "This writes the subset back to your external stage location in S3.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) If two teams need separate access to the same bucket prefix, how would you design **stages, roles, and patterns**?\n",
    "\n",
    "👉 Solution:\n",
    "\n",
    "* Create **two different external stages**, each pointing to the **same S3 bucket** but **restricted by allowed location/prefix**.\n",
    "* Assign **separate roles** with `USAGE` grants only on their stage.\n",
    "* Optionally, use `PATTERN` in `COPY INTO` jobs to ensure they load only their data.\n",
    "\n",
    "Example:\n",
    "\n",
    "```sql\n",
    "-- Team A stage\n",
    "CREATE OR REPLACE STAGE ext_teamA_stage\n",
    "  URL='s3://company-datalake/raw/teamA/'\n",
    "  STORAGE_INTEGRATION=si_s3_data\n",
    "  FILE_FORMAT=(TYPE=CSV);\n",
    "\n",
    "-- Team B stage\n",
    "CREATE OR REPLACE STAGE ext_teamB_stage\n",
    "  URL='s3://company-datalake/raw/teamB/'\n",
    "  STORAGE_INTEGRATION=si_s3_data\n",
    "  FILE_FORMAT=(TYPE=CSV);\n",
    "\n",
    "-- Grant only their stage to each role\n",
    "GRANT USAGE ON STAGE ext_teamA_stage TO ROLE teamA_role;\n",
    "GRANT USAGE ON STAGE ext_teamB_stage TO ROLE teamB_role;\n",
    "```\n",
    "\n",
    "👉 This way, both teams share the same S3 bucket but operate securely on separate “sub-parking lots”.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f4a9d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. **What is the difference between a traditional DWH staging schema and a Snowflake Stage?**\n",
    "\n",
    "* **Traditional DWH staging schema** → a schema + tables used as a *temporary landing zone* before transformations.\n",
    "* **Snowflake Stage** → a *storage location (internal or external)* for holding **files** (CSV, JSON, Parquet, etc.) before loading into tables.\n",
    "  👉 Big difference: In Snowflake, staging is about **files in storage**, not **rows in tables**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Explain the three types of stages in Snowflake (user, table, named) with use cases.**\n",
    "\n",
    "* **User Stage** (`@~`)\n",
    "\n",
    "  * Auto-created per user.\n",
    "  * Best for *personal, ad-hoc* data loading/testing.\n",
    "* **Table Stage** (`@%table_name`)\n",
    "\n",
    "  * Auto-created per table.\n",
    "  * Files staged here are directly tied to that table.\n",
    "  * Best for one-off loads specific to a table.\n",
    "* **Named Stage** (`@stage_name`)\n",
    "\n",
    "  * Explicitly created object, reusable across users/tables.\n",
    "  * Best for *production pipelines, Snowpipe, shared use*.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **How do internal stages differ from external stages in terms of management, security, and cost?**\n",
    "\n",
    "* **Internal stage**\n",
    "\n",
    "  * Snowflake manages storage inside the platform.\n",
    "  * Cost = Snowflake storage cost.\n",
    "  * Secure by default (encryption, RBAC).\n",
    "* **External stage**\n",
    "\n",
    "  * Files live in your cloud storage (S3, GCS, ADLS).\n",
    "  * Cost = cloud provider’s storage.\n",
    "  * You manage security (IAM roles, access policies).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **What’s the difference between defining an external stage with credentials vs. using a storage integration object?**\n",
    "\n",
    "* **Inline credentials** (hardcoded in stage)\n",
    "\n",
    "  * Credentials (AWS key, secret) directly stored in Snowflake.\n",
    "  * Easier for POCs, but insecure (password-like values in SQL).\n",
    "* **Storage Integration**\n",
    "\n",
    "  * Secure object managed by Snowflake.\n",
    "  * Uses **cloud IAM roles** with trust policies.\n",
    "  * Best practice for production (rotates creds automatically, no secrets in SQL).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Can you unload/export data from Snowflake into a stage? How?**\n",
    "\n",
    "✅ Yes. Using `COPY INTO @stage_name` from a table.\n",
    "\n",
    "Example:\n",
    "\n",
    "```sql\n",
    "COPY INTO @my_stage/orders_data\n",
    "FROM orders\n",
    "FILE_FORMAT = (TYPE = CSV)\n",
    "HEADER = TRUE;\n",
    "```\n",
    "\n",
    "This writes table rows → staged files.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Why might you use a table stage vs. a named stage?**\n",
    "\n",
    "* **Table stage**\n",
    "\n",
    "  * Good for one-time loads tied *only* to that table.\n",
    "  * Example: small CSV upload for `@%customers`.\n",
    "* **Named stage**\n",
    "\n",
    "  * Shared, reusable.\n",
    "  * Example: large pipelines, Snowpipe ingestion, multiple tables.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **What’s the lifecycle of files in an internal stage?**\n",
    "\n",
    "* Files in **internal stage** persist until explicitly removed (`REMOVE`).\n",
    "* **Special case**: If tied to a table stage, and the table is dropped → files remain **7 days** (Time Travel retention).\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **If your company already stores petabytes of logs in S3, would you use an internal or external stage? Why?**\n",
    "\n",
    "👉 **External stage.**\n",
    "\n",
    "* Data already exists in S3 → avoid double storage + cost.\n",
    "* Snowflake can directly query/load from S3 without moving.\n",
    "* Internal stage would require copying petabytes = \\$\\$\\$ + time.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **How do you ensure security and governance when multiple teams use the same stage?**\n",
    "\n",
    "* Use **Named stages** + RBAC (grant privileges only to required roles).\n",
    "* Use **Storage Integration** for secure, role-based cloud access.\n",
    "* Implement **directory structures + file naming conventions** for team separation.\n",
    "* Optionally, use **object tagging + monitoring** for governance.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Can stages be used with Snowpipe for continuous ingestion?**\n",
    "\n",
    "✅ Yes.\n",
    "\n",
    "* Snowpipe listens to a stage (internal or external).\n",
    "* Cloud events (S3, GCS, Azure) can **auto-trigger ingestion** into Snowflake when new files land.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb0632c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2. File Format\n",
    "\n",
    "### ❓ What is it?\n",
    "\n",
    "A **File Format** is like the **instruction manual** Snowflake needs to read the file correctly.\n",
    "Think: if someone gave you a book in French, you’d need to know “the language” first before you could read it. File Format tells Snowflake:\n",
    "\n",
    "* What delimiter to expect (CSV? Comma? Tab?)\n",
    "* Is the first line a header?\n",
    "* Is the file compressed? If yes, how?\n",
    "\n",
    "### ❓ Purpose of it?\n",
    "\n",
    "Without a file format, Snowflake wouldn’t know how to interpret the incoming file.\n",
    "Example: If your file is `data.csv` with columns separated by commas, and you don’t tell Snowflake about the delimiter, it might read the whole line as a single column.\n",
    "So, **purpose** = to ensure correct parsing of file → correct data loading.\n",
    "\n",
    "### ⚡ Most Important Parameters\n",
    "\n",
    "These depend on file type, but here are the big ones:\n",
    "\n",
    "* **TYPE** → CSV, JSON, PARQUET, AVRO, ORC, XML\n",
    "* **FIELD\\_DELIMITER** → For CSV/TSV (comma, tab, pipe)\n",
    "* **SKIP\\_HEADER** → Skip header rows\n",
    "* **FIELD\\_OPTIONALLY\\_ENCLOSED\\_BY** → Handle quoted text (e.g., `\"Hello, World\"`)\n",
    "* **NULL\\_IF** → Define what represents NULL (like empty string `''` or `'NULL'`)\n",
    "* **COMPRESSION** → gzip, bzip2, none\n",
    "* **DATE\\_FORMAT, TIME\\_FORMAT, TIMESTAMP\\_FORMAT** → If loading date/time data\n",
    "\n",
    "👉 Example:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT my_csv_format\n",
    "  TYPE = CSV\n",
    "  FIELD_DELIMITER = ','\n",
    "  SKIP_HEADER = 1\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  NULL_IF = ('NULL', 'null')\n",
    "  COMPRESSION = AUTO;\n",
    "```\n",
    "\n",
    "This tells Snowflake:\n",
    "“Expect a CSV file, fields separated by commas, ignore the first row, handle quoted text properly, treat 'NULL' or 'null' as NULL, and automatically detect compression.”\n",
    "\n",
    "📌 Pro Tip: **File Formats are reusable** → You create once, and use in multiple COPY INTO commands.\n",
    "\n",
    "---\n",
    "\n",
    "2. **PUT and GET Commands**\n",
    "\n",
    "   * `PUT` → Upload local file → stage\n",
    "   * `GET` → Download staged file → local\n",
    "\n",
    "---\n",
    "\n",
    "3. **Data Unloading**\n",
    "\n",
    "   * Stages aren’t only for loading data INTO Snowflake.\n",
    "   * You can also **export data from tables into files** in a stage.\n",
    "\n",
    "   ```sql\n",
    "   COPY INTO @sales_stage\n",
    "   FROM sales\n",
    "   FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = '|');\n",
    "   ```\n",
    "\n",
    "   This writes the sales table data back into files.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Best Practices**\n",
    "\n",
    "   * Use **Named Stages** for production pipelines.\n",
    "   * Use **External Stages** if data already lives in S3/GCS/Azure → avoid double storage.\n",
    "   * Use **Table/User stages** only for quick testing.\n",
    "   * Always define **File Formats** instead of inline parameters to keep things consistent.\n",
    "   * Always use **compression (gzip)** for large files → reduces upload/download time.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Must-Know Questions (for mastery)\n",
    "\n",
    "1. What is a Stage in Snowflake, and why do we need it?\n",
    "2. Difference between Internal vs External stages?\n",
    "3. Difference between User Stage, Table Stage, and Named Stage?\n",
    "4. When would you use `@~`, `@%table_name`, and `@stage_name`?\n",
    "5. Can a table stage be dropped? Why/why not?\n",
    "6. What is the purpose of a File Format in Snowflake?\n",
    "7. How do you specify file format — inline vs named file format?\n",
    "8. Explain `PUT` and `GET` with examples.\n",
    "9. How would you design a pipeline where files arrive daily in S3 and must be ingested into Snowflake? (external stage answer expected).\n",
    "10. What’s the advantage of using compression in staging?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e8e4d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ❓ Must-Know Questions on Snowflake Stages\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **What is a Stage in Snowflake, and why do we need it?**\n",
    "\n",
    "* A **Stage** is a location in Snowflake where files are temporarily stored **before loading into a table** or **after unloading from a table**.\n",
    "* Think of it like a *buffer room* between your external data sources and your Snowflake tables.\n",
    "\n",
    "✅ **Why do we need it?**\n",
    "\n",
    "* Because raw files often come in various formats (CSV, JSON, Parquet) and may be compressed.\n",
    "* Snowflake needs a staging area where the files can be:\n",
    "\n",
    "  * Uploaded (using `PUT`)\n",
    "  * Downloaded (using `GET`)\n",
    "  * Processed and loaded into tables (using `COPY INTO`)\n",
    "* Without a stage, we’d have no systematic way to organize and load data efficiently.\n",
    "\n",
    "📌 Example:\n",
    "Business sends you `sales.csv` daily → You upload to stage → Then load into `sales` table.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Difference between Internal vs External stages?**\n",
    "\n",
    "| Feature  | Internal Stage 🏠 (inside Snowflake)                 | External Stage 🌍 (outside Snowflake, in cloud storage)   |\n",
    "| -------- | ---------------------------------------------------- | --------------------------------------------------------- |\n",
    "| Location | Snowflake-managed storage                            | External storage (S3, Azure Blob, GCS)                    |\n",
    "| Cost     | Files stored count towards Snowflake storage billing | Files stored in your cloud account                        |\n",
    "| Use Case | Quick testing, temporary pipelines                   | Enterprise pipelines with data lakes                      |\n",
    "| Access   | Managed fully by Snowflake                           | Need credentials (AWS IAM keys, Azure SAS token, GCP key) |\n",
    "| Example  | `@~` , `@%table_name` , `@stage_name`                | `@my_s3_stage` linked to `s3://bucket/path/`              |\n",
    "\n",
    "✅ **Rule of thumb:**\n",
    "\n",
    "* Use **Internal Stages** when uploading small test files or ad-hoc analysis.\n",
    "* Use **External Stages** when raw data is already sitting in S3/GCS/Azure → avoids duplication.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Difference between User Stage, Table Stage, and Named Stage?**\n",
    "\n",
    "| Stage Type      | Representation | Use Case                                          | Limitations                                       |\n",
    "| --------------- | -------------- | ------------------------------------------------- | ------------------------------------------------- |\n",
    "| **User Stage**  | `@~`           | Personal “locker” for quick testing               | Only accessible by the user                       |\n",
    "| **Table Stage** | `@%table_name` | Storage tied to a table, multiple users can share | Can’t be dropped/altered, no file format metadata |\n",
    "| **Named Stage** | `@stage_name`  | Flexible, reusable, sharable stage object         | Needs explicit creation and grants                |\n",
    "\n",
    "📌 Example:\n",
    "\n",
    "```sql\n",
    "-- User stage\n",
    "PUT file://data.csv @~;\n",
    "\n",
    "-- Table stage\n",
    "PUT file://data.csv @%employees;\n",
    "\n",
    "-- Named stage\n",
    "CREATE OR REPLACE STAGE sales_stage FILE_FORMAT = my_csv_format;\n",
    "PUT file://data.csv @sales_stage;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **When would you use `@~`, `@%table_name`, and `@stage_name`?**\n",
    "\n",
    "* `@~` → When I’m testing with my own file, quick one-time load.\n",
    "  *Scenario: Data engineer testing new schema with one CSV.*\n",
    "\n",
    "* `@%table_name` → When the file is specific to that table and multiple users may need it.\n",
    "  *Scenario: HR team loads monthly employee files into `employees` table.*\n",
    "\n",
    "* `@stage_name` → When I want flexibility, multiple users, multiple tables, or external data.\n",
    "  *Scenario: Finance team shares sales files with multiple teams, stored in `@sales_stage`.*\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Can a table stage be dropped? Why/why not?**\n",
    "\n",
    "* ❌ **No. A table stage cannot be dropped.**\n",
    "* Reason: Table stages are **system-generated** and tied directly to the table’s lifecycle.\n",
    "\n",
    "  * If you drop the table, the stage disappears automatically.\n",
    "  * But you can’t drop just the stage.\n",
    "\n",
    "✅ Think of it like: *If you own a house, the basement comes with it — you can’t drop the basement without demolishing the house.*\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **What is the purpose of a File Format in Snowflake?**\n",
    "\n",
    "* File Format tells Snowflake **how to interpret the raw file** when loading/unloading.\n",
    "* Without it, Snowflake can’t parse the file correctly.\n",
    "\n",
    "📌 Example:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT my_csv_format\n",
    "  TYPE = CSV\n",
    "  FIELD_DELIMITER = ','\n",
    "  SKIP_HEADER = 1;\n",
    "```\n",
    "\n",
    "This tells Snowflake to expect CSV with commas and skip the first row.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **How do you specify file format — inline vs named file format?**\n",
    "\n",
    "* **Named File Format** → Reusable object in Snowflake.\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO employees\n",
    "  FROM @sales_stage\n",
    "  FILE_FORMAT = (FORMAT_NAME = my_csv_format);\n",
    "  ```\n",
    "\n",
    "* **Inline File Format** → Define directly in the command (good for one-off loads).\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO employees\n",
    "  FROM @sales_stage\n",
    "  FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ',' SKIP_HEADER = 1);\n",
    "  ```\n",
    "\n",
    "✅ Best practice: Use **Named File Formats** for production pipelines → consistency.\n",
    "Use inline formats only for quick experiments.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Explain `PUT` and `GET` with examples.**\n",
    "\n",
    "* `PUT` → Upload file from **local system → stage**\n",
    "\n",
    "  ```sql\n",
    "  PUT file://C:/data/employees.csv @%employees;\n",
    "  ```\n",
    "\n",
    "* `GET` → Download file from **stage → local system**\n",
    "\n",
    "  ```sql\n",
    "  GET @%employees file://C:/download/;\n",
    "  ```\n",
    "\n",
    "📌 Use case:\n",
    "\n",
    "* `PUT` when preparing to load data into Snowflake.\n",
    "* `GET` when you want to take processed data out of Snowflake.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **How would you design a pipeline where files arrive daily in S3 and must be ingested into Snowflake?**\n",
    "\n",
    "**Answer:** Use an **External Stage**.\n",
    "\n",
    "1. Create external stage pointing to S3 bucket:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE s3_stage\n",
    "  URL='s3://company-data/sales/'\n",
    "  CREDENTIALS=(AWS_KEY_ID='xxxx' AWS_SECRET_KEY='yyyy')\n",
    "  FILE_FORMAT = my_csv_format;\n",
    "```\n",
    "\n",
    "2. Use `COPY INTO` to load data from stage into table:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales\n",
    "FROM @s3_stage\n",
    "FILE_FORMAT = (FORMAT_NAME = my_csv_format)\n",
    "ON_ERROR = 'CONTINUE';\n",
    "```\n",
    "\n",
    "✅ Benefit: No need to re-upload files. Snowflake directly reads from S3.\n",
    "This is the **enterprise way** of doing ingestion pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **What’s the advantage of using compression in staging?**\n",
    "\n",
    "* Faster file transfer (`PUT`/`GET`) because less data moves across the network.\n",
    "* Lower storage costs (compressed file consumes less space).\n",
    "* Snowflake auto-detects compression (gzip, bzip2, etc.) → no manual decompression needed.\n",
    "\n",
    "📌 Example: If your 1 GB CSV is compressed to 100 MB gzip:\n",
    "\n",
    "* Upload 10x faster.\n",
    "* Pay less for storage.\n",
    "* Snowflake still reads it directly.\n",
    "\n",
    "✅ Always compress large files before uploading.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2789fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
