{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab032849",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üöÄ Snowflake COPY Command Deep Dive\n",
    "\n",
    "## 1. What is COPY Command in Snowflake?\n",
    "\n",
    "Think of Snowflake as a house (your **database**), and you want to move your furniture (data) from a truck (files in **S3, Azure Blob, GCS, or Snowflake Stage**) into the house (a **table**).\n",
    "\n",
    "The **COPY INTO <table>** command is the ‚Äúmovers team‚Äù that takes care of this. It knows how to:\n",
    "\n",
    "* Pick which boxes (files) to load\n",
    "* Unpack them according to format (CSV, JSON, Parquet, Avro, ORC)\n",
    "* Handle errors if some items don‚Äôt fit well (bad records)\n",
    "* Keep track of what‚Äôs already loaded (no double moving unless you ask for it)\n",
    "\n",
    "So COPY Command is **the backbone of all ETL/ELT in Snowflake**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. File Level Options\n",
    "\n",
    "### üîπ The `FILES` Option\n",
    "\n",
    "This is when you want to **load specific files** instead of all files in a stage.\n",
    "\n",
    "**Scenario:**\n",
    "Your upstream system drops daily sales data into an S3 bucket like this:\n",
    "\n",
    "```\n",
    "s3://company-data/sales/\n",
    " ‚îú‚îÄ‚îÄ sales_20250901.csv\n",
    " ‚îú‚îÄ‚îÄ sales_20250902.csv\n",
    " ‚îú‚îÄ‚îÄ sales_20250903.csv\n",
    " ‚îú‚îÄ‚îÄ sales_20250904.csv\n",
    "```\n",
    "\n",
    "Normally, if you run:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_s3_stage/sales/\n",
    "FILE_FORMAT = (TYPE = CSV);\n",
    "```\n",
    "\n",
    "üëâ Snowflake will try to load **all four files**.\n",
    "\n",
    "But let‚Äôs say you only want `sales_20250903.csv`. Then:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_s3_stage/sales/\n",
    "FILES = ('sales_20250903.csv')\n",
    "FILE_FORMAT = (TYPE = CSV);\n",
    "```\n",
    "\n",
    "That‚Äôs **FILE option in action.**\n",
    "\n",
    "‚ö†Ô∏è **Important restriction**: You **can‚Äôt use negation (like `NOT 'sales_20250903.csv'`)** with `FILES`.\n",
    "Snowflake doesn‚Äôt support exclusion here ‚Äî it only allows you to **pick explicitly**.\n",
    "\n",
    "üëâ If you want exclusion, you must use `PATTERN` instead (we‚Äôll cover that below).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ `ON_ERROR = CONTINUE` vs `ON_ERROR = ABORT`\n",
    "\n",
    "Errors happen when:\n",
    "\n",
    "* A column has wrong data type\n",
    "* Missing delimiters in CSV\n",
    "* Unexpected JSON structure\n",
    "\n",
    "**Scenario Example:**\n",
    "Your file has 1000 rows. Out of them, 5 rows are malformed.\n",
    "\n",
    "* `ON_ERROR = ABORT_STATEMENT` (default):\n",
    "  Snowflake **stops everything**. Nothing gets loaded.\n",
    "  It‚Äôs like movers finding a broken box and refusing to unload the entire truck.\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO sales_table\n",
    "  FROM @my_s3_stage/sales/\n",
    "  FILE_FORMAT = (TYPE = CSV)\n",
    "  ON_ERROR = 'ABORT_STATEMENT';\n",
    "  ```\n",
    "\n",
    "* `ON_ERROR = CONTINUE`:\n",
    "  Snowflake **loads all valid rows**, skips the bad ones.\n",
    "  So 995 rows get loaded, 5 rejected.\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO sales_table\n",
    "  FROM @my_s3_stage/sales/\n",
    "  FILE_FORMAT = (TYPE = CSV)\n",
    "  ON_ERROR = 'CONTINUE';\n",
    "  ```\n",
    "\n",
    "üëâ Which one to use?\n",
    "\n",
    "* Use `ABORT` when data quality must be 100% strict (like financial transactions).\n",
    "* Use `CONTINUE` when you‚Äôd rather not block the pipeline (like logs or clickstream).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ The `PATTERN` Option\n",
    "\n",
    "This is super powerful because often data is **partitioned by date or category** in S3.\n",
    "\n",
    "**Example: Logs in S3**\n",
    "\n",
    "```\n",
    "s3://company-data/logs/\n",
    " ‚îú‚îÄ‚îÄ date=2025-09-01/part-000.csv\n",
    " ‚îú‚îÄ‚îÄ date=2025-09-02/part-001.csv\n",
    " ‚îú‚îÄ‚îÄ date=2025-09-03/part-002.csv\n",
    "```\n",
    "\n",
    "Now you want to only load **Sept 2nd logs**. You can use:\n",
    "\n",
    "```sql\n",
    "COPY INTO logs_table\n",
    "FROM @my_s3_stage/logs/\n",
    "PATTERN = '.*date=2025-09-02/.*[.]csv'\n",
    "FILE_FORMAT = (TYPE = CSV);\n",
    "```\n",
    "\n",
    "üëâ PATTERN uses **regex**. Common examples:\n",
    "\n",
    "* `'.*202509.*'` ‚Üí load all September 2025 files\n",
    "* `'.*2025090[1-5].*'` ‚Üí load files from Sept 1‚Äì5\n",
    "* `'.*\\.json'` ‚Üí load only JSON files\n",
    "\n",
    "üìå Remember: PATTERN is where you can ‚Äúexclude‚Äù indirectly by choosing regex that ignores certain files.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Deeper into ON\\_ERROR\n",
    "\n",
    "We already saw CONTINUE vs ABORT. But let‚Äôs go deeper into **reject handling**.\n",
    "\n",
    "Snowflake allows you to **capture rejected rows** for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ How to capture rejected records\n",
    "\n",
    "After a COPY command runs, you can query the **load history**:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM INFORMATION_SCHEMA.LOAD_HISTORY\n",
    "ORDER BY LAST_LOAD_TIME DESC;\n",
    "```\n",
    "\n",
    "Each COPY operation gets a **query ID**.\n",
    "\n",
    "üëâ You can also fetch the query ID of your last COPY like this:\n",
    "\n",
    "```sql\n",
    "SELECT LAST_QUERY_ID();\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Query rejected records\n",
    "\n",
    "Snowflake stores rejected rows in the **`VALIDATION_MODE`** or via system function `VALIDATE`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_s3_stage/sales/\n",
    "FILE_FORMAT = (TYPE = CSV)\n",
    "ON_ERROR = 'CONTINUE';\n",
    "```\n",
    "\n",
    "Now, to check rejected rows:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM TABLE(VALIDATE(\n",
    "  sales_table,\n",
    "  JOB_ID => '<query_id_of_copy_command>'\n",
    "));\n",
    "```\n",
    "\n",
    "This gives you details like:\n",
    "\n",
    "* File name\n",
    "* Line number\n",
    "* Column parsing error\n",
    "* Row content\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Create a table of rejected rows\n",
    "\n",
    "You can persist rejected records for debugging:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE rejected_sales AS\n",
    "SELECT *\n",
    "FROM TABLE(VALIDATE(\n",
    "  sales_table,\n",
    "  JOB_ID => '<query_id_of_copy_command>'\n",
    "));\n",
    "```\n",
    "\n",
    "Now you can analyze why they failed ‚Äî maybe wrong delimiter, missing column, or wrong encoding.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Must-Know Questions to be Ready\n",
    "\n",
    "Here are the questions I would grill you on if you joined my team:\n",
    "\n",
    "1. What‚Äôs the difference between `FILES` and `PATTERN` in COPY command?\n",
    "2. Why can‚Äôt you use negation with `FILES`? How do you handle exclusion instead?\n",
    "3. When would you choose `ON_ERROR = CONTINUE` vs `ON_ERROR = ABORT_STATEMENT` in a pipeline?\n",
    "4. How do you capture rejected rows after a COPY command?\n",
    "5. Show me how you‚Äôd get the query ID of a COPY operation.\n",
    "6. How would you load only ‚Äúlast 3 days of data‚Äù from a partitioned S3 folder using PATTERN?\n",
    "7. If you want to load zipped files, how would you configure COPY command?\n",
    "8. What happens if you run COPY command multiple times on the same file? How does Snowflake handle duplicates?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8b2e9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üìå Must-Know COPY Command Questions (with Detailed Answers)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **What‚Äôs the difference between `FILES` and `PATTERN` in COPY command?**\n",
    "\n",
    "* **FILES**\n",
    "\n",
    "  * Lets you load **specific files by name**.\n",
    "  * Example:\n",
    "\n",
    "    ```sql\n",
    "    COPY INTO sales_table\n",
    "    FROM @my_s3_stage/sales/\n",
    "    FILES = ('sales_20250901.csv', 'sales_20250902.csv')\n",
    "    FILE_FORMAT = (TYPE = CSV);\n",
    "    ```\n",
    "  * Works well if you know exact filenames.\n",
    "\n",
    "* **PATTERN**\n",
    "\n",
    "  * Uses **regex** to load files that match a naming pattern.\n",
    "  * Example:\n",
    "\n",
    "    ```sql\n",
    "    COPY INTO sales_table\n",
    "    FROM @my_s3_stage/sales/\n",
    "    PATTERN = '.*2025090[1-5].*'\n",
    "    FILE_FORMAT = (TYPE = CSV);\n",
    "    ```\n",
    "  * Works best for **partitioned datasets** (like date folders or daily dumps).\n",
    "\n",
    "üëâ **Key difference**:\n",
    "\n",
    "* `FILES` = Explicit filenames (like picking items by hand).\n",
    "* `PATTERN` = Regex-based bulk selection (like setting rules).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Why can‚Äôt you use negation with `FILES`? How do you handle exclusion instead?**\n",
    "\n",
    "* `FILES` only accepts **explicit file lists**. It doesn‚Äôt support `NOT` or `EXCEPT`.\n",
    "* Example ‚ùå (invalid):\n",
    "\n",
    "  ```sql\n",
    "  FILES != ('sales_20250901.csv')\n",
    "  ```\n",
    "\n",
    "üëâ **Solution**: Use `PATTERN`.\n",
    "If you want everything **except Sept 1st**, you can do:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_s3_stage/sales/\n",
    "PATTERN = '^(?!.*20250901).*\\.csv'\n",
    "FILE_FORMAT = (TYPE = CSV);\n",
    "```\n",
    "\n",
    "This regex means: ‚ÄúLoad all `.csv` files that **do not contain 20250901**.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **When would you choose `ON_ERROR = CONTINUE` vs `ON_ERROR = ABORT_STATEMENT` in a pipeline?**\n",
    "\n",
    "* **ON\\_ERROR = ABORT\\_STATEMENT**\n",
    "\n",
    "  * Pipeline **fails immediately** if even one bad record is found.\n",
    "  * Use when:\n",
    "\n",
    "    * Data must be **100% correct**.\n",
    "    * Example: Bank transactions ‚Üí you cannot afford to skip rows.\n",
    "\n",
    "* **ON\\_ERROR = CONTINUE**\n",
    "\n",
    "  * Pipeline **loads valid rows**, skips bad ones.\n",
    "  * Use when:\n",
    "\n",
    "    * You prefer data flow to continue even if some rows are bad.\n",
    "    * Example: Web logs ‚Üí one malformed row shouldn‚Äôt block millions of valid rows.\n",
    "\n",
    "üëâ Rule of thumb:\n",
    "\n",
    "* **Financial/critical systems** ‚Üí `ABORT`\n",
    "* **High-volume semi-structured data** ‚Üí `CONTINUE`\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **How do you capture rejected rows after a COPY command?**\n",
    "\n",
    "After running COPY:\n",
    "\n",
    "1. Get the **query ID**:\n",
    "\n",
    "   ```sql\n",
    "   SELECT LAST_QUERY_ID();\n",
    "   ```\n",
    "\n",
    "2. Check rejected rows:\n",
    "\n",
    "   ```sql\n",
    "   SELECT *\n",
    "   FROM TABLE(VALIDATE(\n",
    "     sales_table,\n",
    "     JOB_ID => '<query_id>'\n",
    "   ));\n",
    "   ```\n",
    "\n",
    "üëâ This returns: file name, line number, error reason, and bad row content.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Show me how you‚Äôd get the query ID of a COPY operation.**\n",
    "\n",
    "Simply run:\n",
    "\n",
    "```sql\n",
    "SELECT LAST_QUERY_ID();\n",
    "```\n",
    "\n",
    "This gives the ID of your **last query in the session**, which includes COPY commands.\n",
    "\n",
    "üëâ You can also fetch query history from:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY_BY_USER(RESULT_LIMIT => 5));\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **How would you load only ‚Äúlast 3 days of data‚Äù from a partitioned S3 folder using PATTERN?**\n",
    "\n",
    "Assume your S3 data is partitioned like this:\n",
    "\n",
    "```\n",
    "s3://company-data/logs/\n",
    " ‚îú‚îÄ‚îÄ date=20250910/file1.csv\n",
    " ‚îú‚îÄ‚îÄ date=20250911/file2.csv\n",
    " ‚îú‚îÄ‚îÄ date=20250912/file3.csv\n",
    " ‚îú‚îÄ‚îÄ date=20250913/file4.csv\n",
    "```\n",
    "\n",
    "üëâ To load **Sept 11‚Äì13 only**:\n",
    "\n",
    "```sql\n",
    "COPY INTO logs_table\n",
    "FROM @my_s3_stage/logs/\n",
    "PATTERN = '.*date=202509(11|12|13)/.*\\.csv'\n",
    "FILE_FORMAT = (TYPE = CSV);\n",
    "```\n",
    "\n",
    "Here `(11|12|13)` is regex for the last 3 days.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **If you want to load zipped files, how would you configure COPY command?**\n",
    "\n",
    "Snowflake can natively **decompress files** when loading.\n",
    "\n",
    "Example for a GZIP file:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_s3_stage/sales/\n",
    "FILE_FORMAT = (TYPE = CSV COMPRESSION = GZIP);\n",
    "```\n",
    "\n",
    "Supported compression types: `AUTO, GZIP, BZ2, BROTLI, ZSTD, DEFLATE, RAW_DEFLATE`\n",
    "\n",
    "üëâ Snowflake detects automatically if `COMPRESSION = AUTO` is set.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **What happens if you run COPY command multiple times on the same file? How does Snowflake handle duplicates?**\n",
    "\n",
    "By default, **Snowflake prevents loading the same file twice** in the same table.\n",
    "\n",
    "It tracks this using **metadata in the load history**.\n",
    "\n",
    "* If you try to load again:\n",
    "\n",
    "  * Snowflake **skips already loaded files**.\n",
    "  * Unless you use `FORCE = TRUE`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_s3_stage/sales/\n",
    "FILE_FORMAT = (TYPE = CSV)\n",
    "FORCE = TRUE;\n",
    "```\n",
    "\n",
    "üëâ `FORCE = TRUE` ignores history and reloads files (can cause duplicates).\n",
    "\n",
    "So best practice:\n",
    "\n",
    "* Keep `FORCE = FALSE` for production loads.\n",
    "* Use staging tables if you need to reload for debugging.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Quick Recap\n",
    "\n",
    "1. `FILES` = explicit list, `PATTERN` = regex\n",
    "2. Negation ‚ùå with `FILES`, ‚úÖ with `PATTERN`\n",
    "3. `ON_ERROR = ABORT` ‚Üí strict data, `CONTINUE` ‚Üí flexible pipelines\n",
    "4. Rejected rows captured with `VALIDATE` and `LAST_QUERY_ID()`\n",
    "5. Query ID via `LAST_QUERY_ID()` or `QUERY_HISTORY`\n",
    "6. Load recent partitions via regex in `PATTERN`\n",
    "7. Compressed files handled by `FILE_FORMAT (COMPRESSION = ‚Ä¶)`\n",
    "8. COPY avoids duplicates by default unless `FORCE = TRUE`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f484a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
