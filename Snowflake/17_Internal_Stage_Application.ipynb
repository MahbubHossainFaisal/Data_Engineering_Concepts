{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b8f736",
   "metadata": {},
   "source": [
    "# Snowflake Staging (Internal Stages) — a practical, story-driven deep dive\n",
    "\n",
    "*Mentor hat on.* We’ll build your mental model first, then get hands-on with a realistic “big file” load using SnowSQL, and finish with a crisp checklist and must-know questions. I’ll also gently fix any common misconceptions along the way.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) The mental model: what is a **stage**?\n",
    "\n",
    "Think of a **stage** as Snowflake’s “loading dock.” Files arrive here first, then forklifts (**COPY INTO**) move rows into your tables. Stages live **inside Snowflake** (internal) or **outside** (external: S3/Azure/GCS).\n",
    "Your focus today is **internal** stages: Snowflake stores the files for you, secures them, and you pay Snowflake storage.\n",
    "\n",
    "### Three flavors you’ll see (all can be internal)\n",
    "\n",
    "* **User stage**: `@~` — private scratchpad per user (great for ad-hoc).\n",
    "* **Table stage**: `@%table_name` — automatically created for each table; perfect when files are **only for that table**.\n",
    "* **Named stage**: `@mystage` — reusable object you `CREATE STAGE`. Add defaults (file format, copy options), organize subfolders/prefixes, and share with roles/teams.\n",
    "\n",
    "> **Misconception to fix**: “Table stage is always best for a single table.”\n",
    "> *Usually true*, but if you need broader access control, reusable defaults, or structured subfolders, a **named stage** can still be the better choice—even for a single table.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Core commands you’ll actually use (cheat-sheet)\n",
    "\n",
    "**Staging file management**\n",
    "\n",
    "```sql\n",
    "-- See files\n",
    "LIST @%orders;                      -- table stage\n",
    "LIST @mystage/incoming/;            -- named stage with a \"subfolder\" (prefix)\n",
    "\n",
    "-- Delete files in a stage\n",
    "REMOVE @%orders PATTERN='.*\\.bad$'; -- delete only *.bad\n",
    "```\n",
    "\n",
    "**Move data between local ↔ stage (SnowSQL only)**\n",
    "\n",
    "```sql\n",
    "-- Upload from your laptop/server to a stage\n",
    "PUT file://C:\\loads\\orders_2025_08_24.csv @%orders AUTO_COMPRESS=TRUE PARALLEL=8 OVERWRITE=FALSE;\n",
    "\n",
    "-- Download from a stage to your machine\n",
    "GET @%orders file://C:\\downloads\\orders_backup\\ PARALLEL=8 PATTERN='.*2025_08_24.*';\n",
    "```\n",
    "\n",
    "**Load from stage → table**\n",
    "\n",
    "```sql\n",
    "COPY INTO ORDERS\n",
    "  FROM @%orders\n",
    "  FILE_FORMAT=(FORMAT_NAME = ff_orders_csv)\n",
    "  PATTERN='.*orders_2025_08_24.*'\n",
    "  ON_ERROR='ABORT_STATEMENT';\n",
    "```\n",
    "\n",
    "**(Optional) Unload from table → stage**\n",
    "\n",
    "```sql\n",
    "COPY INTO @mystage/exports/orders_2025_08_24/\n",
    "  FROM (SELECT * FROM ORDERS WHERE order_date = '2025-08-24')\n",
    "  FILE_FORMAT=(TYPE=CSV COMPRESSION=GZIP);\n",
    "```\n",
    "\n",
    "> Tip: After any `COPY INTO`, inspect the result instantly:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) File formats (the “decoder ring”)\n",
    "\n",
    "Create **file format objects** once; reuse everywhere. They make complex CSVs, JSON, or Parquet easy and consistent.\n",
    "\n",
    "**CSV example**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT ff_orders_csv\n",
    "  TYPE = CSV\n",
    "  FIELD_DELIMITER = ','\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  SKIP_HEADER = 1\n",
    "  NULL_IF = ('\\\\N','NULL','')\n",
    "  EMPTY_FIELD_AS_NULL = TRUE\n",
    "  TRIM_SPACE = TRUE;\n",
    "```\n",
    "\n",
    "**JSON example (newlines JSON)**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT ff_json_lines\n",
    "  TYPE = JSON\n",
    "  STRIP_OUTER_ARRAY = TRUE; -- if each file contains one big array\n",
    "```\n",
    "\n",
    "**Parquet example**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT ff_parquet TYPE = PARQUET;\n",
    "```\n",
    "\n",
    "> **Gotchas**\n",
    "> • If your local file is **already gzipped** (`.gz`), don’t double-compress: use `AUTO_COMPRESS=FALSE` in `PUT` and set `SOURCE_COMPRESSION=GZIP` if needed.\n",
    "> • For CSVs with commas/newlines **inside quotes**, make sure `FIELD_OPTIONALLY_ENCLOSED_BY='\"'` is set, or you’ll get column-count errors.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Table stage vs Named stage (when to use what)\n",
    "\n",
    "| Use case                                                      | Table stage `@%table`               | Named stage `@mystage`              |\n",
    "| ------------------------------------------------------------- | ----------------------------------- | ----------------------------------- |\n",
    "| Files only for one table                                      | ✅ Best fit                          | Possible but not necessary          |\n",
    "| Reuse defaults (file format / copy options) across many loads | Meh                                 | ✅ Strong                            |\n",
    "| Team collaboration / fine-grained privileges                  | Limited (inherits table privileges) | ✅ Grant READ/WRITE/USAGE explicitly |\n",
    "| Clean subfolder conventions (`incoming/processed/failed/`)    | Basic                               | ✅ Clean and scalable                |\n",
    "| Long-lived pipelines                                          | OK                                  | ✅ Prefer                            |\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Best practices (internal stages)\n",
    "\n",
    "1. **Right-size your files**: target **100–250 MB compressed** for fast parallel loads. Avoid one giant monolith; split if possible.\n",
    "2. **Create and reuse file formats**; don’t inline all options in every `COPY`.\n",
    "3. **Test the load** before committing: use `VALIDATION_MODE`.\n",
    "4. **Decide your error policy up front**: `ON_ERROR` = `'ABORT_STATEMENT' | 'CONTINUE' | 'SKIP_FILE' | 'SKIP_FILE_<n>'`.\n",
    "5. **Idempotency**: keep original filenames stable; Snowflake tracks loaded files and **skips duplicates** unless you set `FORCE=TRUE`.\n",
    "6. **Cleanup**: use `PURGE=TRUE` in `COPY` or `REMOVE` after successful loads to reduce storage cost.\n",
    "7. **Security**: grant the **least** privileges (READ/WRITE on named stages only to loaders), and prefer named stages for shared pipelines.\n",
    "8. **Observability**: capture `LAST_QUERY_ID()` from `COPY` and store the load stats; keep a lightweight load history table.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) The scenario: “Upload a **large** file for one table using SnowSQL + table stage”\n",
    "\n",
    "### The story\n",
    "\n",
    "You’re the data engineer for **RiverRetail**. Finance sent you a chunky CSV: `orders_2025_08_24.csv` (15 GB). It belongs to a single table `ORDERS`. You’ll use the **table stage** (`@%ORDERS`) and **SnowSQL** (CLI) because the web UI is meant for smaller files and lacks the advanced knobs we need.\n",
    "\n",
    "#### a) Prepare the target table\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE ORDERS (\n",
    "  order_id           NUMBER,\n",
    "  order_ts           TIMESTAMP_NTZ,\n",
    "  customer_id        NUMBER,\n",
    "  amount             NUMBER(12,2),\n",
    "  currency           VARCHAR(3),\n",
    "  notes              STRING\n",
    ");\n",
    "```\n",
    "\n",
    "#### b) Create a reusable file format (CSV with header, quoted fields)\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT ff_orders_csv\n",
    "  TYPE = CSV\n",
    "  FIELD_DELIMITER = ','\n",
    "  SKIP_HEADER = 1\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  NULL_IF = ('','NULL')\n",
    "  EMPTY_FIELD_AS_NULL = TRUE\n",
    "  TRIM_SPACE = TRUE;\n",
    "```\n",
    "\n",
    "#### c) Upload with **SnowSQL** `PUT`\n",
    "\n",
    "From your terminal (connected with a role that owns/has rights on `ORDERS`):\n",
    "\n",
    "```bash\n",
    "snowsql -a <account> -u <user>\n",
    "-- inside snowsql:\n",
    "PUT file://C:\\RiverRetail\\loads\\orders_2025_08_24.csv @%ORDERS \\\n",
    "    AUTO_COMPRESS=TRUE PARALLEL=8 OVERWRITE=FALSE;\n",
    "```\n",
    "\n",
    "**Why these options?**\n",
    "\n",
    "* `AUTO_COMPRESS=TRUE` → client compresses before upload (smaller, faster).\n",
    "* `PARALLEL=8` → concurrent parts (tune based on local CPU/network).\n",
    "* `OVERWRITE=FALSE` → don’t clobber if a file with the same name already exists.\n",
    "\n",
    "> **If file is already `.gz`**:\n",
    "> Use `AUTO_COMPRESS=FALSE` and (optionally) `SOURCE_COMPRESSION=GZIP`:\n",
    ">\n",
    "> ```sql\n",
    "> PUT file://.../orders_2025_08_24.csv.gz @%ORDERS AUTO_COMPRESS=FALSE SOURCE_COMPRESSION=GZIP;\n",
    "> ```\n",
    "\n",
    "Check what landed:\n",
    "\n",
    "```sql\n",
    "LIST @%ORDERS;\n",
    "```\n",
    "\n",
    "#### d) **Dry-run** the load (validate, don’t insert)\n",
    "\n",
    "```sql\n",
    "COPY INTO ORDERS\n",
    "  FROM @%ORDERS\n",
    "  FILES = ('orders_2025_08_24.csv.gz')       -- use exact name returned by LIST\n",
    "  FILE_FORMAT = (FORMAT_NAME = ff_orders_csv)\n",
    "  VALIDATION_MODE = 'RETURN_ERRORS';\n",
    "```\n",
    "\n",
    "* If you want to preview rows Snowflake *would* load:\n",
    "\n",
    "```sql\n",
    "COPY INTO ORDERS\n",
    "  FROM @%ORDERS\n",
    "  FILES = ('orders_2025_08_24.csv.gz')\n",
    "  FILE_FORMAT = (FORMAT_NAME = ff_orders_csv)\n",
    "  VALIDATION_MODE = RETURN_1000_ROWS;\n",
    "```\n",
    "\n",
    "Review validation output:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));\n",
    "```\n",
    "\n",
    "**Fix common errors** (examples)\n",
    "\n",
    "* Column count mismatch → check delimiter/enclosure options.\n",
    "* Bad timestamp → set `TIMESTAMP_FORMAT` in file format (`AUTO`, or a pattern).\n",
    "* Corrupt lines → pick a policy: `ON_ERROR='SKIP_FILE_5'` (skip file if >5 errors), or `'CONTINUE'` (load good rows, log bad).\n",
    "\n",
    "#### e) **Load for real**\n",
    "\n",
    "```sql\n",
    "COPY INTO ORDERS\n",
    "  FROM @%ORDERS\n",
    "  FILES = ('orders_2025_08_24.csv.gz')\n",
    "  FILE_FORMAT = (FORMAT_NAME = ff_orders_csv)\n",
    "  ON_ERROR = 'ABORT_STATEMENT'   -- choose the strictness you want\n",
    "  PURGE = TRUE                   -- remove file from stage if load succeeds\n",
    "  FORCE = FALSE                  -- don't reload if Snowflake thinks it's the same file\n",
    "  MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE;  -- helpful if column order differs\n",
    "```\n",
    "\n",
    "Confirm outcome:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));  -- rows_loaded, errors_seen, etc.\n",
    "SELECT COUNT(*) FROM ORDERS WHERE CAST(order_ts AS DATE) = '2025-08-24';\n",
    "```\n",
    "\n",
    "#### f) **If you must reload** the same filename\n",
    "\n",
    "* Set `FORCE=TRUE` in `COPY` (tells Snowflake to ignore the duplicate-file guard), **or**\n",
    "* Rename the staged file to a new unique name before loading (safer for lineage).\n",
    "\n",
    "#### g) **Housekeeping**\n",
    "\n",
    "* If you didn’t use `PURGE=TRUE`, clean up:\n",
    "\n",
    "```sql\n",
    "REMOVE @%ORDERS PATTERN='.*orders_2025_08_24.*';\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Extra depth knobs you’ll appreciate\n",
    "\n",
    "* **Parallelism & warehouse size**: Larger warehouses load faster thanks to more threads. If your files are split well (100–250 MB), scaling up can reduce wall-clock time significantly.\n",
    "* **Patterns vs files**:\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO ORDERS FROM @mystage/incoming/ PATTERN='.*orders_2025_08_.*\\.csv\\.gz';\n",
    "  ```\n",
    "\n",
    "  Patterns are great for daily partitions (prefixes like `incoming/2025/08/24/`).\n",
    "* **Skips and truncation**: Prefer fixing the source or file format to using `TRUNCATECOLUMNS=TRUE` (last resort).\n",
    "* **Auditing**: Save `LAST_QUERY_ID()` and `CURRENT_TIMESTAMP()` to a control table with the row counts you expect. This becomes your SLA dashboard.\n",
    "* **Permissions (internal stages)**:\n",
    "\n",
    "  * **Named stage**: grant `USAGE` (see stage), `READ` (GET, LIST), `WRITE` (PUT, REMOVE).\n",
    "  * **Table stage**: access is tied to table privileges; loaders typically need `INSERT` on the table and the ability to `PUT` to its stage (ownership often used).\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Quick cookbook\n",
    "\n",
    "**Create a named internal stage with defaults**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE stg_finance_in\n",
    "  FILE_FORMAT = (FORMAT_NAME = ff_orders_csv);\n",
    "-- (internal by default since no external URL/credentials specified)\n",
    "```\n",
    "\n",
    "**Use a named stage in COPY without restating format**\n",
    "\n",
    "```sql\n",
    "COPY INTO ORDERS\n",
    "  FROM @stg_finance_in\n",
    "  PATTERN='.*2025_08_24.*'\n",
    "  ON_ERROR='CONTINUE';\n",
    "```\n",
    "\n",
    "**List only “.bad” reject files**\n",
    "\n",
    "```sql\n",
    "LIST @stg_finance_in PATTERN='.*\\.bad$';\n",
    "```\n",
    "\n",
    "**Download a sample back to your laptop**\n",
    "\n",
    "```sql\n",
    "GET @stg_finance_in file://C:\\samples\\ PARALLEL=4 PATTERN='.*sample.*';\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9) What most people miss (but you won’t)\n",
    "\n",
    "* **Duplicate protection**: Snowflake tracks which files were loaded into a table and **skips them** on subsequent runs. Use `FORCE=TRUE` judiciously and keep filenames stable for idempotent pipelines.\n",
    "* **Compression awareness**: Don’t recompress an already compressed file unless you intend to; it wastes time and can slow loading.\n",
    "* **Validation first**: `VALIDATION_MODE` saves you from half-loaded tables and messy rollbacks.\n",
    "* **Storage costs**: Internal stages are convenient but not free—`PURGE` or time-based `REMOVE` is part of a healthy pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Must-know questions to test yourself\n",
    "\n",
    "1. Explain **user**, **table**, and **named** stages. When would you choose each for an internal stage?\n",
    "2. Walk through the **end-to-end steps** to load a large CSV into a single table using a **table stage** and **SnowSQL**. Include commands.\n",
    "3. What does `AUTO_COMPRESS` do in `PUT`? When should you set it to `FALSE`?\n",
    "4. How does Snowflake prevent **duplicate loads**? When would you use `FORCE=TRUE`?\n",
    "5. Compare `ON_ERROR='ABORT_STATEMENT'` vs `'CONTINUE'` vs `'SKIP_FILE_n'`. When is each appropriate?\n",
    "6. Show how to **validate** a load without inserting data, and how to **inspect results** of a `COPY` run.\n",
    "7. Why are **100–250 MB compressed** files recommended? What happens if you load a single 100 GB file?\n",
    "8. What privileges do you need to **PUT/GET/LIST/REMOVE** on a **named internal stage** vs on a **table stage**?\n",
    "9. Which **file format** settings fix comma-in-quotes and header row issues for CSV?\n",
    "10. How do you clean up stage files after a successful load, and why might you choose **`PURGE=TRUE`**?\n",
    "\n",
    "---\n",
    "\n",
    "## 11) TL;DR blueprint for your exact input\n",
    "\n",
    "* **Internal staging area**: prefer **table stage** for a single-table drop; consider a **named stage** if you need richer access control and defaults.\n",
    "* **Commands**:\n",
    "\n",
    "  * Upload: `PUT file://... @%table AUTO_COMPRESS=TRUE PARALLEL=8`\n",
    "  * Download: `GET @%table file://...`\n",
    "  * Use table stage: `@%table` ; named stage: create via `CREATE STAGE` then reference `@name`\n",
    "  * Best practices: right-size files, reusable file formats, validate first, decide error policy, idempotency, purge/cleanup, minimal privileges.\n",
    "* **Scenario** (large file, single table, SnowSQL):\n",
    "\n",
    "  1. Create **file format** (`ff_orders_csv`).\n",
    "  2. `PUT` the file to `@%ORDERS` (no double compression).\n",
    "  3. `LIST` to verify.\n",
    "  4. `COPY INTO ORDERS` with `VALIDATION_MODE` first.\n",
    "  5. Real `COPY` with chosen `ON_ERROR`, `PURGE`, `MATCH_BY_COLUMN_NAME`.\n",
    "  6. Inspect results via `RESULT_SCAN(LAST_QUERY_ID())`.\n",
    "  7. Cleanup (`PURGE` or `REMOVE`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba75c6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1. Explain user, table, and named stages. When would you choose each for an internal stage?**\n",
    "\n",
    "* **User stage (`@~`)**\n",
    "\n",
    "  * Every user automatically gets one.\n",
    "  * Files here are private to the user.\n",
    "  * Best for **ad-hoc loads/tests** where only you need the data.\n",
    "  * Example: Quickly testing a 5MB CSV you just downloaded.\n",
    "\n",
    "* **Table stage (`@%table_name`)**\n",
    "\n",
    "  * Every table automatically gets one.\n",
    "  * Files are **meant for loading into that specific table**.\n",
    "  * Great for **one-off large loads** where the file belongs *only* to that table.\n",
    "  * Example: Finance sends a big CSV just for `ORDERS`; you `PUT` into `@%ORDERS` and `COPY` directly.\n",
    "\n",
    "* **Named stage (`@mystage`)**\n",
    "\n",
    "  * You explicitly `CREATE STAGE`.\n",
    "  * Can hold defaults like file format, copy options.\n",
    "  * Sharable between users/roles.\n",
    "  * Best for **reusable pipelines** or **shared ingestion** with structured subfolders.\n",
    "  * Example: Daily batch loads of sales data into multiple tables using `@sales_stage/incoming/`.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Walk through the end-to-end steps to load a large CSV into a single table using a table stage and SnowSQL.**\n",
    "\n",
    "1. **Create the target table**:\n",
    "\n",
    "   ```sql\n",
    "   CREATE OR REPLACE TABLE ORDERS (...columns...);\n",
    "   ```\n",
    "\n",
    "2. **Create file format**:\n",
    "\n",
    "   ```sql\n",
    "   CREATE OR REPLACE FILE FORMAT ff_orders_csv TYPE=CSV SKIP_HEADER=1 FIELD_OPTIONALLY_ENCLOSED_BY='\"';\n",
    "   ```\n",
    "\n",
    "3. **Upload the file with SnowSQL**:\n",
    "\n",
    "   ```bash\n",
    "   PUT file://C:\\loads\\orders.csv @%ORDERS AUTO_COMPRESS=TRUE PARALLEL=8;\n",
    "   ```\n",
    "\n",
    "4. **Check file landed**:\n",
    "\n",
    "   ```sql\n",
    "   LIST @%ORDERS;\n",
    "   ```\n",
    "\n",
    "5. **Validate load**:\n",
    "\n",
    "   ```sql\n",
    "   COPY INTO ORDERS\n",
    "   FROM @%ORDERS\n",
    "   FILE_FORMAT=(FORMAT_NAME=ff_orders_csv)\n",
    "   VALIDATION_MODE='RETURN_ERRORS';\n",
    "   ```\n",
    "\n",
    "6. **Load for real**:\n",
    "\n",
    "   ```sql\n",
    "   COPY INTO ORDERS\n",
    "   FROM @%ORDERS\n",
    "   FILE_FORMAT=(FORMAT_NAME=ff_orders_csv)\n",
    "   ON_ERROR='ABORT_STATEMENT'\n",
    "   PURGE=TRUE;\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. What does `AUTO_COMPRESS` do in PUT? When should you set it to FALSE?**\n",
    "\n",
    "* `AUTO_COMPRESS=TRUE` → SnowSQL compresses your file (gzip) **before uploading**. Faster transfer, smaller storage.\n",
    "* Set `AUTO_COMPRESS=FALSE` if your file is **already compressed** (`.gz`, `.bz2`, `.zip`)—otherwise you’d double compress.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. How does Snowflake prevent duplicate loads? When would you use `FORCE=TRUE`?**\n",
    "\n",
    "* Snowflake tracks loaded files **by filename** in the table’s load history.\n",
    "* If you run `COPY` again with the same filename, Snowflake **skips it** by default.\n",
    "* Use `FORCE=TRUE` to reload even if Snowflake thinks it’s already loaded.\n",
    "* Example: If your file was fixed but kept the same name, you need `FORCE=TRUE`.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Compare `ON_ERROR='ABORT_STATEMENT'` vs `'CONTINUE'` vs `'SKIP_FILE_n'`. When is each appropriate?**\n",
    "\n",
    "* `ABORT_STATEMENT` → Stop everything on first error.\n",
    "\n",
    "  * Best for **strict financial or critical data**.\n",
    "\n",
    "* `CONTINUE` → Load good rows, log bad ones.\n",
    "\n",
    "  * Best for **event data / logs** where some rows can be sacrificed.\n",
    "\n",
    "* `SKIP_FILE_n` → Skip the file if more than `n` errors occur.\n",
    "\n",
    "  * Best for **multi-file loads**: don’t waste time if a file is very corrupt.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Show how to validate a load without inserting data, and how to inspect results of a COPY run.**\n",
    "\n",
    "* **Validate only**:\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO ORDERS\n",
    "  FROM @%ORDERS\n",
    "  FILE_FORMAT=(FORMAT_NAME=ff_orders_csv)\n",
    "  VALIDATION_MODE='RETURN_ERRORS';\n",
    "  ```\n",
    "\n",
    "* **Inspect results after COPY**:\n",
    "\n",
    "  ```sql\n",
    "  SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));\n",
    "  ```\n",
    "\n",
    "This returns row counts, errors seen, and file status.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Why are 100–250 MB compressed files recommended? What happens if you load a single 100 GB file?**\n",
    "\n",
    "* Snowflake loads files **in parallel**. Each file is a unit of parallelism.\n",
    "* 100–250 MB compressed = “sweet spot” for balancing throughput and parallelism.\n",
    "* If you load a **single 100 GB file**, only **one thread** works on it → load is painfully slow.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. What privileges do you need to PUT/GET/LIST/REMOVE on a named internal stage vs a table stage?**\n",
    "\n",
    "* **Named stage**:\n",
    "\n",
    "  * `USAGE` → see the stage\n",
    "  * `READ` → `LIST` / `GET`\n",
    "  * `WRITE` → `PUT` / `REMOVE`\n",
    "\n",
    "* **Table stage**:\n",
    "\n",
    "  * Controlled by **table privileges**. If you can `INSERT` into the table, you usually can stage and load files to it.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Which file format settings fix comma-in-quotes and header row issues for CSV?**\n",
    "\n",
    "* For commas inside quotes:\n",
    "\n",
    "  ```sql\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY='\"'\n",
    "  ```\n",
    "* For header row:\n",
    "\n",
    "  ```sql\n",
    "  SKIP_HEADER=1\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **10. How do you clean up stage files after a successful load, and why might you choose PURGE=TRUE?**\n",
    "\n",
    "* **Option 1: Let Snowflake auto-remove** with:\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO ORDERS ... PURGE=TRUE;\n",
    "  ```\n",
    "* **Option 2: Manually**:\n",
    "\n",
    "  ```sql\n",
    "  REMOVE @%ORDERS PATTERN='.*2025_08_24.*';\n",
    "  ```\n",
    "\n",
    "Why PURGE?\n",
    "\n",
    "* Saves **storage cost**.\n",
    "* Prevents accidental **reloading** of old files.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ab8a0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1) What if I recompress a file that’s already compressed?**\n",
    "\n",
    "👉 Imagine you already have a file: `orders.csv.gz` (gzip-compressed).\n",
    "\n",
    "Now in your `PUT` command you mistakenly write:\n",
    "\n",
    "```sql\n",
    "PUT file://orders.csv.gz @%ORDERS AUTO_COMPRESS=TRUE;\n",
    "```\n",
    "\n",
    "### What happens?\n",
    "\n",
    "* SnowSQL takes your **already compressed gzip** and compresses it **again** into another gzip.\n",
    "* The result is a **double-compressed file** (e.g., `.csv.gz.gz`).\n",
    "* Inside Snowflake, when you try to load it with a CSV file format, Snowflake **won’t know how to decode it correctly** (it only decompresses once).\n",
    "* You’ll likely get **garbled data** or **load failures** like:\n",
    "\n",
    "  * “File not recognized as valid gzip”\n",
    "  * Wrong row/column parsing (Snowflake sees binary junk instead of CSV).\n",
    "\n",
    "### Analogy\n",
    "\n",
    "It’s like putting a suitcase inside another suitcase and giving it to someone who only knows how to unzip **one layer**. They’ll open the first zipper and still see a packed bag inside that they don’t know how to open.\n",
    "\n",
    "✅ **Best practice**:\n",
    "\n",
    "* If file is **already compressed** (`.gz`, `.bz2`, `.zip`), use:\n",
    "\n",
    "  ```sql\n",
    "  PUT file://orders.csv.gz @%ORDERS AUTO_COMPRESS=FALSE;\n",
    "  ```\n",
    "* And if needed, tell Snowflake how it’s compressed:\n",
    "\n",
    "  ```sql\n",
    "  FILE_FORMAT=(TYPE=CSV COMPRESSION=GZIP)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **2) What if I reinsert (upload) a file and `OVERWRITE=FALSE`?**\n",
    "\n",
    "👉 Let’s say yesterday you uploaded `orders_2025_08_24.csv.gz` into your table stage:\n",
    "\n",
    "```sql\n",
    "PUT file://orders_2025_08_24.csv.gz @%ORDERS AUTO_COMPRESS=TRUE OVERWRITE=FALSE;\n",
    "```\n",
    "\n",
    "Now today you try again with the same command (same filename).\n",
    "\n",
    "### What happens?\n",
    "\n",
    "* Snowflake checks if the **exact same filename already exists** in the stage.\n",
    "* Since `OVERWRITE=FALSE`, Snowflake will **skip the upload**.\n",
    "* It will **not replace** the old file, even if the contents are different.\n",
    "* You’ll see an output message like:\n",
    "\n",
    "  ```\n",
    "  Skipping file because it already exists and OVERWRITE=FALSE\n",
    "  ```\n",
    "\n",
    "### Why does this matter?\n",
    "\n",
    "* If finance sent you an updated version of the file (with the same name), you’ll accidentally still be working with the **old version**.\n",
    "* To truly replace it, you must either:\n",
    "\n",
    "  * Use `OVERWRITE=TRUE` in `PUT`, **or**\n",
    "  * Rename the new file before uploading (e.g., `orders_2025_08_24_v2.csv.gz`).\n",
    "\n",
    "### And then in COPY INTO?\n",
    "\n",
    "* Snowflake’s load history will **remember that filename** was already loaded, so a plain `COPY` will skip it (to avoid duplicates).\n",
    "* If you really want to reload that same filename into the table, you’ll also need:\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO ORDERS FROM @%ORDERS FORCE=TRUE;\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**:\n",
    "\n",
    "* **Recompress** → leads to double compression → unreadable files or errors.\n",
    "* **Reinsert with `OVERWRITE=FALSE`** → Snowflake will skip uploading; you’ll keep the old file in stage.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eba0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
