{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d290a8de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Snowflake Performance Optimization — A Comprehensive Guide\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is Performance Tuning in General?\n",
    "\n",
    "### Story to set the context:\n",
    "\n",
    "Imagine you run a huge online store — \"MegaShop.\" Millions of customers visit daily, and they want to see product details, place orders, and get reports instantly. But your data system is slow; queries take minutes. Your job is to **tune the system** to make everything faster and smoother.\n",
    "\n",
    "**Performance tuning** is the art and science of optimizing how queries and data operations are executed to reduce latency and improve throughput. It means identifying bottlenecks and applying the right techniques to get results faster, cheaper, and efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### Traditional Database Performance Tuning (for context):\n",
    "\n",
    "In classic databases like Oracle, MySQL, SQL Server, you often use:\n",
    "\n",
    "* **Indexes** to speed up data lookups.\n",
    "* **Primary keys** to enforce uniqueness and sometimes help query planners.\n",
    "* **Partitions** to divide large tables so queries touch only relevant parts.\n",
    "* **Analyze execution plans** to understand what the database engine does internally.\n",
    "* **Remove unnecessary full table scans** especially on huge tables.\n",
    "* **Cache small tables** to avoid repeated disk I/O.\n",
    "* **Use query hints** to force query planners to choose specific indexes or join methods.\n",
    "* **Ordering joins** to reduce data shuffling and improve join performance.\n",
    "\n",
    "---\n",
    "\n",
    "### But — this is **NOT** exactly how Snowflake works!\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How Snowflake is Different? What Does Snowflake Performance Tuning Really Mean?\n",
    "\n",
    "### Story continuation:\n",
    "\n",
    "In MegaShop, you decide to migrate to Snowflake, a cloud-native data platform. After migration, you realize many old tuning tricks don’t apply — but Snowflake has its own unique ways of optimizing performance.\n",
    "\n",
    "### Key differences:\n",
    "\n",
    "* **No traditional indexes or primary keys enforcement** (Primary keys and foreign keys are metadata only, not enforced).\n",
    "* **Automatic micro-partitioning** by Snowflake behind the scenes (you don’t create partitions explicitly).\n",
    "* **Massively parallel processing (MPP) architecture with compute clusters (virtual warehouses).**\n",
    "* **Automatic query optimization engine and caching at multiple layers.**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Snowflake’s Fundamental Performance Optimization Concepts\n",
    "\n",
    "Let's break down what really matters:\n",
    "\n",
    "### 3.1 Micro-partitions\n",
    "\n",
    "* Snowflake automatically splits tables into **micro-partitions**, contiguous units of storage that contain metadata like min/max column values.\n",
    "* This metadata lets Snowflake **prune irrelevant micro-partitions** during query time, avoiding scanning unnecessary data.\n",
    "* **You cannot create partitions manually**, but you can influence clustering.\n",
    "\n",
    "### 3.2 Clustering Keys (Manual Optimization)\n",
    "\n",
    "* If your table grows very large, Snowflake might not prune micro-partitions effectively.\n",
    "* You can define a **clustering key** to reorganize data so micro-partitions are sorted by that key.\n",
    "* Example: If MegaShop often filters orders by `order_date`, defining clustering on `order_date` helps queries skip irrelevant micro-partitions.\n",
    "\n",
    "### 3.3 Virtual Warehouses (Compute Resources)\n",
    "\n",
    "* You can **scale compute power up or out** by resizing or adding warehouses.\n",
    "* Larger warehouses = more nodes = faster query but higher cost.\n",
    "* Auto-suspend and auto-resume features avoid waste.\n",
    "\n",
    "### 3.4 Query Caching\n",
    "\n",
    "* Snowflake caches data at several levels:\n",
    "\n",
    "  * **Result Cache:** If the exact query ran before and underlying data didn't change, Snowflake returns cached results instantly.\n",
    "  * **Local Disk Cache:** Each compute node caches data it reads to speed up repeated scans.\n",
    "  * **Metadata Cache:** Cached micro-partition metadata speeds pruning.\n",
    "\n",
    "### 3.5 Data Pruning (Skipping irrelevant data)\n",
    "\n",
    "* Snowflake uses min/max statistics in micro-partitions to skip irrelevant partitions, greatly speeding up queries.\n",
    "\n",
    "### 3.6 Query Profiling & Query Plan Analysis\n",
    "\n",
    "* Snowflake provides **Query Profile** in the UI to visualize query steps, identify slow operations like big scans, spills, or joins.\n",
    "* This helps identify bottlenecks and optimize.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Now, let's map your points — What applies, what not, and what to do instead in Snowflake.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 \"We add indexes, primary keys\"\n",
    "\n",
    "* **Traditional databases:** Indexes speed up lookup.\n",
    "* **Snowflake:** There are **no user-managed indexes**. Primary keys are only for metadata, not enforced, and do NOT speed up queries.\n",
    "* **Optimization Instead:** Use **clustering keys** to optimize micro-partition pruning for large tables.\n",
    "\n",
    "**Example:**\n",
    "MegaShop has a huge `orders` table with billions of rows. Queries filter on `customer_id`. Defining clustering on `customer_id` improves pruning and query speed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 \"We create table partitions\"\n",
    "\n",
    "* **Traditional:** You manually partition big tables (like by date).\n",
    "* **Snowflake:** Snowflake automatically **micro-partitions** data; no manual partitioning allowed.\n",
    "* **Optimization Instead:** Use clustering keys if automatic pruning isn’t sufficient.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 \"We analyze query execution plan\"\n",
    "\n",
    "* This is **very important**.\n",
    "* Snowflake provides detailed **Query Profile** which visualizes:\n",
    "\n",
    "  * Time spent in scanning, filtering, joins, data shuffles, etc.\n",
    "  * Bottlenecks like large data scans or compute spillover.\n",
    "* Use Query Profile to identify if your queries are doing large scans, improper joins, or spilling to disk.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 \"Remove unnecessary large-table full-table scans\"\n",
    "\n",
    "* In Snowflake, **full table scans happen if filters do not help pruning**.\n",
    "* Ensure your filters use columns with good micro-partition pruning.\n",
    "* Use **clustering keys** or rewrite queries to leverage pruning.\n",
    "* Avoid `SELECT *` if you only need few columns.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5 \"Cache small-table full-table scans\"\n",
    "\n",
    "* Snowflake caches data internally, but to improve join performance on small tables, use:\n",
    "\n",
    "  * **Broadcast Join Hint** (`/*+ BROADCAST(table) */`) in SQL to force small tables to be broadcasted to all nodes, avoiding shuffle.\n",
    "* Snowflake automatically chooses broadcast vs. shuffle join based on size but can be overridden.\n",
    "\n",
    "---\n",
    "\n",
    "let’s unpack **“Cache small-table full-table scans”** and **Broadcast Join Hint** in Snowflake with a story so it’s crystal clear.\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ First, the Big Picture\n",
    "\n",
    "Snowflake already **caches data** in multiple layers (local SSD cache in the Virtual Warehouse, Remote Disk cache, and Result Cache).\n",
    "When you join **a huge table** with **a tiny table**, the **query optimizer** decides the join strategy:\n",
    "\n",
    "* **Broadcast Join** → Copy the small table to every compute node so each node can join locally. (No shuffling of large table.)\n",
    "* **Shuffle Join** → Split both tables into chunks based on the join key, shuffle those chunks across nodes so matching rows end up together. (Costly for large datasets.)\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Why “Cache small-table full-table scans” helps\n",
    "\n",
    "If you have a very small table (say a lookup table of country codes, product categories, etc.), you can **force Snowflake to broadcast it** instead of shuffling.\n",
    "\n",
    "This:\n",
    "\n",
    "* **Avoids large network movement** of the big table’s rows.\n",
    "* Makes join faster.\n",
    "* Reduces compute cost.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Story Example — “The Airport Immigration Desk”\n",
    "\n",
    "Imagine:\n",
    "\n",
    "* **Big Table** → `PASSENGERS` (50 million rows — all passengers arriving at an airport).\n",
    "* **Small Table** → `COUNTRY_CODES` (200 rows — country code to country name mapping).\n",
    "\n",
    "You want to find all passengers from countries starting with **'B'**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Without Broadcast Join (Shuffle Join)\n",
    "\n",
    "Snowflake might decide:\n",
    "\n",
    "1. Split **PASSENGERS** into multiple chunks (based on join key `country_code`).\n",
    "2. Split **COUNTRY\\_CODES** into chunks.\n",
    "3. Shuffle both so that all matching `country_code` values end up on the same node.\n",
    "4. Join the chunks on each node.\n",
    "\n",
    "Problem:\n",
    "\n",
    "* Even though `COUNTRY_CODES` is tiny, it still gets shuffled unnecessarily.\n",
    "* And `PASSENGERS` — a massive dataset — might also get shuffled, which is expensive.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ With Broadcast Join\n",
    "\n",
    "If you tell Snowflake:\n",
    "\n",
    "```sql\n",
    "SELECT /*+ BROADCAST(cc) */\n",
    "    p.passenger_id,\n",
    "    p.name,\n",
    "    cc.country_name\n",
    "FROM PASSENGERS p\n",
    "JOIN COUNTRY_CODES cc\n",
    "  ON p.country_code = cc.country_code\n",
    "WHERE cc.country_name LIKE 'B%';\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "* Snowflake sends **COUNTRY\\_CODES** (tiny table) to **all nodes** instantly (fast).\n",
    "* Each node already has its own slice of `PASSENGERS` and can join locally with `COUNTRY_CODES` — **no need to move `PASSENGERS` at all**.\n",
    "* This is much faster and cheaper.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Visualization\n",
    "\n",
    "```\n",
    "Without Broadcast Join:\n",
    "PASSENGERS chunks  → shuffle across network → join\n",
    "COUNTRY_CODES chunks → shuffle across network → join\n",
    "\n",
    "With Broadcast Join:\n",
    "COUNTRY_CODES → copied to each node (tiny, fast)\n",
    "PASSENGERS chunks → joined locally with COUNTRY_CODES (no shuffle)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ When to Use This\n",
    "\n",
    "You **don’t need** to use `BROADCAST` hint most of the time — Snowflake’s optimizer detects small tables and broadcasts them automatically.\n",
    "\n",
    "You **should** use it if:\n",
    "\n",
    "* You know a table is small (but Snowflake’s statistics might not).\n",
    "* Snowflake unexpectedly chooses a shuffle join for a small table.\n",
    "* The query plan shows expensive repartitioning steps.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### 4.6 \"Verify optimal index usage\"\n",
    "\n",
    "* No indexes in Snowflake, so this does not apply.\n",
    "* Instead, verify **clustering** and **data pruning** effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.7 \"Using hints to tune Oracle SQL\"\n",
    "\n",
    "* Snowflake supports **some query hints** like broadcast join, join strategy, etc.\n",
    "* Use hints **sparingly**; Snowflake’s optimizer is strong.\n",
    "* Always test with and without hints.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.8 \"Self-order the table joins\"\n",
    "\n",
    "* In traditional DBs, join order can matter a lot.\n",
    "* Snowflake’s optimizer chooses join order automatically.\n",
    "* However, writing well-structured queries and **using clustering and broadcast join hints** can improve join efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Additional Important Topics for Snowflake Performance\n",
    "\n",
    "### 5.1 Warehouse Sizing & Concurrency\n",
    "\n",
    "* Bigger warehouse = faster query but higher cost.\n",
    "* If many users run queries, scaling out (multi-cluster warehouses) helps concurrency without queuing.\n",
    "* Scenario: MegaShop has 50 analysts running reports at once; a multi-cluster warehouse auto-scales to serve everyone smoothly.\n",
    "\n",
    "### 5.2 Data Types and Compression\n",
    "\n",
    "* Snowflake uses automatic compression.\n",
    "* Choosing optimal data types (e.g., using INTEGER vs STRING where possible) reduces storage and speeds queries.\n",
    "\n",
    "### 5.3 Avoid Data Skew\n",
    "\n",
    "* When joining tables, skew (very large partitions for some keys) can cause some compute nodes to be overwhelmed.\n",
    "* To fix, pre-aggregate data, or repartition via clustering.\n",
    "\n",
    "let’s break down **Avoid Data Skew** in Snowflake with an easy story and then go deeper into what it means for performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ What is Data Skew?\n",
    "\n",
    "In a distributed system like Snowflake, data is split into **partitions** and spread across compute nodes.\n",
    "When joining two tables, Snowflake **shuffles** data by the join key so matching rows end up on the same node.\n",
    "\n",
    "If one **join key value** has way more rows than others, that key’s partition becomes **huge** — and the node that gets it will have to process way more work.\n",
    "This is **data skew** → imbalance in workload across nodes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Story Example — “Restaurant Orders”\n",
    "\n",
    "Imagine:\n",
    "\n",
    "* **BIG Table** → `ORDERS` (1 billion rows — orders from multiple restaurants)\n",
    "* **SMALL Table** → `RESTAURANTS` (10,000 rows — restaurant info)\n",
    "* Join key → `restaurant_id`\n",
    "\n",
    "If:\n",
    "\n",
    "* Most restaurants have **10,000 orders** (normal load)\n",
    "* But **McDonald’s** has **200 million orders** (way bigger)\n",
    "* When Snowflake shuffles data by `restaurant_id` to join, the partition for McDonald’s will be **huge**, and one compute node will get stuck with this monster partition while others finish early and wait.\n",
    "\n",
    "This slows down the whole query because **the query finishes only when the slowest node finishes**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ How it looks in Snowflake query execution\n",
    "\n",
    "In the **Query Profile**:\n",
    "\n",
    "* Some partitions (nodes) show processing **gigabytes of data**\n",
    "* Others show processing only **megabytes of data**\n",
    "* The big ones are the skewed keys\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ How to Fix It\n",
    "\n",
    "### **Option 1 — Pre-Aggregate**\n",
    "\n",
    "If possible, aggregate skewed data before the join so the monster partition shrinks.\n",
    "\n",
    "```sql\n",
    "-- Instead of joining raw orders\n",
    "SELECT restaurant_id, SUM(order_amount) as total_sales\n",
    "FROM ORDERS\n",
    "GROUP BY restaurant_id;\n",
    "-- Now join with RESTAURANTS\n",
    "```\n",
    "\n",
    "This reduces the amount of data per key before joining.\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2 — Repartition / Add Distribution Key**\n",
    "\n",
    "If you can, **spread the skewed data more evenly** by:\n",
    "\n",
    "* Adding another column to the join key (e.g., `restaurant_id` + `order_date`)\n",
    "* Randomly assigning a distribution bucket to break large groups apart\n",
    "\n",
    "Example:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT o.*, HASH(order_id) % 10 as bucket\n",
    "    FROM ORDERS o\n",
    ") o2\n",
    "JOIN (\n",
    "    SELECT r.*, seq_bucket as bucket\n",
    "    FROM RESTAURANTS r\n",
    "    CROSS JOIN (SELECT seq4() % 10 as seq_bucket FROM TABLE(GENERATOR(ROWCOUNT => 10)))\n",
    ") r2\n",
    "ON o2.restaurant_id = r2.restaurant_id AND o2.bucket = r2.bucket;\n",
    "```\n",
    "\n",
    "This trick splits large groups into multiple smaller ones.\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 3 — Clustering**\n",
    "\n",
    "If skew is due to **storage layout**, clustering can help Snowflake prune data better before join.\n",
    "\n",
    "```sql\n",
    "ALTER TABLE ORDERS CLUSTER BY (restaurant_id);\n",
    "```\n",
    "\n",
    "This doesn’t remove skew but can reduce how much data is read.\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Summary Table\n",
    "\n",
    "| Problem        | Cause                              | Fix                                  |\n",
    "| -------------- | ---------------------------------- | ------------------------------------ |\n",
    "| Data skew      | One/few keys have most of the data | Pre-aggregate, split key, bucketize  |\n",
    "| Node imbalance | One node gets too much work        | Add extra key to distribute evenly   |\n",
    "| Slow joins     | Shuffling huge partitions          | Broadcast small tables, bucket joins |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 5.4 Materialized Views\n",
    "\n",
    "* Use materialized views for expensive repeated queries.\n",
    "* Snowflake maintains these automatically and speeds up query time.\n",
    "\n",
    "\n",
    "let’s make **Materialized Views** (MVs) in Snowflake simple and memorable.\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ What is a Materialized View?\n",
    "\n",
    "A **Materialized View** is like taking a *snapshot* of your query’s results and storing it physically (like a pre-computed table).\n",
    "\n",
    "* You write a query once\n",
    "* Snowflake runs it and **stores the result**\n",
    "* Snowflake automatically keeps it **up-to-date** when the underlying tables change\n",
    "* When you query the MV, it’s **way faster** because it’s already pre-processed\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Story Example — “The Coffee Shop Sales Report”\n",
    "\n",
    "Imagine:\n",
    "\n",
    "* You have a giant **`ORDERS`** table with **1 billion rows**\n",
    "* Every day, you run:\n",
    "\n",
    "```sql\n",
    "SELECT store_id, SUM(sales_amount) AS total_sales\n",
    "FROM ORDERS\n",
    "WHERE order_date >= CURRENT_DATE - INTERVAL '7 DAYS'\n",
    "GROUP BY store_id;\n",
    "```\n",
    "\n",
    "This is expensive — scanning 1B rows every time is slow and costs credits.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ How Materialized View Helps\n",
    "\n",
    "Instead of recalculating **from scratch** each time:\n",
    "\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW mv_weekly_sales AS\n",
    "SELECT store_id, SUM(sales_amount) AS total_sales\n",
    "FROM ORDERS\n",
    "WHERE order_date >= CURRENT_DATE - INTERVAL '7 DAYS'\n",
    "GROUP BY store_id;\n",
    "```\n",
    "\n",
    "* Snowflake stores **`mv_weekly_sales`** like a table\n",
    "* When new orders come in, Snowflake **incrementally updates** the MV in the background\n",
    "* Querying:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM mv_weekly_sales;\n",
    "```\n",
    "\n",
    "is **instant** because the heavy lifting is already done.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Why it’s Faster\n",
    "\n",
    "Without MV:\n",
    "\n",
    "* Query engine scans huge table → filters → aggregates → returns\n",
    "\n",
    "With MV:\n",
    "\n",
    "* Just reads small, pre-aggregated result\n",
    "* Less I/O + less CPU = faster & cheaper queries\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Things to Keep in Mind\n",
    "\n",
    "* **Best for repeated, expensive queries** — the more you reuse it, the more you save.\n",
    "* Snowflake updates MVs **automatically**, but this has **storage cost** and **compute cost** for refreshing.\n",
    "* Works best when **base table changes slowly** compared to how often you query.\n",
    "* **Cannot** use non-deterministic functions (`CURRENT_TIMESTAMP`, `RANDOM()`).\n",
    "* Good with **aggregations**, **joins**, and **filters** that are stable.\n",
    "\n",
    "---\n",
    "\n",
    "### 6️⃣ Real Example in Analytics\n",
    "\n",
    "**Scenario**: Marketing team asks for daily “active users” count by country.\n",
    "Instead of calculating daily from billions of log records:\n",
    "\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW mv_active_users AS\n",
    "SELECT country, COUNT(DISTINCT user_id) AS daily_active_users\n",
    "FROM USER_EVENTS\n",
    "WHERE event_date >= CURRENT_DATE - INTERVAL '1 DAY'\n",
    "GROUP BY country;\n",
    "```\n",
    "\n",
    "Now, dashboard queries:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM mv_active_users;\n",
    "```\n",
    "\n",
    "load in milliseconds.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Bottom line:**\n",
    "Materialized Views in Snowflake = *“Set it once, Snowflake keeps it fresh, you get results instantly.”*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 5.5 Using Streams & Tasks\n",
    "\n",
    "* For incremental data loads and transformations, using Snowflake Streams and Tasks can optimize performance by processing only changed data.\n",
    "\n",
    "let’s break down **Streams & Tasks in Snowflake** in a way that clicks instantly.\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ The Problem They Solve\n",
    "\n",
    "Normally, when new data arrives in a table, you have two options:\n",
    "\n",
    "* **Reprocess everything** (wasteful)\n",
    "* Or somehow process **only the new/changed rows**\n",
    "\n",
    "That’s where **Streams** + **Tasks** come in.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ What is a Snowflake Stream?\n",
    "\n",
    "A **Stream** is like a **“change log”** or **“to-do list”** for a table.\n",
    "\n",
    "* It tracks **only the rows that have been inserted, updated, or deleted** since you last checked it.\n",
    "* It doesn’t store full copies of data — just metadata about the changes.\n",
    "\n",
    "Think: *“Hey, since your last run, these are the rows that changed — process just these!”*\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Incremental ETL with Streams\n",
    "\n",
    "Let’s say you have a raw landing table:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE raw_orders (\n",
    "    order_id INT,\n",
    "    product_id INT,\n",
    "    quantity INT,\n",
    "    order_date DATE\n",
    ");\n",
    "```\n",
    "\n",
    "You create a Stream on it:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STREAM raw_orders_stream ON TABLE raw_orders;\n",
    "```\n",
    "\n",
    "Now:\n",
    "\n",
    "* If **10,000 new rows** land today, the stream will list only those 10,000 rows (not the 50 million in the table).\n",
    "* Once you read them in your transformation, Snowflake marks them as processed.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ What is a Snowflake Task?\n",
    "\n",
    "A **Task** is like a **scheduled job** or **cron job** inside Snowflake.\n",
    "\n",
    "* You define a SQL statement to run at a set frequency or trigger.\n",
    "* It can run every minute, every hour, daily, or even in a dependency chain.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Automating the Transformation\n",
    "\n",
    "You want to process new orders every hour:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TASK process_new_orders\n",
    "WAREHOUSE = my_etl_wh\n",
    "SCHEDULE = 'USING CRON 0 * * * * UTC'  -- every hour\n",
    "AS\n",
    "INSERT INTO processed_orders\n",
    "SELECT *\n",
    "FROM raw_orders_stream;\n",
    "```\n",
    "\n",
    "Now, Snowflake automatically:\n",
    "\n",
    "1. Checks the stream for new rows every hour.\n",
    "2. Inserts just those rows into `processed_orders`.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Why Streams + Tasks Boost Performance\n",
    "\n",
    "* **No full-table scans** → Only changed rows are processed.\n",
    "* **Automation** → No need for external schedulers or manual runs.\n",
    "* **Faster ETL pipelines** → Less data = less compute cost.\n",
    "* **Reliable** → Snowflake handles tracking and scheduling.\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Real-World Scenario\n",
    "\n",
    "**Data warehouse with daily sales feeds**:\n",
    "\n",
    "* Sales table gets millions of rows daily.\n",
    "* Instead of aggregating all sales each night:\n",
    "\n",
    "  * **Stream** captures new rows.\n",
    "  * **Task** runs hourly to update the aggregated “sales by store” table with just the new data.\n",
    "* Your dashboards stay up to date **without reprocessing history**.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Bottom line**:\n",
    "**Streams** = change tracking\n",
    "**Tasks** = scheduling\n",
    "Together → *“Process just the new stuff, automatically, inside Snowflake.”*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 6. Real Case Scenario — Putting It All Together\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario: MegaShop Reporting\n",
    "\n",
    "MegaShop has a massive sales dataset with 5 billion rows in `sales_data`. Analysts often query total sales by `region` and `order_date`.\n",
    "\n",
    "**Problem:** Queries are slow (minutes), and warehouses are huge but still not enough.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Analyze Query Profile\n",
    "\n",
    "* Find full scans of `sales_data`.\n",
    "* Filters on `region` and `order_date` are not pruning many micro-partitions.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Add Clustering Key\n",
    "\n",
    "* Define clustering on `(region, order_date)`.\n",
    "* Snowflake reclusters data in background, organizing micro-partitions to group same region and date.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Rerun Queries\n",
    "\n",
    "* Query Profile shows micro-partition pruning increased significantly.\n",
    "* Scan data reduced from 5B rows to 50M rows.\n",
    "* Query runtime dropped from 10 minutes to 1 minute.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Use Broadcast Join Hint\n",
    "\n",
    "* Join with small `region_metadata` table.\n",
    "* Add `/*+ BROADCAST(region_metadata) */` hint.\n",
    "* Join shuffles data less, speeding query further.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Warehouse Resize and Multi-cluster\n",
    "\n",
    "* During peak hours, analysts run many reports.\n",
    "* Enable multi-cluster warehouse with auto-scale.\n",
    "* Queries don’t queue anymore.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Must-Know Questions (for deep understanding)\n",
    "\n",
    "* Why doesn’t Snowflake use traditional indexes, and how does it optimize query performance instead?\n",
    "* What are micro-partitions and how do they affect query speed?\n",
    "* How does clustering key improve pruning and what are its costs?\n",
    "* When should you consider resizing a virtual warehouse versus rewriting queries?\n",
    "* How does Snowflake handle join operations internally, and when would you use broadcast join hints?\n",
    "* What is the role of query profiling in optimizing Snowflake queries?\n",
    "* How do result caching and data caching work in Snowflake?\n",
    "* What strategies help avoid data skew in joins?\n",
    "* How do materialized views help in Snowflake performance?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary — Key Takeaways\n",
    "\n",
    "| Traditional DB Tuning | Snowflake Approach                                |\n",
    "| --------------------- | ------------------------------------------------- |\n",
    "| Indexes               | No indexes; relies on micro-partition pruning     |\n",
    "| Primary Keys          | Metadata only, no enforcement                     |\n",
    "| Manual Partitioning   | Automatic micro-partitions, manual clustering     |\n",
    "| Query Hints           | Limited hints, optimizer mostly automatic         |\n",
    "| Join Order            | Optimizer chooses, hints available                |\n",
    "| Caching               | Result, local disk, metadata cache auto-managed   |\n",
    "| Warehouse Size        | Adjustable compute size and multi-cluster scaling |\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thought\n",
    "\n",
    "Snowflake performance tuning is about **understanding how Snowflake stores data internally (micro-partitions)**, leveraging **clustering for large datasets**, using **compute resources wisely**, and **reading query profiles to pinpoint inefficiencies**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b902051",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Must-Know Questions and Answers on Snowflake Performance Optimization\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Why doesn’t Snowflake use traditional indexes, and how does it optimize query performance instead?\n",
    "\n",
    "**Answer:**\n",
    "Unlike traditional databases that rely heavily on user-created indexes (like B-trees) to speed up data lookups, Snowflake **does not use indexes** in the classic sense.\n",
    "\n",
    "**Why?** Because Snowflake uses a **columnar, cloud-optimized architecture with automatic micro-partitioning.**\n",
    "\n",
    "* **Micro-partitions:** When data is loaded, Snowflake automatically divides it into small contiguous units (micro-partitions, \\~16MB each compressed).\n",
    "* Each micro-partition stores metadata such as **min/max values for each column** and other statistics.\n",
    "\n",
    "**How does this help?**\n",
    "When you run a query with filters, Snowflake uses this metadata to **prune irrelevant micro-partitions** (skip scanning those that cannot contain matching data). This drastically reduces the amount of data scanned, leading to faster queries.\n",
    "\n",
    "**Example:**\n",
    "If your query filters on `order_date = '2025-01-01'`, Snowflake only scans micro-partitions whose `order_date` range includes this date, skipping all others automatically — no need for indexes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What are micro-partitions and how do they affect query speed?\n",
    "\n",
    "**Answer:**\n",
    "Micro-partitions are the foundational storage unit in Snowflake:\n",
    "\n",
    "* Automatic data files (\\~16MB compressed) that Snowflake creates internally.\n",
    "* Stored column-wise.\n",
    "* Contain metadata for each column like min/max values, number of distinct values, null counts, etc.\n",
    "\n",
    "**Impact on speed:**\n",
    "During query execution, Snowflake leverages micro-partition metadata for **pruning**, i.e., skipping scanning micro-partitions that cannot satisfy the query filter predicates.\n",
    "\n",
    "This avoids full table scans and reduces IO and CPU usage.\n",
    "\n",
    "**Example:**\n",
    "A 1TB table might be broken into 60,000 micro-partitions. For a query filtering on a date range of 1 day, only a small fraction of these partitions need scanning.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. How does clustering key improve pruning and what are its costs?\n",
    "\n",
    "**Answer:**\n",
    "If data is inserted in a random order, micro-partitions might not be organized well, reducing pruning effectiveness.\n",
    "\n",
    "**Clustering Key:**\n",
    "A user-defined column or set of columns to **organize data physically** in micro-partitions (similar to sorting). Snowflake reclusters data in the background accordingly.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Better micro-partition pruning on clustered columns.\n",
    "* Faster queries filtering on clustering keys.\n",
    "\n",
    "**Costs:**\n",
    "\n",
    "* Clustering consumes compute resources (cost).\n",
    "* Reclustering background tasks run periodically and might take time for very large tables.\n",
    "* Over-clustering or clustering on high cardinality columns can be inefficient.\n",
    "\n",
    "**Example:**\n",
    "If MegaShop clusters the `orders` table by `(region, order_date)`, queries filtering on these columns scan fewer micro-partitions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. When should you consider resizing a virtual warehouse versus rewriting queries?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Resize warehouse (scale up):**\n",
    "  When queries are CPU or memory-bound, and rewriting queries doesn’t reduce the data scanned, increasing warehouse size (more nodes) gives more parallelism and faster execution.\n",
    "\n",
    "* **Rewrite queries:**\n",
    "  When queries scan too much data unnecessarily or do inefficient joins. Optimize filters, projections (select only needed columns), join strategies.\n",
    "\n",
    "**Rule of thumb:**\n",
    "\n",
    "* First, try to optimize query logic and data design.\n",
    "* If still slow due to volume, scale warehouse size.\n",
    "* For many concurrent users, consider multi-cluster warehouses instead of a single large one.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. How does Snowflake handle join operations internally, and when would you use broadcast join hints?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Snowflake supports several join strategies:\n",
    "\n",
    "* **Broadcast join:** Small table is copied to all compute nodes to avoid shuffling large tables.\n",
    "* **Shuffle join:** Large tables are redistributed by join key among nodes to perform join.\n",
    "\n",
    "**Snowflake optimizer automatically picks the best join strategy** based on table size.\n",
    "\n",
    "**Use broadcast join hints when:**\n",
    "\n",
    "* You know a table is small and want to force broadcast join to speed up.\n",
    "* The optimizer picks shuffle join causing performance issues.\n",
    "\n",
    "**Example:**\n",
    "`SELECT /*+ BROADCAST(small_table) */ * FROM large_table JOIN small_table ON ...`\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What is the role of query profiling in optimizing Snowflake queries?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Query Profile** is an interactive visual tool in Snowflake UI showing:\n",
    "\n",
    "* Detailed step-by-step execution plan.\n",
    "* Time spent on scanning, filtering, joins, shuffles, and data spill.\n",
    "* Data scanned vs. returned rows.\n",
    "* Helps identify bottlenecks like full scans, large data shuffles, or skewed processing.\n",
    "\n",
    "Using it, you can:\n",
    "\n",
    "* Detect inefficient operations.\n",
    "* Pinpoint if clustering, filtering, or warehouse size needs adjustment.\n",
    "* Understand which part of query takes most time.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. How do result caching and data caching work in Snowflake?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Result Cache:**\n",
    "  If the exact same query was run recently and underlying data unchanged, Snowflake returns the cached results immediately — zero compute cost.\n",
    "\n",
    "* **Local Disk Cache:**\n",
    "  Each compute node caches data files it reads, so repeated scans of same data by that node are faster.\n",
    "\n",
    "* **Metadata Cache:**\n",
    "  Micro-partition metadata cached for pruning without reloading.\n",
    "\n",
    "**This layered caching significantly speeds repeated or similar queries.**\n",
    "\n",
    "---\n",
    "\n",
    "### 8. What strategies help avoid data skew in joins?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Data skew** happens when one or few join key values have disproportionately large data, causing uneven work distribution among compute nodes.\n",
    "\n",
    "**Avoid skew by:**\n",
    "\n",
    "* Pre-aggregating or filtering large keys.\n",
    "* Choosing join keys with uniform data distribution.\n",
    "* Using clustering keys to physically order data.\n",
    "* Avoiding joins on high-skew keys or implementing logic to split skewed keys.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. How do materialized views help in Snowflake performance?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Materialized Views:\n",
    "\n",
    "* Store precomputed query results physically.\n",
    "* Automatically updated incrementally when base data changes.\n",
    "* Speed up expensive repeated queries by avoiding re-computation.\n",
    "\n",
    "**Use cases:**\n",
    "\n",
    "* Complex joins or aggregations used often.\n",
    "* Heavy transformation queries.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary Table of Questions and Short Answers\n",
    "\n",
    "| Question                           | Short Summary                                                   |\n",
    "| ---------------------------------- | --------------------------------------------------------------- |\n",
    "| Why no traditional indexes?        | Uses micro-partitions & pruning instead.                        |\n",
    "| What are micro-partitions?         | Small data files with metadata for pruning.                     |\n",
    "| Clustering key?                    | Organizes data for better pruning, costs extra compute.         |\n",
    "| Resize warehouse or rewrite query? | Rewrite first; resize if compute-bound.                         |\n",
    "| Join handling?                     | Automatic broadcast/shuffle; hints to force broadcast.          |\n",
    "| Query profiling role?              | Visualize query steps & bottlenecks.                            |\n",
    "| Caching?                           | Result, local disk, metadata caches speed repeated queries.     |\n",
    "| Avoid skew?                        | Uniform key distribution, clustering, pre-aggregation.          |\n",
    "| Materialized views?                | Precomputed, incrementally maintained results for fast queries. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177728b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
