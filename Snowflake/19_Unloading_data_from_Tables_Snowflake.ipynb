{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97996d50",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Snowflake Staging, Unloading & Downloading — from zero to expert\n",
    "\n",
    "## 0) Mental model (why stages exist at all)\n",
    "\n",
    "* **Tables = kitchen** where data is prepared.\n",
    "* **Stages = pickup counters** (temporary file space). You can write data **out** from tables/queries to a stage (this is called **unloading**), and you can **GET** those staged files down to your laptop.\n",
    "* **Kinds of stages** you’ll meet while downloading:\n",
    "\n",
    "  * **User stage**: `@~` (each user has one).\n",
    "  * **Table stage**: `@%MY_TABLE` (attached to a table).\n",
    "  * **Named stage**: `@MY_STAGE` (a first-class object you create; can be internal or external like S3/GCS/Azure).\n",
    "\n",
    "> Key truth: You don’t download rows directly from a table. You first **unload** rows to a stage (files), then **GET** those files to local.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Two legit ways to “download data” with SnowSQL\n",
    "\n",
    "### Way A — Quick and dirty: print query results to a local file\n",
    "\n",
    "Sometimes you just need a one-off CSV of a query. SnowSQL can spool the **query result** (not staged files) straight to disk.\n",
    "\n",
    "**Example**\n",
    "\n",
    "```bash\n",
    "# 1) Connect\n",
    "snowsql -a <account> -u <user> -r <role> -w <warehouse> -d <database> -s <schema>\n",
    "\n",
    "# 2) (Optional) Make the output CSV-friendly\n",
    "!set output_format=csv\n",
    "!set timing=false\n",
    "\n",
    "# 3) Run your query and send results to a local file\n",
    "!spool /home/you/downloads/orders_2025-08-24.csv\n",
    "SELECT ORDER_ID, CUSTOMER_ID, TOTAL_AMOUNT\n",
    "FROM ANALYTICS.SALES.ORDERS\n",
    "WHERE ORDER_DATE >= '2025-08-01';\n",
    "!spool off\n",
    "```\n",
    "\n",
    "**When to use**: Small/medium result sets, ad-hoc exports, no need to manage stage files.\n",
    "\n",
    "**Limitations**: This is *client* export—no parallelization, no server-side partitioning, and fewer controls (compression, file sizing) than a proper unload.\n",
    "\n",
    "---\n",
    "\n",
    "### Way B — Production-grade: UNLOAD to a stage, then GET to local\n",
    "\n",
    "This is the **recommended** way for anything real: scalable, resumable, controllable formats, partitioning, and good for very large datasets.\n",
    "\n",
    "**Step B1: Create a file format (repeatable, explicit)**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT ff_csv_gz\n",
    "  TYPE = CSV\n",
    "  COMPRESSION = GZIP\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  NULL_IF = ('\\\\N', 'NULL', '');\n",
    "```\n",
    "\n",
    "**Step B2: Choose a stage**\n",
    "\n",
    "* User stage (quick): `@~`\n",
    "* Named internal stage (reusable):\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE mystage_internal\n",
    "  FILE_FORMAT = ff_csv_gz;  -- optional default\n",
    "```\n",
    "\n",
    "**Step B3: UNLOAD (server-side export)**\n",
    "\n",
    "```sql\n",
    "-- Example: unload recent orders to a subfolder in a named stage\n",
    "COPY INTO @mystage_internal/exports/orders/2025-08-24/\n",
    "FROM (\n",
    "  SELECT ORDER_ID, CUSTOMER_ID, TOTAL_AMOUNT, ORDER_DATE\n",
    "  FROM ANALYTICS.SALES.ORDERS\n",
    "  WHERE ORDER_DATE >= '2025-08-01'\n",
    ")\n",
    "FILE_FORMAT = (FORMAT_NAME = ff_csv_gz HEADER = TRUE)\n",
    "OVERWRITE = TRUE           -- replace any prior files in this target path\n",
    "SINGLE = FALSE             -- produce multiple files for parallelism (good for big data)\n",
    "MAX_FILE_SIZE = 50000000;  -- ~50MB target chunks (tune as you like)\n",
    "```\n",
    "\n",
    "> **What is “unloading”?**\n",
    "> In Snowflake, **unloading** means using `COPY INTO <stage>` to write the *result of a table or query* into files (CSV/JSON/PARQUET, compressed or not) on a stage (internal or external). Other databases might have a statement literally named `UNLOAD`; Snowflake uses `COPY INTO` for both *loading* (files → table) and *unloading* (table/query → files).\n",
    "\n",
    "**Step B4: GET the files to your laptop (SnowSQL)**\n",
    "\n",
    "```bash\n",
    "# Connect if not already connected\n",
    "snowsql -a <account> -u <user> -r <role>\n",
    "\n",
    "# Download all files from that path into a local folder\n",
    "GET @mystage_internal/exports/orders/2025-08-24/ file:///home/you/downloads/orders_2025-08-24/ \\\n",
    "  PATTERN='.*' \\\n",
    "  PARALLEL=8;\n",
    "```\n",
    "\n",
    "**Notes**\n",
    "\n",
    "* `GET` is a **SnowSQL** command (runs in the SnowSQL client, not in SQL worksheets).\n",
    "* Files come down **as they exist on the stage**. If you wrote `.gz` files, you’ll get `.gz` locally (which is usually what you want).\n",
    "* Use a **new subfolder per run** (like `.../YYYY-MM-DD/`) for tidy versioning.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) “Can I unload on an existing staging file?”\n",
    "\n",
    "Short, honest answer: **No, you can’t append to an existing file.**\n",
    "\n",
    "* Snowflake **generates file names** for unloads (e.g., `data_0_0_0.csv.gz`, possibly with a query id).\n",
    "* You **cannot** say “write exactly into `orders.csv`” or “append to `orders.csv`”.\n",
    "* You **can** set `OVERWRITE = TRUE` to **replace** any files **already present** in the *target path* (Snowflake will delete/replace files it would otherwise create).\n",
    "* If you need strict idempotency without clobbering, add `INCLUDE_QUERY_ID = TRUE` so filenames include a unique query id:\n",
    "\n",
    "```sql\n",
    "COPY INTO @mystage_internal/exports/orders/2025-08-24/\n",
    "FROM (SELECT ... )\n",
    "FILE_FORMAT=(FORMAT_NAME=ff_csv_gz HEADER=TRUE)\n",
    "OVERWRITE=FALSE\n",
    "INCLUDE_QUERY_ID=TRUE;\n",
    "```\n",
    "\n",
    "* Best practice: **write to a new prefix/folder** each run (`.../run_ts=<timestamp>/`). That makes lineage and reruns painless.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) End-to-end mini “stories” you’ll actually do\n",
    "\n",
    "### Story A: Analyst wants a quick CSV today\n",
    "\n",
    "* Needs small extract, doesn’t care about staging.\n",
    "\n",
    "```bash\n",
    "snowsql -a <acct> -u <user>\n",
    "!set output_format=csv\n",
    "!spool ~/Downloads/top_customers.csv\n",
    "SELECT CUSTOMER_ID, SUM(TOTAL_AMOUNT) AS revenue\n",
    "FROM ANALYTICS.SALES.ORDERS\n",
    "GROUP BY 1\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 1000;\n",
    "!spool off\n",
    "```\n",
    "\n",
    "Done in a minute. Perfect for ad-hoc shares.\n",
    "\n",
    "---\n",
    "\n",
    "### Story B: Data engineer ships a repeatable daily export\n",
    "\n",
    "* Wants compressed, partitioned files, clean foldering, same shape every day.\n",
    "\n",
    "```sql\n",
    "-- 1) One-time setup\n",
    "CREATE OR REPLACE FILE FORMAT ff_csv_gz TYPE=CSV COMPRESSION=GZIP HEADER=TRUE;\n",
    "CREATE OR REPLACE STAGE exports_stage FILE_FORMAT=ff_csv_gz;\n",
    "\n",
    "-- 2) Daily job (e.g., in Task or Airflow)\n",
    "SET run_dt = TO_CHAR(CURRENT_DATE, 'YYYY-MM-DD');\n",
    "\n",
    "COPY INTO @exports_stage/orders/${run_dt}/\n",
    "FROM (\n",
    "  SELECT ORDER_ID, CUSTOMER_ID, ORDER_DATE, TOTAL_AMOUNT\n",
    "  FROM ANALYTICS.SALES.ORDERS\n",
    "  WHERE ORDER_DATE = CURRENT_DATE - 1\n",
    ")\n",
    "FILE_FORMAT=(FORMAT_NAME=ff_csv_gz)\n",
    "OVERWRITE=TRUE\n",
    "SINGLE=FALSE\n",
    "MAX_FILE_SIZE=50000000;\n",
    "```\n",
    "\n",
    "Then your operator (or you) pulls files as needed:\n",
    "\n",
    "```bash\n",
    "snowsql -a <acct> -u <user>\n",
    "GET @exports_stage/orders/2025-08-23/ file:///data/exports/orders/2025-08-23/ PARALLEL=8;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Story C: Use the user stage for quick experiments\n",
    "\n",
    "```sql\n",
    "COPY INTO @~/scratch/orders_sample/\n",
    "FROM (SELECT * FROM ANALYTICS.SALES.ORDERS SAMPLE (1));\n",
    "```\n",
    "\n",
    "```bash\n",
    "GET @~/scratch/orders_sample/ file:///home/you/tmp/orders_sample/;\n",
    "```\n",
    "\n",
    "No objects to manage. Great for trying things.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Common options you’ll actually tune\n",
    "\n",
    "* **FILE\\_FORMAT**: CSV/JSON/PARQUET; compression `GZIP`, `BZIP2`, `ZSTD`, or `NONE`.\n",
    "* **HEADER** (CSV): `TRUE`/`FALSE`.\n",
    "* **SINGLE**:\n",
    "\n",
    "  * `TRUE` = one file (convenient but can be large & slower).\n",
    "  * `FALSE` = multiple files (parallel, scalable).\n",
    "* **MAX\\_FILE\\_SIZE**: Target size per file (helps downstream limits).\n",
    "* **OVERWRITE**: `TRUE` to replace any preexisting output files at that path.\n",
    "* **INCLUDE\\_QUERY\\_ID**: Helps make filenames unique across retries/reruns.\n",
    "* **PATTERN (GET)**: Regex to filter which staged files to download.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Permissions you need (quick checklist)\n",
    "\n",
    "* To **unload**: `SELECT` on the source table/view/query’s objects **and** ability to write to the **target stage** (own user stage is fine; for named stages you need appropriate privileges on the stage’s schema/object).\n",
    "* To **GET**: Ability to **read** from that stage (your user stage is automatically fine; for named stages ensure your role has been granted access).\n",
    "* For **external stages** (S3/GCS/Azure): the stage must be created with working credentials or a **storage integration**; your role needs usage rights on the stage.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Practical troubleshooting & gotchas\n",
    "\n",
    "* **“GET command not found”**: `GET` is a SnowSQL client command. It won’t run in web UI worksheets. Use the SnowSQL CLI.\n",
    "* **“Overwrite or append?”**: You can **replace** (`OVERWRITE=TRUE`), but not **append to an existing file**. Write to new folders for each run if you need both old and new.\n",
    "* **Huge single files**: If downstream tools struggle with one giant file, use `SINGLE=FALSE` and tune `MAX_FILE_SIZE`.\n",
    "* **Local decompression**: If you unloaded with `COMPRESSION=GZIP`, you’ll download `.gz`. Decompress locally if needed (`gunzip`, 7-Zip, etc.).\n",
    "* **Consistent schemas**: Lock your export schema via a view so column order/types remain stable over time.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Fully worked example you can copy-paste\n",
    "\n",
    "**SQL (one-time + daily)**\n",
    "\n",
    "```sql\n",
    "-- One-time setup\n",
    "CREATE OR REPLACE FILE FORMAT ff_csv_gz\n",
    "  TYPE=CSV COMPRESSION=GZIP FIELD_OPTIONALLY_ENCLOSED_BY='\"' NULL_IF=('');\n",
    "\n",
    "CREATE OR REPLACE STAGE daily_exports FILE_FORMAT=ff_csv_gz;\n",
    "\n",
    "-- Daily export (yesterday’s orders)\n",
    "COPY INTO @daily_exports/orders/dt=${TO_CHAR(CURRENT_DATE-1,'YYYY-MM-DD')}/\n",
    "FROM (\n",
    "  SELECT ORDER_ID, CUSTOMER_ID, ORDER_DATE, TOTAL_AMOUNT\n",
    "  FROM ANALYTICS.SALES.ORDERS\n",
    "  WHERE ORDER_DATE = CURRENT_DATE - 1\n",
    ")\n",
    "FILE_FORMAT=(FORMAT_NAME=ff_csv_gz HEADER=TRUE)\n",
    "OVERWRITE=TRUE\n",
    "SINGLE=FALSE\n",
    "MAX_FILE_SIZE=52428800; -- 50MB\n",
    "```\n",
    "\n",
    "**SnowSQL (download)**\n",
    "\n",
    "```bash\n",
    "snowsql -a <account> -u <user> -r <role>\n",
    "GET @daily_exports/orders/dt=2025-08-23/ file:///home/you/exports/orders/2025-08-23/ \\\n",
    "  PATTERN='.*\\.csv\\.gz' PARALLEL=8;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Extra fundamentals that matter (even if not asked)\n",
    "\n",
    "* **Table stage** `@%TABLE_NAME`: Handy when exporting data related to a single table and you want to keep artifacts “with” the table.\n",
    "* **External stages**: If you eventually want others (or Spark/Databricks) to pick up files from S3/GCS/Azure, create a **storage integration** and unload there; the GET step becomes optional because consumers read directly from cloud storage.\n",
    "* **Data formats**: Prefer **PARQUET** for analytics pipelines (columnar, typed); prefer **CSV** for human-sharing or tools that expect CSV.\n",
    "* **Versioning**: Put **dates or run IDs in the path**; don’t rely on overwriting in place.\n",
    "* **Idempotency**: Use `INCLUDE_QUERY_ID=TRUE` or new folders to avoid collisions on retries.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Must-know questions to test yourself\n",
    "\n",
    "1. What are the differences between **user**, **table**, and **named** stages, and when would you choose each for an export?\n",
    "2. Explain **unloading** in Snowflake. Which statement do you use and why is it used for both load and unload?\n",
    "3. How do **SINGLE**, **MAX\\_FILE\\_SIZE**, and **HEADER** affect your exported files?\n",
    "4. Can you **append** to a staged file during unload? If not, what pattern achieves the same business goal?\n",
    "5. When would you choose **CSV** vs **PARQUET** for exports?\n",
    "6. How do you **GET** only certain files from a stage to your laptop?\n",
    "7. What privileges are required for unloading and GET-ing from **named stages** vs the **user stage**?\n",
    "8. Why might you add `INCLUDE_QUERY_ID=TRUE` to your unload, and what problem does it solve?\n",
    "9. How do you design a **repeatable daily export** so that reruns don’t corrupt or mix outputs?\n",
    "10. What are the trade-offs of using **SnowSQL spooling** vs **server-side unload + GET**?\n",
    "\n",
    "---\n",
    "\n",
    "## Quick cheat sheet\n",
    "\n",
    "* **Ad-hoc to local**: `!spool file.csv` … `SELECT ...;` `!spool off`\n",
    "* **Proper export**: `COPY INTO @stage/path/ FROM (SELECT ...) FILE_FORMAT=(...) OVERWRITE=TRUE;`\n",
    "* **Download files**: `GET @stage/path/ file:///local/path/ PATTERN='.*' PARALLEL=8;`\n",
    "* **No append**: Use new path per run or `OVERWRITE=TRUE` to replace.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9798c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Must-Know Q\\&A on Snowflake Unloading & Staging\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Do you need to create a separate stage to unload data?\n",
    "\n",
    "* **No**.\n",
    "\n",
    "  * You can unload into **existing built-in stages**:\n",
    "\n",
    "    * **User stage** (`@~`)\n",
    "    * **Table stage** (`@%MY_TABLE`)\n",
    "  * You create a **named stage** only when you want something reusable, shared, or external.\n",
    "* **Example**:\n",
    "\n",
    "```sql\n",
    "-- Unload to user stage\n",
    "COPY INTO @~/orders_unload/ FROM (SELECT * FROM orders);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Do internal stages cost money?\n",
    "\n",
    "* **Yes**, because files are stored in **Snowflake-managed storage**, billed at the same per-TB rate as table storage.\n",
    "* If you unload 100GB and don’t remove it, you keep paying for it until you `REMOVE`.\n",
    "* **Best practice**: `GET` → download → `REMOVE` from stage.\n",
    "* **Example**:\n",
    "\n",
    "```sql\n",
    "-- After downloading, free up space\n",
    "REMOVE @~/orders_unload/;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Do external stages cost money?\n",
    "\n",
    "* **No Snowflake storage charge**.\n",
    "* Files sit in your cloud bucket (S3, GCS, Azure Blob), so you pay **cloud provider storage fees** instead.\n",
    "* You still pay Snowflake **compute** to run the unload (`COPY INTO`).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. What is “unloading” in Snowflake?\n",
    "\n",
    "* **Unloading = exporting query/table results into files on a stage**.\n",
    "* Done using `COPY INTO @stage ...` (the same command used for loading).\n",
    "* Supports **CSV, PARQUET, JSON** with compression options.\n",
    "* **Example**:\n",
    "\n",
    "```sql\n",
    "COPY INTO @mystage/orders/2025-08-24/\n",
    "FROM (SELECT * FROM orders WHERE order_date = '2025-08-23')\n",
    "FILE_FORMAT=(TYPE=CSV COMPRESSION=GZIP HEADER=TRUE);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Can you unload onto an existing staging file?\n",
    "\n",
    "* **No append to a file.**\n",
    "* You can only:\n",
    "\n",
    "  * **Overwrite files** (`OVERWRITE=TRUE`)\n",
    "  * Or **write to a new folder/prefix** (best practice).\n",
    "* **Workaround for uniqueness**: Use `INCLUDE_QUERY_ID=TRUE`.\n",
    "* **Example**:\n",
    "\n",
    "```sql\n",
    "COPY INTO @mystage/orders/run_2025_08_24/\n",
    "FROM (SELECT * FROM orders)\n",
    "OVERWRITE=TRUE;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. How do you download staged data to your laptop?\n",
    "\n",
    "* Use the **SnowSQL `GET` command** (client-side).\n",
    "* Runs outside SQL worksheets (only in SnowSQL CLI).\n",
    "* **Example**:\n",
    "\n",
    "```bash\n",
    "GET @mystage/orders/run_2025_08_24/ file:///home/you/downloads/orders/2025-08-24/ PARALLEL=8;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. When would you choose CSV vs Parquet for unloads?\n",
    "\n",
    "* **CSV**:\n",
    "\n",
    "  * For business users, Excel, ad-hoc sharing.\n",
    "  * Easy to read, but larger size, no data types.\n",
    "* **Parquet**:\n",
    "\n",
    "  * For data pipelines, analytics, Spark/Databricks/BigQuery.\n",
    "  * Columnar, compressed, type-aware, efficient.\n",
    "* **Rule of thumb**: CSV for humans, Parquet for machines.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. What’s the difference between spooling (`!spool`) vs unloading (`COPY INTO + GET`)?\n",
    "\n",
    "* **Spooling**:\n",
    "\n",
    "  * Done in SnowSQL with `!spool filename.csv`.\n",
    "  * Good for small ad-hoc exports.\n",
    "  * No stage involved.\n",
    "* **Unloading**:\n",
    "\n",
    "  * Server-side export with `COPY INTO`.\n",
    "  * Scalable, supports partitioning, compression, multiple files.\n",
    "  * Best for large/production exports.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. What permissions do you need to unload and download?\n",
    "\n",
    "* **Unloading**:\n",
    "\n",
    "  * `SELECT` privilege on source tables/views.\n",
    "  * `USAGE` + `WRITE` on target stage.\n",
    "* **Downloading (GET)**:\n",
    "\n",
    "  * `READ` privilege on stage.\n",
    "  * User stage (`@~`) needs no extra grant (it’s private).\n",
    "* **External stage**: stage must be created with valid cloud credentials or storage integration, and your role needs access to that stage.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. How do you design a repeatable daily export safely?\n",
    "\n",
    "* Use **new subfolders per run** (`.../YYYY-MM-DD/`).\n",
    "* Use `OVERWRITE=TRUE` only within that run’s folder.\n",
    "* Optionally add `INCLUDE_QUERY_ID=TRUE` for unique file names.\n",
    "* Clean up when done.\n",
    "* **Example**:\n",
    "\n",
    "```sql\n",
    "SET dt = TO_CHAR(CURRENT_DATE-1,'YYYY-MM-DD');\n",
    "\n",
    "COPY INTO @daily_exports/orders/${dt}/\n",
    "FROM (SELECT * FROM orders WHERE order_date = CURRENT_DATE-1)\n",
    "FILE_FORMAT=(TYPE=CSV COMPRESSION=GZIP HEADER=TRUE)\n",
    "OVERWRITE=TRUE;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 Quick Memory Anchors\n",
    "\n",
    "* **Stages = pickup counters** (files wait there before you grab them).\n",
    "* **Internal stage = Snowflake’s pantry** → you pay rent (storage).\n",
    "* **External stage = Your cloud pantry** → you pay AWS/GCP/Azure.\n",
    "* **Unload = COPY INTO @stage**.\n",
    "* **No appends** → overwrite or version folders.\n",
    "* **Download = GET** in SnowSQL, not in web UI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae8ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
