{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28a0a8f",
   "metadata": {},
   "source": [
    "# Snowflake → S3 Unloading (with stories, gotchas & rock-solid validation)\n",
    "\n",
    "Imagine you’re the data engineering lead for “SkyCart,” an e-commerce company. Every morning by 06:00, Marketing expects a fresh, partitioned export of yesterday’s orders in S3 for downstream tools (Athena, Glue jobs, a Python notebook). Your job: make the unload **correct, repeatable, and easy to audit**—and be able to prove it.\n",
    "\n",
    "Below I’ll teach you the fundamentals, then go step-by-step (with copy-pasteable SQL), then deep-dive into validation so you can **prove** the S3 files are complete and correct. I’ll also add missing but important topics (formats, partitioning, encryption, performance, costs, and failure modes) and finish with must-know practice questions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) The 3 core ideas (fundamentals)\n",
    "\n",
    "1. **UNLOAD = `COPY INTO <external location>`**\n",
    "   In Snowflake, unloading means running `COPY INTO '<s3://...>' FROM (<query>)` (or `COPY INTO @my_external_stage/... FROM (<query>)`). You can use a named external stage (recommended for security) or a direct S3 URL with a storage integration. Options control file format (CSV/Parquet/JSON), partitioning, compression, naming, and overwrite behavior. ([Snowflake Documentation][1])\n",
    "\n",
    "2. **Consistency & repeatability**\n",
    "   Every `SELECT` (and your unload query is a `SELECT`) reads a **single snapshot** of data as of the moment the statement starts. To make validation bullet-proof across multiple statements, capture a timestamp and use the **Time Travel `AT (TIMESTAMP => …)`** clause in both your `COUNT(*)` validation and the unload query so they read **the same snapshot**.\n",
    "\n",
    "3. **Validation is not optional**\n",
    "   You want to prove:\n",
    "\n",
    "   * the **row count** in S3 == the **row count** from Snowflake at the same snapshot;\n",
    "   * the **file set** in S3 is what you expect (names/partitions);\n",
    "   * the **data shape** matches (columns, types, null handling, delimiters).\n",
    "     We’ll use: the `COPY` output (with `DETAILED_OUTPUT=TRUE`), a **read-back query** directly from S3 via the stage, and optional spot checks/hashes. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "## 2) One-time setup (the secure way)\n",
    "\n",
    "### a) Create an AWS IAM Role and Storage Integration\n",
    "\n",
    "You’ll let Snowflake **assume** an IAM role to write into your bucket/prefix.\n",
    "\n",
    "**In Snowflake:**\n",
    "\n",
    "```sql\n",
    "use role ACCOUNTADMIN;\n",
    "\n",
    "-- 1) Create storage integration (replace ARNs and bucket path)\n",
    "create or replace storage integration SKY_OUT_INT\n",
    "  type = external_stage\n",
    "  storage_provider = s3\n",
    "  enabled = true\n",
    "  storage_aws_role_arn = 'arn:aws:iam::123456789012:role/skycart-snowflake-writer'\n",
    "  storage_allowed_locations = ('s3://skycart-analytics/exports/');\n",
    "\n",
    "-- 2) Get values to finish AWS trust (external id, user arn)\n",
    "describe integration SKY_OUT_INT;\n",
    "```\n",
    "\n",
    "*In AWS IAM, create/update the role trust policy to allow Snowflake’s **AWS IAM user ARN** with the **external ID** returned by `DESCRIBE INTEGRATION`, and attach an S3 policy allowing `s3:PutObject`, `s3:ListBucket`, (optionally `s3:DeleteObject` if you’ll use `OVERWRITE=TRUE`) on the allowed prefix.* ([Snowflake Documentation][2])\n",
    "\n",
    "### b) (Recommended) Create a named external stage\n",
    "\n",
    "```sql\n",
    "use role SYSADMIN;\n",
    "use database PROD;\n",
    "use schema SHARED;\n",
    "\n",
    "create or replace stage SKY_S3_STAGE\n",
    "  url = 's3://skycart-analytics/exports/'\n",
    "  storage_integration = SKY_OUT_INT;\n",
    "```\n",
    "\n",
    "Named stages centralize credentials and let you query/list files easily later. ([Snowflake Documentation][3])\n",
    "\n",
    "### c) Define reusable file formats\n",
    "\n",
    "We’ll demo **Parquet** (great for downstream analytics) and **CSV** (for tools that need delimited text).\n",
    "\n",
    "```sql\n",
    "-- Parquet format (column names preserved, compressed)\n",
    "create or replace file format FF_PARQUET\n",
    "  type = parquet;\n",
    "\n",
    "-- CSV format (standard, with headers)\n",
    "create or replace file format FF_CSV\n",
    "  type = csv\n",
    "  field_delimiter = ','\n",
    "  record_delimiter = '\\n'\n",
    "  skip_header = 0\n",
    "  null_if = ('\\\\N','NULL')\n",
    "  empty_field_as_null = true\n",
    "  field_optionally_enclosed_by = '\"'\n",
    "  compression = auto;  -- Snowflake chooses sensible default\n",
    "```\n",
    "\n",
    "(You can override these inline on the COPY command if needed.) ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "## 3) The unload—**step-by-step with a real scenario**\n",
    "\n",
    "### Goal\n",
    "\n",
    "Export “yesterday’s” completed orders from `PROD.SALES.ORDERS` into S3, **partitioned by `order_date`** (one folder per day), in **Parquet**, and make it **idempotent**.\n",
    "\n",
    "### a) Capture a consistent snapshot time\n",
    "\n",
    "```sql\n",
    "use role ANALYST;\n",
    "use warehouse ETL_XL;\n",
    "use database PROD;\n",
    "use schema SALES;\n",
    "\n",
    "set SNAP_TS = current_timestamp();\n",
    "```\n",
    "\n",
    "### b) (Optional preview) Validate the query itself before exporting\n",
    "\n",
    "```sql\n",
    "-- Prove the query returns the expected rows (no unload yet)\n",
    "select * from (\n",
    "  select order_id, customer_id, total_amount, order_date, updated_at\n",
    "  from ORDERS\n",
    "  where status = 'COMPLETED'\n",
    "    and order_date = dateadd(day, -1, current_date())\n",
    "  qualify row_number() over (order by order_id) <= 5\n",
    ")\n",
    "  at (timestamp => $SNAP_TS);\n",
    "```\n",
    "\n",
    "For quick “does my query work?” checks, Snowflake also supports `VALIDATION_MODE = RETURN_ROWS` on `COPY INTO <location>` to run the query instead of unloading. ([Snowflake Documentation][1])\n",
    "\n",
    "### c) Count rows at the same snapshot (for later comparison)\n",
    "\n",
    "```sql\n",
    "set ROWS_EXPECTED = (\n",
    "  select count(*) from ORDERS\n",
    "    at (timestamp => $SNAP_TS)\n",
    "   where status = 'COMPLETED'\n",
    "     and order_date = dateadd(day, -1, current_date())\n",
    ");\n",
    "select $ROWS_EXPECTED as rows_expected;\n",
    "```\n",
    "\n",
    "### d) Unload to S3 (Parquet, partitioned)\n",
    "\n",
    "```sql\n",
    "-- Folder: s3://skycart-analytics/exports/orders/parquet/\n",
    "copy into @PROD.SHARED.SKY_S3_STAGE/orders/parquet/\n",
    "from (\n",
    "  select order_id, customer_id, total_amount, order_date, updated_at\n",
    "  from ORDERS\n",
    "    at (timestamp => $SNAP_TS)\n",
    "  where status = 'COMPLETED'\n",
    "    and order_date = dateadd(day, -1, current_date())\n",
    ")\n",
    "file_format = (format_name = FF_PARQUET)\n",
    "-- Write files under a predictable partition folder structure:\n",
    "partition by (to_varchar(order_date, 'YYYY-MM-DD'))\n",
    "-- Helpful for uniqueness/auditing of filenames:\n",
    "include_query_id = true\n",
    "-- Return per-file stats so we can sum/verify:\n",
    "detailed_output = true;\n",
    "```\n",
    "\n",
    "> **Notes you should understand**\n",
    ">\n",
    "> * `PARTITION BY` creates folder layers like `…/partition_0=2025-08-29/…`. (Names may reflect column order as `partition_0`, etc.) This is ideal for downstream engines.\n",
    "> * `PARTITION BY` **cannot** be combined with `SINGLE=TRUE` or `OVERWRITE=TRUE`. If you need to overwrite, target a **new** dated prefix each day (e.g., `…/dt=2025-08-29/`) and manage retention with lifecycle rules. ([Snowflake Documentation][1])\n",
    "\n",
    "### e) (Alternative) Unload to S3 in CSV with headers\n",
    "\n",
    "```sql\n",
    "copy into 's3://skycart-analytics/exports/orders/csv/'\n",
    "  from ( select * from ORDERS\n",
    "           at (timestamp => $SNAP_TS)\n",
    "         where status='COMPLETED'\n",
    "           and order_date = dateadd(day, -1, current_date()) )\n",
    "  storage_integration = SKY_OUT_INT\n",
    "  file_format = (format_name = FF_CSV)\n",
    "  header = true\n",
    "  max_file_size = 50000000         -- ~50 MB target chunks\n",
    "  include_query_id = true\n",
    "  detailed_output = true;\n",
    "```\n",
    "\n",
    "> **CSV gotchas**: Think through **NULL vs empty string**, quotes and escapes. Set `NULL_IF` and `FIELD_OPTIONALLY_ENCLOSED_BY` consciously to avoid downstream surprises. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Validation: **prove nothing is missing and the data matches**\n",
    "\n",
    "### A) Use the `COPY` result set as your first audit\n",
    "\n",
    "`COPY INTO <location>` returns a result set you can immediately capture:\n",
    "\n",
    "```sql\n",
    "-- Capture the unload report for auditing\n",
    "create or replace temporary table TMP_UNLOAD_AUDIT as\n",
    "select * from table(result_scan(last_query_id()));\n",
    "select sum(rows_unloaded) as rows_in_files from TMP_UNLOAD_AUDIT;\n",
    "```\n",
    "\n",
    "* With `DETAILED_OUTPUT=TRUE`, you get **one row per file** including `file_path`, file size, and `rows_unloaded`. Summing `rows_unloaded` gives total exported rows.\n",
    "* `INCLUDE_QUERY_ID=TRUE` stamps filenames with the query id, which is gold for traceability/deduping.\n",
    "  (These behaviors are documented in the `COPY INTO <location>` options.) ([Snowflake Documentation][1])\n",
    "\n",
    "**Compare totals**:\n",
    "\n",
    "```sql\n",
    "select $ROWS_EXPECTED as rows_expected;\n",
    "select sum(rows_unloaded) as rows_in_files from TMP_UNLOAD_AUDIT;\n",
    "```\n",
    "\n",
    "These must match exactly. If not, investigate (see “Common pitfalls & fixes” below).\n",
    "\n",
    "### B) List and verify file/partition structure\n",
    "\n",
    "```sql\n",
    "-- Show what landed in S3 (via the external stage)\n",
    "list @PROD.SHARED.SKY_S3_STAGE/orders/parquet/;\n",
    "```\n",
    "\n",
    "You can also create a quick directory report (names, sizes, timestamps) and diff it over time. Snowflake stages and listing semantics are well documented. ([Snowflake Documentation][4])\n",
    "\n",
    "### C) **Read back from S3** and recount in Snowflake\n",
    "\n",
    "You can query staged files directly—no table needed:\n",
    "\n",
    "```sql\n",
    "-- Recount Parquet files directly in S3\n",
    "select count(*) as rows_read_back\n",
    "from @PROD.SHARED.SKY_S3_STAGE/orders/parquet/\n",
    "  ( file_format => 'FF_PARQUET' );\n",
    "\n",
    "-- Recount CSV\n",
    "select count(*) as rows_read_back\n",
    "from @PROD.SHARED.SKY_S3_STAGE/orders/csv/\n",
    "  ( file_format => 'FF_CSV' );\n",
    "```\n",
    "\n",
    "This is the cleanest way to prove “what’s in S3 equals the source snapshot.” It’s officially supported for both internal and external stages. ([Snowflake Documentation][4])\n",
    "\n",
    "### D) Optional content checks\n",
    "\n",
    "Do light spot checks to catch delimiter/encoding issues:\n",
    "\n",
    "```sql\n",
    "-- Spot check min/max by partition\n",
    "select to_varchar(metadata$filename) as file, count(*) cnt, min($3) min_amt, max($3) max_amt\n",
    "from @PROD.SHARED.SKY_S3_STAGE/orders/csv/ (file_format => 'FF_CSV')\n",
    "group by 1\n",
    "order by 1\n",
    "limit 20;\n",
    "```\n",
    "\n",
    "(`METADATA$FILENAME` and friends are accessible when querying staged files.) ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Formats, encryption, performance & costs (things people forget)\n",
    "\n",
    "### Choosing a file format\n",
    "\n",
    "* **Parquet** (default Snappy compression) → smaller files, typed columns, faster scans in Athena/Glue/Spark.\n",
    "* **CSV** → human-friendly, ubiquitous, but bigger and needs careful null/quote handling.\n",
    "  Snowflake supports Parquet/CSV/JSON/Avro/ORC for unload. See file-format options in `COPY INTO <location>`. ([Snowflake Documentation][1])\n",
    "\n",
    "### Partitioning strategy\n",
    "\n",
    "Use `PARTITION BY` on fields you’ll filter downstream (e.g., `order_date`, `region`). It creates hierarchical folders and can drastically cut costs in engines like Athena. Remember: not compatible with `SINGLE=TRUE` or `OVERWRITE=TRUE`. ([Snowflake Documentation][1])\n",
    "\n",
    "### Encryption\n",
    "\n",
    "If your bucket uses SSE-S3 by default, you’re fine. If you need **KMS**:\n",
    "\n",
    "```sql\n",
    "copy into @SKY_S3_STAGE/secure/orders/\n",
    "from ( select ... )\n",
    "file_format = (format_name = FF_PARQUET)\n",
    "encryption = ( type = 'AWS_SSE_KMS', kms_key_id = 'arn:aws:kms:us-east-1:123456789012:key/abcd-...' );\n",
    "```\n",
    "\n",
    "Supported values are `AWS_SSE_S3`, `AWS_SSE_KMS`, or `NONE`. ([Snowflake Documentation][1])\n",
    "\n",
    "### Performance tips\n",
    "\n",
    "* **Don’t `ORDER BY`** in unload queries unless you need it; it forces global sort and slows things down.\n",
    "* Tune **`MAX_FILE_SIZE`** to produce 50–250 MB compressed files for balanced parallelism.\n",
    "* For huge exports, prefer **Parquet** and **partitioning**.\n",
    "* Size the warehouse (e.g., `ETL_XL` vs `ETL_2XL`) and consider multi-cluster if concurrency is needed.\n",
    "\n",
    "### Costs\n",
    "\n",
    "* You pay for **warehouse time** during unload.\n",
    "* **Cloud egress** can apply if your Snowflake account region and S3 bucket region differ—keep them co-located.\n",
    "* S3 storage and request costs apply as usual.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Common pitfalls & how to fix them\n",
    "\n",
    "1. **Counts don’t match.**\n",
    "\n",
    "   * You didn’t read the same snapshot: ensure both `COUNT(*)` and `COPY` use `AT (TIMESTAMP => $SNAP_TS)`.\n",
    "   * You applied filters differently: copy the exact `WHERE` clause into both.\n",
    "   * Your CSV file format is swallowing rows (e.g., multiline fields or bad quoting). Revisit `FIELD_OPTIONALLY_ENCLOSED_BY`, `ESCAPE_UNENCLOSED_FIELD`, `RECORD_DELIMITER`. Try a small sample and read back from S3 to spot the pattern.\n",
    "\n",
    "2. **“AccessDenied” or nothing lands in S3.**\n",
    "\n",
    "   * Storage integration lacks permission to the exact prefix, or the bucket policy/trust policy isn’t set with Snowflake’s **IAM user ARN** + **external ID**. Recheck `DESCRIBE INTEGRATION` and bucket/role policies. ([Snowflake Documentation][2])\n",
    "\n",
    "3. **`PARTITION BY` with `OVERWRITE`/`SINGLE` errors.**\n",
    "   Limitations are by design; use a new dated prefix instead of overwrite, and avoid `SINGLE=TRUE` when partitioning. ([Snowflake Documentation][1])\n",
    "\n",
    "4. **Parquet + timezone types.**\n",
    "   Certain timestamp flavors (e.g., `TIMESTAMP_TZ`/`TIMESTAMP_LTZ`) can error when unloading to Parquet; cast to `TIMESTAMP_NTZ` first (best practice in many pipelines). ([Snowflake Documentation][1])\n",
    "\n",
    "5. **Zero rows exported.**\n",
    "   Snowflake won’t create a data file if the query returns 0 rows. If your downstream expects the path to exist, create a small marker file separately or design consumers to tolerate missing partitions. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "## 7) A tidy, reusable “daily export” pattern (put it on a TASK)\n",
    "\n",
    "1. Wrap the steps into a stored procedure (capture snapshot → count → unload → verify → write audit row).\n",
    "2. Schedule with a **TASK** at 05:45 so files are ready by 06:00.\n",
    "3. Use a **date-stamped prefix**: `.../orders/dt=YYYY-MM-DD/` to avoid overwrites and make lineage clean.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Full walk-through (copy/paste block)\n",
    "\n",
    "```sql\n",
    "-- 0) Context\n",
    "use role SYSADMIN;\n",
    "use warehouse ETL_XL;\n",
    "use database PROD;\n",
    "use schema SALES;\n",
    "\n",
    "-- 1) Snapshot & expected count\n",
    "set SNAP_TS = current_timestamp();\n",
    "\n",
    "set ROWS_EXPECTED = (\n",
    "  select count(*) from ORDERS\n",
    "    at (timestamp => $SNAP_TS)\n",
    "   where status='COMPLETED'\n",
    "     and order_date = dateadd(day, -1, current_date())\n",
    ");\n",
    "\n",
    "-- 2) Unload to S3 (Parquet, partitioned, auditable names)\n",
    "copy into @PROD.SHARED.SKY_S3_STAGE/orders/parquet/\n",
    "from (\n",
    "  select\n",
    "    order_id, customer_id, total_amount::number(12,2) as total_amount,\n",
    "    order_date, updated_at::timestamp_ntz as updated_at\n",
    "  from ORDERS at (timestamp => $SNAP_TS)\n",
    "  where status='COMPLETED'\n",
    "    and order_date = dateadd(day, -1, current_date())\n",
    ")\n",
    "file_format = (format_name = FF_PARQUET)\n",
    "partition by (to_varchar(order_date, 'YYYY-MM-DD'))\n",
    "include_query_id = true\n",
    "detailed_output = true;\n",
    "\n",
    "-- 3) Audit the COPY output\n",
    "create or replace temporary table TMP_UNLOAD_AUDIT as\n",
    "select * from table(result_scan(last_query_id()));\n",
    "\n",
    "select $ROWS_EXPECTED as rows_expected;\n",
    "select sum(rows_unloaded) as rows_in_files from TMP_UNLOAD_AUDIT;\n",
    "\n",
    "-- 4) Read back from S3 and compare again\n",
    "select count(*) as rows_read_back\n",
    "from @PROD.SHARED.SKY_S3_STAGE/orders/parquet/\n",
    "  (file_format => 'FF_PARQUET');\n",
    "\n",
    "-- 5) Optional: list files (eyeball partitions & sizes)\n",
    "list @PROD.SHARED.SKY_S3_STAGE/orders/parquet/;\n",
    "```\n",
    "\n",
    "(Options like `ENCRYPTION = (TYPE='AWS_SSE_KMS', KMS_KEY_ID='…')`, or switching to CSV with `FILE_FORMAT=FF_CSV` are easy tweaks.) ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Your validation checklist (print-worthy)\n",
    "\n",
    "* [ ] **Same snapshot** used in both count and unload (`AT (TIMESTAMP => $SNAP_TS)`).\n",
    "* [ ] `COPY` **result set captured**; sum of `rows_unloaded` equals expected count. ([Snowflake Documentation][1])\n",
    "* [ ] **Read-back count** from S3 via stage equals expected count. ([Snowflake Documentation][4])\n",
    "* [ ] **Partitions present** as designed (e.g., `partition_0=YYYY-MM-DD/`).\n",
    "* [ ] **File sizes reasonable** (not tons of tiny files, not a single huge file).\n",
    "* [ ] **CSV**: nulls/quotes/escapes validated; **Parquet**: timestamp types cast as needed. ([Snowflake Documentation][1])\n",
    "* [ ] (If required) **Encryption** validated (KMS key id). ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Extra patterns you’ll use in the wild\n",
    "\n",
    "* **Idempotency**: Use `INCLUDE_QUERY_ID=TRUE` to avoid accidental overwrites, or emit into a **dated prefix** and treat each run as immutable output. ([Snowflake Documentation][1])\n",
    "* **Schema evolution**: Prefer Parquet; CSV + headers is fragile for evolving schemas.\n",
    "* **Downstream friendliness**: Pick partition columns your consumers filter by; avoid too many tiny files.\n",
    "* **Auditing**: Insert a row into an `EXPORT_AUDIT` table with `query_id`, `rows_expected`, `rows_in_files`, `prefix`, and a `verification_status`.\n",
    "\n",
    "---\n",
    "\n",
    "## Must-know questions (to test yourself)\n",
    "\n",
    "1. What are the pros/cons of unloading to a **named external stage** vs a direct **S3 URL** with `STORAGE_INTEGRATION`?\n",
    "2. How does Snowflake guarantee **read consistency**, and how do you use `AT (TIMESTAMP => …)` for validation across multiple statements?\n",
    "3. Why use **`INCLUDE_QUERY_ID`** and **`DETAILED_OUTPUT`** in `COPY INTO <location>`? What do you get back and how do you use it? ([Snowflake Documentation][1])\n",
    "4. Explain why **`PARTITION BY`** can’t be combined with **`SINGLE=TRUE`** or **`OVERWRITE=TRUE`**, and how you design around it. ([Snowflake Documentation][1])\n",
    "5. When would you pick **Parquet** over **CSV**, and what **CSV file format** options prevent data corruption (nulls, quotes, newlines)? ([Snowflake Documentation][1])\n",
    "6. How do you **read back** your S3 files in Snowflake to validate counts and content without loading into a table? Show the exact SQL. ([Snowflake Documentation][4])\n",
    "7. What permissions/policies are required for a **storage integration** to write to S3, and how do **external ID** and **IAM trust** fit in? ([Snowflake Documentation][2])\n",
    "8. What are the **encryption** options for unloading to S3 and when do you need `AWS_SSE_KMS`? Show the syntax. ([Snowflake Documentation][1])\n",
    "9. What happens if your query returns **zero rows** and your downstream expects a file? How do you design for this? ([Snowflake Documentation][1])\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1555b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
