{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021c6569",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Frequent Real-World Snowflake Staging Issues (with detailed solutions)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. üö´ **Table Stages are not reusable across multiple tables**\n",
    "\n",
    "* **Issue:** Developers often assume that a *table stage* (`@%table_name`) is a general-purpose location. But in Snowflake, a **table stage is tied only to that specific table**. You cannot use it to load data into other tables.\n",
    "* **Why it happens:** Table stages are designed for temporary, table-specific loads. They‚Äôre automatically created per table and are not shared resources.\n",
    "* **Real case:** A dev loads a CSV into `@%orders` stage but later wants to load the same file into `customers` ‚Äî they get an error because `@%orders` belongs strictly to `orders` table.\n",
    "* **Solution:**\n",
    "\n",
    "  * If you want to use data across multiple tables ‚Üí use a **named stage** (`CREATE STAGE my_stage`) or an **external stage** (S3, Azure Blob, GCS).\n",
    "  * Rule of thumb: *Table stages = quick one-off loads. Named stages = reusable pipelines.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üéØ **Loading specific columns from staging files**\n",
    "\n",
    "* **Issue:** CSV files often contain more columns than needed. Developers mistakenly think they must match table schema exactly or load all columns.\n",
    "* **Why it happens:** `COPY INTO` works by positional mapping of file columns ‚Üí table columns. If you don‚Äôt align them correctly, data mismatches or load failures happen.\n",
    "* **Solution:**\n",
    "\n",
    "  * Use **file formats** (`CREATE FILE FORMAT`) with options like `SKIP_HEADER`, `FIELD_OPTIONALLY_ENCLOSED_BY` to clean up raw files.\n",
    "  * In `COPY INTO`, you can **select only the columns you need**:\n",
    "\n",
    "    ```sql\n",
    "    COPY INTO my_table(col1, col2, col3)\n",
    "    FROM (SELECT t.$1, t.$3, t.$5\n",
    "          FROM @my_stage/file.csv (FILE_FORMAT => my_csv_format) t);\n",
    "    ```\n",
    "  * This way you can ignore unnecessary columns and load only the required ones.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üìù **Tracking file names & rows for auditing**\n",
    "\n",
    "* **Issue:** In regulated environments, devs need to track *which file a record came from* and sometimes even the row number.\n",
    "* **Why it happens:** By default, Snowflake just loads the data without adding file lineage unless explicitly told.\n",
    "* **Solution:** Use Snowflake‚Äôs **metadata columns**:\n",
    "\n",
    "  * `METADATA$FILENAME` ‚Üí gives you the source file name\n",
    "  * `METADATA$FILE_ROW_NUMBER` ‚Üí gives you the line number\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO my_table(col1, col2, file_name, row_number)\n",
    "  FROM (\n",
    "      SELECT t.$1, t.$2, METADATA$FILENAME, METADATA$FILE_ROW_NUMBER\n",
    "      FROM @my_stage/file.csv (FILE_FORMAT => my_csv_format) t\n",
    "  );\n",
    "  ```\n",
    "\n",
    "  ‚úÖ Best practice: Always store these in audit tables for traceability.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. üîç **Debugging mismatched values or column counts**\n",
    "\n",
    "* **Issue:** Sometimes devs notice wrong/missing values after loading ‚Äî often because **number of table columns ‚â† number of file columns**.\n",
    "* **Why it happens:** COPY INTO does a positional mapping ‚Äî `$1 ‚Üí col1`, `$2 ‚Üí col2`. If the file has more/fewer columns, you‚Äôll either get errors or garbage data in wrong columns.\n",
    "* **Solution:**\n",
    "\n",
    "  * ‚úÖ Before loading, **query staged files directly**:\n",
    "\n",
    "    ```sql\n",
    "    SELECT $1, $2, $3\n",
    "    FROM @my_stage/file.csv (FILE_FORMAT => my_csv_format);\n",
    "    ```\n",
    "\n",
    "    This avoids having to download and check manually.\n",
    "  * Use `VALIDATION_MODE = RETURN_ERRORS` in COPY INTO to simulate load and see mismatches before actually loading.\n",
    "  * If file schema evolves frequently, store data in a **raw landing table (all columns as VARIANT)**, then transform with SQL into structured tables.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÇÔ∏è **Using SPLIT\\_PART during file loading**\n",
    "\n",
    "* **Issue:** Developers sometimes wonder: *Why use `SPLIT_PART` if the file already has delimiters?*\n",
    "* **Why it happens:** Sometimes raw files are not cleanly delimited (e.g., entire row is dumped as one string with `|` inside). Or files have extra JSON/XML data inside a column.\n",
    "* **Solution:** Use `SPLIT_PART` in a staging query:\n",
    "\n",
    "  ```sql\n",
    "  COPY INTO my_table(col1, col2)\n",
    "  FROM (\n",
    "    SELECT SPLIT_PART($1, '|', 1),\n",
    "           SPLIT_PART($1, '|', 2)\n",
    "    FROM @my_stage/file.csv\n",
    "  );\n",
    "  ```\n",
    "\n",
    "  ‚úÖ This helps when files don‚Äôt follow consistent formatting rules.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. üóÇÔ∏è **Recompression confusion**\n",
    "\n",
    "* **Issue:** Developers wonder: *What happens if I re-compress a file (gzip again) and re-upload?*\n",
    "* **Reality:** Snowflake auto-detects file compression (gzip, bz2, etc.). If you re-compress and upload as a new file:\n",
    "\n",
    "  * Snowflake can still read it fine.\n",
    "  * But if the **file name changes**, Snowflake sees it as a new file ‚Üí risk of duplicates.\n",
    "  * If **file name is same and overwrite = true**, old file is replaced.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. üîÑ **Re-uploading same files ‚Üí duplicates**\n",
    "\n",
    "* **Your Question:** *How can duplicates happen if `OVERWRITE=FALSE` makes Snowflake ignore same-named files?*\n",
    "* **Explanation:**\n",
    "\n",
    "  * Snowflake tracks **file ingestion by file name**, not by file content.\n",
    "  * If you re-upload the same file but with a **different name** (e.g., `orders.csv` ‚Üí `orders_v2.csv`), Snowflake treats it as new ‚Üí loads again ‚Üí duplicates.\n",
    "  * If pipeline renames files automatically (common in ETL jobs, like `file_20250101.csv`, `file_20250101_copy.csv`) ‚Üí duplicates slip in.\n",
    "* **Solution:**\n",
    "\n",
    "  * Use **deduplication strategy**:\n",
    "\n",
    "    * Add a unique file tracking table (insert filename + checksum after load).\n",
    "    * On subsequent loads, check if file already processed.\n",
    "  * Or enforce `ON_ERROR = SKIP_FILE` and deduplication logic at query level (`ROW_NUMBER()` partitioned by business keys, keep only latest).\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ‚ö†Ô∏è **File format mismatch issues**\n",
    "\n",
    "* **Issue:** Common when delimiter/encoding/quotes are different than expected ‚Üí load either fails or scrambles data.\n",
    "* **Example:** CSV has `;` delimiter but file format defined as `,`. ‚Üí Snowflake loads entire row as one column.\n",
    "* **Solution:**\n",
    "\n",
    "  * Always test files by querying stage before loading.\n",
    "  * Maintain **centralized reusable file formats** (not per dev).\n",
    "  * Keep strict contracts with data providers.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. üïê **Slow loads due to large numbers of small files**\n",
    "\n",
    "* **Issue:** Many small files ‚Üí Snowflake spends time on overhead (opening/closing connections, planning), not actual data loading.\n",
    "* **Solution:**\n",
    "\n",
    "  * Bundle small files before staging (e.g., 100MB chunks are optimal).\n",
    "  * Use external stages with Snowpipe auto-ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. üß© **Semi-structured data confusion**\n",
    "\n",
    "* **Issue:** JSON/Parquet files staged, but devs try to load directly into relational tables and fail.\n",
    "* **Solution:**\n",
    "\n",
    "  * Load into a **VARIANT column** first.\n",
    "  * Then use Snowflake functions (`:key`, `OBJECT_INSERT`, `ARRAY_AGG`) to transform into structured tables.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary**:\n",
    "\n",
    "* Table stages are isolated.\n",
    "* Column mismatches ‚Üí query staged files first.\n",
    "* Use metadata columns for traceability.\n",
    "* SPLIT\\_PART helps with messy delimiters.\n",
    "* Re-compressing is safe, but renaming files creates duplicates.\n",
    "* Deduplication strategy is a must in production.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b8d95",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **1. ‚ùå File not visible in stage after PUT command**\n",
    "\n",
    "**Problem:**\n",
    "You uploaded a file using `PUT` but when you query the stage (`LIST @mystage;`), the file doesn‚Äôt show up.\n",
    "**Why it happens:**\n",
    "\n",
    "* Wrong path used in the stage.\n",
    "* File uploaded to the wrong stage (user stage instead of table stage).\n",
    "* Local file path was incorrect in the `PUT`.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Always `LIST` immediately after `PUT` to confirm file presence.\n",
    "* Use fully qualified stage paths (`@~`, `@%table`, `@schema.stage`).\n",
    "* If using automation, log the `PUT` output because it shows where the file was actually stored.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. üïí Time travel confusion ‚Äî why is old file still loading?**\n",
    "\n",
    "**Problem:**\n",
    "You re-uploaded a file with the same name and set `OVERWRITE=TRUE`, but somehow the old data is still loaded.\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "* Snowflake caches file history to prevent accidental re-ingestion.\n",
    "* The **COPY INTO** command checks file metadata (name, size, checksum). If unchanged, it doesn‚Äôt reload.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Use `FORCE=TRUE` in `COPY INTO` to force reload regardless of history.\n",
    "* Or version your files (e.g., `data_20250824_v1.csv`) to avoid confusion.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. üìÇ Folder structures in external stages (S3, Azure, GCS)**\n",
    "\n",
    "**Problem:**\n",
    "External stage has nested folders (`/2025/08/24/data.csv`). Developer runs `COPY INTO` without recursive settings, and no data is loaded.\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "* By default, Snowflake doesn‚Äôt scan subdirectories unless `PATTERN` or `RECURSIVE=TRUE` is used.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Use `COPY INTO ... PATTERN='.*2025/08/24/.*'`\n",
    "* Or set `RECURSIVE=TRUE` if you want to scan folders automatically.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. üè∑ Wrong file format used during COPY**\n",
    "\n",
    "**Problem:**\n",
    "File loads with garbage values, NULLs, or unexpected column splits.\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "* Used wrong delimiter (`,` vs `|`)\n",
    "* CSV with headers wasn‚Äôt set with `SKIP_HEADER=1`.\n",
    "* Compression type mismatch (gz vs parquet vs json).\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Always define `FILE_FORMAT` explicitly (never rely on defaults).\n",
    "* Test with a small sample file before large loads.\n",
    "* Use `VALIDATION_MODE=RETURN_ERRORS` to preview issues without loading data.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. üßπ ‚ÄúPhantom files‚Äù causing reprocessing**\n",
    "\n",
    "**Problem:**\n",
    "Even after processing, files keep showing up in the stage or reload unexpectedly.\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "* `COPY INTO` doesn‚Äôt delete files from stage.\n",
    "* Files must be manually removed (internal stage) or lifecycle-managed (external stage).\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* For internal stages: run `REMOVE @mystage/file.csv;` after load.\n",
    "* For external stages: configure storage lifecycle rules (S3 lifecycle, GCS bucket policy).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. üîë Permissions issue on external stage**\n",
    "\n",
    "**Problem:**\n",
    "`COPY INTO` fails with *‚ÄúAccess denied‚Äù* when trying to read from S3/Azure/GCS.\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "* Wrong IAM role or service principal permissions.\n",
    "* Snowflake storage integration not correctly configured.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Use **storage integration objects** instead of embedding keys.\n",
    "* Verify external bucket policies (must allow Snowflake‚Äôs cloud provider service principal).\n",
    "* Test by running `LIST @ext_stage;` to check access.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. üß© Schema evolution issue (new columns in file)**\n",
    "\n",
    "**Problem:**\n",
    "Your files now have 12 columns, but the table has 10. The load fails.\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "* Snowflake expects file columns to match table columns by position.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Create a staging table with flexible schema (`VARIANT` column for JSON, or wide VARCHARs for CSV).\n",
    "* Load raw ‚Üí then transform into target table.\n",
    "* If CSV, use `FILE_FORMAT (ERROR_ON_COLUMN_COUNT_MISMATCH=FALSE)` to load partial columns.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. ‚è≥ Large file splitting issues**\n",
    "\n",
    "**Problem:**\n",
    "Loading a single huge 200GB file takes forever.\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "* Snowflake parallelism shines when files are **many small chunks** (ideal: 100‚Äì250MB compressed).\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Pre-split large files before loading.\n",
    "* For external stages (S3), use tools like AWS Glue or Spark to split.\n",
    "* For internal stages, use Snowflake‚Äôs `COPY INTO` auto-splitting for supported formats like Parquet.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. üßÆ Duplicate rows after re-uploads (your last question)**\n",
    "\n",
    "**Problem:**\n",
    "Even with `OVERWRITE=FALSE`, duplicate rows appear. Why?\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "* `OVERWRITE=FALSE` prevents *stage file overwrite*, not duplicate ingestion.\n",
    "* If you re-upload with a **different filename** (`data.csv` vs `data_v2.csv`), Snowflake sees it as new.\n",
    "* `COPY INTO` ingests both ‚Üí duplicates.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Maintain **file tracking table** to record what files have been loaded (`INSERT INTO log_table SELECT metadata$filename`).\n",
    "* Run `COPY INTO ... FILES=()` only for *new files*.\n",
    "* Use deduplication strategy in target (e.g., MERGE with unique keys).\n",
    "\n",
    "---\n",
    "\n",
    "### **10. ‚ö° ‚ÄúHalf-loaded‚Äù files when COPY is interrupted**\n",
    "\n",
    "**Problem:**\n",
    "You stop a running `COPY INTO`, and only part of the file‚Äôs rows exist in the table.\n",
    "\n",
    "**Why it happens:**\n",
    "\n",
    "* Snowflake commits rows in batches; if job is canceled, partial batches stay.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Use `COPY INTO ... ON_ERROR='CONTINUE'` only when safe.\n",
    "* Better: load into a staging table, then run atomic `INSERT INTO target SELECT ...` to ensure consistency.\n",
    "* If issue occurs, truncate staging and reload.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412cc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
