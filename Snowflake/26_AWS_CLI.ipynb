{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd4303d",
   "metadata": {},
   "source": [
    "*You‚Äôve just joined my team, and I need to make sure you‚Äôre not only able to run AWS CLI commands but also understand why we use them, how they connect to Snowflake, and what pitfalls you must avoid.*\n",
    "\n",
    "We‚Äôll go step by step. üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "# 1. **AWS CLI Basics ‚Äì The Foundation**\n",
    "\n",
    "Imagine AWS CLI as your **remote control** for AWS services.\n",
    "Instead of logging in to the AWS Console (the web UI), you can send commands from your terminal and instantly interact with **S3 buckets, EC2, IAM, etc.**\n",
    "\n",
    "For Snowflake, why do we care?\n",
    "üëâ Because **Snowflake integrates heavily with S3** ‚Äì we often load/unload data between Snowflake and S3. If you don‚Äôt know AWS CLI, you‚Äôll be blind when verifying files, moving data, or debugging failures.\n",
    "\n",
    "Example scenario:\n",
    "\n",
    "* You set up a Snowflake `COPY INTO` command to pull data from an S3 bucket.\n",
    "* Your job fails. Snowflake error message says `File not found`.\n",
    "* With AWS CLI, you can instantly check: ‚ÄúDoes the file even exist in the bucket?‚Äù\n",
    "\n",
    "So, CLI is like your **stethoscope as a doctor** when diagnosing S3‚ÄìSnowflake pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. **How to Create AWS CLI Configuration**\n",
    "\n",
    "This is your first step.\n",
    "AWS CLI needs to know **‚ÄúWho are you?‚Äù** and **‚ÄúWhich AWS account should I connect to?‚Äù**\n",
    "\n",
    "### Step 2.1 ‚Äì Install AWS CLI\n",
    "\n",
    "* Windows: Download MSI installer.\n",
    "* Mac/Linux: `brew install awscli` or `sudo apt-get install awscli`.\n",
    "  Verify installation:\n",
    "\n",
    "```bash\n",
    "aws --version\n",
    "```\n",
    "\n",
    "### Step 2.2 ‚Äì Get AWS Access Credentials\n",
    "\n",
    "Now here‚Äôs where students usually get stuck. To use AWS CLI, you need an **Access Key ID** and a **Secret Access Key**.\n",
    "Think of this as your **username + password** for programmatic access.\n",
    "\n",
    "üëâ How do you get them?\n",
    "\n",
    "1. Login to AWS Console (with your company credentials).\n",
    "2. Go to **IAM (Identity & Access Management)**.\n",
    "3. Choose **Users** ‚Üí Select your user account.\n",
    "4. Under **Security Credentials**, create a new **Access Key**.\n",
    "\n",
    "   * AWS gives you two values:\n",
    "\n",
    "     * `AWS_ACCESS_KEY_ID = AKIAIOSFODNN7EXAMPLE`\n",
    "     * `AWS_SECRET_ACCESS_KEY = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY`\n",
    "\n",
    "‚ö†Ô∏è Important:\n",
    "\n",
    "* Treat the secret key like your bank card PIN. If leaked ‚Üí someone can delete your S3 buckets!\n",
    "* Many companies (including top data teams) prefer using **IAM Roles** instead of static keys, but for learning, keys are fine.\n",
    "\n",
    "### Step 2.3 ‚Äì Configure AWS CLI\n",
    "\n",
    "Run:\n",
    "\n",
    "```bash\n",
    "aws configure\n",
    "```\n",
    "\n",
    "It will ask for:\n",
    "\n",
    "```\n",
    "AWS Access Key ID [None]: <paste here>\n",
    "AWS Secret Access Key [None]: <paste here>\n",
    "Default region name [None]: us-east-1\n",
    "Default output format [None]: json\n",
    "```\n",
    "\n",
    "üëâ Region is critical!\n",
    "\n",
    "* If your bucket is in `us-east-1` but you accidentally set `ap-south-1`, you‚Äôll keep getting ‚ÄúBucket not found‚Äù errors.\n",
    "\n",
    "üëâ Output format:\n",
    "\n",
    "* `json` (default)\n",
    "* `table` (pretty printed)\n",
    "* `text` (simplified)\n",
    "\n",
    "Pro tip: I keep mine as `table` for human readability.\n",
    "\n",
    "‚úÖ Now your AWS CLI is ready.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. **Checking List of Files in an S3 Bucket**\n",
    "\n",
    "Let‚Äôs say your Snowflake pipeline needs to read files from `s3://company-data/raw/2025/`.\n",
    "\n",
    "To confirm what‚Äôs inside:\n",
    "\n",
    "```bash\n",
    "aws s3 ls s3://company-data/raw/2025/\n",
    "```\n",
    "\n",
    "Output example:\n",
    "\n",
    "```\n",
    "2025-08-01 12:45:32     145123 sales_data_2025-08-01.csv\n",
    "2025-08-02 14:22:10     167890 sales_data_2025-08-02.csv\n",
    "```\n",
    "\n",
    "* First column = Date uploaded\n",
    "* Second = Time\n",
    "* Third = File size (in bytes)\n",
    "* Fourth = File name\n",
    "\n",
    "üëâ Why is this important for Snowflake?\n",
    "\n",
    "* Before you do `COPY INTO my_table FROM @stage`, you can **verify the files exist** and check if they have the right size.\n",
    "\n",
    "Scenario:\n",
    "Your pipeline fails with ‚ÄúFile empty‚Äù ‚Üí You run `aws s3 ls` and notice the file size is `0`. Boom! Problem solved.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. **How to Copy Files from S3 to Local Folder**\n",
    "\n",
    "Sometimes, as a Data Engineer, you need to quickly download files from S3 to check their raw content.\n",
    "\n",
    "Command:\n",
    "\n",
    "```bash\n",
    "aws s3 cp s3://company-data/raw/2025/sales_data_2025-08-01.csv ./local_folder/\n",
    "```\n",
    "\n",
    "* `s3://company-data/...` = S3 path\n",
    "* `./local_folder/` = Local path\n",
    "\n",
    "üëâ If you want to copy **entire folders**:\n",
    "\n",
    "```bash\n",
    "aws s3 cp s3://company-data/raw/2025/ ./local_folder/ --recursive\n",
    "```\n",
    "\n",
    "Pro Tip:\n",
    "If you don‚Äôt add `--recursive`, only 1 file will copy, not the whole folder.\n",
    "\n",
    "Real-world case:\n",
    "You download a file locally and open it in Excel/Notepad. Suddenly you realize:\n",
    "\n",
    "* The delimiter is `|` not `,`.\n",
    "* That‚Äôs why Snowflake `COPY INTO` failed.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. **How to Upload Files from Local to S3 Bucket**\n",
    "\n",
    "Now imagine you‚Äôre testing a pipeline. You created a dummy CSV file locally and want to push it into S3 so Snowflake can read it.\n",
    "\n",
    "Command:\n",
    "\n",
    "```bash\n",
    "aws s3 cp ./local_folder/my_test_file.csv s3://company-data/raw/2025/\n",
    "```\n",
    "\n",
    "üëâ For multiple files:\n",
    "\n",
    "```bash\n",
    "aws s3 cp ./local_folder/ s3://company-data/raw/2025/ --recursive\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6. **Additional Must-Know AWS CLI for Snowflake**\n",
    "\n",
    "Since you‚Äôll use AWS CLI for **Snowflake integrations**, here are extra commands you must know:\n",
    "\n",
    "### 6.1 ‚Äì Sync local and S3\n",
    "\n",
    "```bash\n",
    "aws s3 sync ./local_folder/ s3://company-data/raw/2025/\n",
    "```\n",
    "\n",
    "* Only copies new/changed files.\n",
    "* Very handy when refreshing test data.\n",
    "\n",
    "### 6.2 ‚Äì Remove file from S3\n",
    "\n",
    "```bash\n",
    "aws s3 rm s3://company-data/raw/2025/sales_data_2025-08-01.csv\n",
    "```\n",
    "\n",
    "### 6.3 ‚Äì Check bucket region\n",
    "\n",
    "```bash\n",
    "aws s3api get-bucket-location --bucket company-data\n",
    "```\n",
    "\n",
    "üëâ Useful when Snowflake gives region mismatch error.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. **Real-Life Story: AWS CLI + Snowflake**\n",
    "\n",
    "You‚Äôre on-call for a production issue:\n",
    "\n",
    "* BI team complains: ‚ÄúToday‚Äôs sales data is missing in Snowflake.‚Äù\n",
    "* You check Snowflake table ‚Üí empty.\n",
    "* You check Snowflake stage ‚Üí no new file.\n",
    "* You run `aws s3 ls s3://company-data/raw/2025/` ‚Üí today‚Äôs file isn‚Äôt there.\n",
    "\n",
    "Turns out, the upstream team forgot to upload the file.\n",
    "You request the file, they give you CSV.\n",
    "You run `aws s3 cp ./sales_data_today.csv s3://company-data/raw/2025/`.\n",
    "Then in Snowflake:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_s3_stage\n",
    "FILE_FORMAT = (TYPE = 'CSV');\n",
    "```\n",
    "\n",
    "‚úÖ Problem solved. AWS CLI saved the day.\n",
    "\n",
    "---\n",
    "\n",
    "# 8. **Must-Know Questions (for you to self-check later)**\n",
    "\n",
    "1. How do you configure AWS CLI for the first time?\n",
    "2. What is the difference between `aws s3 cp` and `aws s3 sync`?\n",
    "3. Why is the `region` important when configuring AWS CLI?\n",
    "4. How can you check if a file exists in an S3 bucket before running a Snowflake `COPY INTO`?\n",
    "5. What‚Äôs the difference between `--recursive` and without it when using `aws s3 cp`?\n",
    "6. How would you debug a Snowflake load failure using AWS CLI?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bbd5d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1. How do you configure AWS CLI for the first time?**\n",
    "\n",
    "You configure AWS CLI with the `aws configure` command.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Install AWS CLI (`aws --version` to verify).\n",
    "2. Generate **Access Key** and **Secret Key** from AWS Console ‚Üí IAM ‚Üí Security Credentials.\n",
    "3. Run:\n",
    "\n",
    "   ```bash\n",
    "   aws configure\n",
    "   ```\n",
    "\n",
    "   It will ask you for:\n",
    "\n",
    "   * **AWS Access Key ID**\n",
    "   * **AWS Secret Access Key**\n",
    "   * **Default Region Name** (e.g., `us-east-1`)\n",
    "   * **Default Output Format** (`json`, `table`, or `text`)\n",
    "\n",
    "üëâ Example:\n",
    "\n",
    "```\n",
    "AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\n",
    "AWS Secret Access Key [None]: abcdefghijklmnopqrstuvwx123456789\n",
    "Default region name [None]: us-east-1\n",
    "Default output format [None]: table\n",
    "```\n",
    "\n",
    "Now CLI is connected to your AWS account.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What is the difference between `aws s3 cp` and `aws s3 sync`?**\n",
    "\n",
    "* **`cp` (copy)** ‚Üí Copies files **one by one** (or multiple with `--recursive`).\n",
    "  Example:\n",
    "\n",
    "  ```bash\n",
    "  aws s3 cp ./file.csv s3://company-data/raw/\n",
    "  ```\n",
    "\n",
    "  üëâ Always copies even if the file already exists in S3.\n",
    "\n",
    "* **`sync` (synchronize)** ‚Üí Makes two locations identical by copying only new or changed files.\n",
    "  Example:\n",
    "\n",
    "  ```bash\n",
    "  aws s3 sync ./local_folder/ s3://company-data/raw/\n",
    "  ```\n",
    "\n",
    "  üëâ Faster and efficient when you have thousands of files.\n",
    "  üëâ Useful in pipelines when you need to keep local + S3 aligned.\n",
    "\n",
    "üîë Think of `cp` as \"manual copy\", while `sync` is \"keep both folders up to date\".\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Why is the `region` important when configuring AWS CLI?**\n",
    "\n",
    "Because **S3 buckets are region-specific**.\n",
    "\n",
    "* If your bucket is in `us-east-1` but your CLI is set to `ap-south-1`, you‚Äôll see errors like:\n",
    "\n",
    "  ```\n",
    "  An error occurred (PermanentRedirect) when calling the ListObjectsV2 operation:\n",
    "  The bucket you are attempting to access must be addressed using the specified region.\n",
    "  ```\n",
    "* Snowflake stages also require you to specify the correct S3 region.\n",
    "\n",
    "üëâ Example:\n",
    "If your company S3 bucket is in `eu-west-1`, but you set `us-east-1`, Snowflake will not be able to read/write files properly.\n",
    "\n",
    "So always **match CLI region with bucket region**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. How can you check if a file exists in an S3 bucket before running a Snowflake `COPY INTO`?**\n",
    "\n",
    "You use `aws s3 ls` to check.\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "aws s3 ls s3://company-data/raw/2025/sales_data_2025-08-01.csv\n",
    "```\n",
    "\n",
    "If the file exists ‚Üí You‚Äôll see file details (date, size, name).\n",
    "If not ‚Üí No output.\n",
    "\n",
    "üëâ Why this matters?\n",
    "Before you run:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table FROM @my_s3_stage;\n",
    "```\n",
    "\n",
    "You want to confirm the file is really there in S3, otherwise your Snowflake query will fail.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. What‚Äôs the difference between `--recursive` and without it when using `aws s3 cp`?**\n",
    "\n",
    "* Without `--recursive`:\n",
    "\n",
    "  ```bash\n",
    "  aws s3 cp s3://company-data/raw/2025/ ./local_folder/\n",
    "  ```\n",
    "\n",
    "  üëâ Only one file (if you specify it) is copied.\n",
    "\n",
    "* With `--recursive`:\n",
    "\n",
    "  ```bash\n",
    "  aws s3 cp s3://company-data/raw/2025/ ./local_folder/ --recursive\n",
    "  ```\n",
    "\n",
    "  üëâ Copies **all files and subfolders** under that prefix.\n",
    "\n",
    "Real case:\n",
    "\n",
    "* You need all daily CSVs in `/2025/`. Without `--recursive`, you‚Äôll only get one file.\n",
    "* With `--recursive`, you‚Äôll get the whole folder at once.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. How would you debug a Snowflake load failure using AWS CLI?**\n",
    "\n",
    "Let‚Äôs say your Snowflake load command fails:\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_s3_stage FILE_FORMAT = (TYPE = 'CSV');\n",
    "```\n",
    "\n",
    "Steps to debug with AWS CLI:\n",
    "\n",
    "1. **Check if file exists in bucket**\n",
    "\n",
    "   ```bash\n",
    "   aws s3 ls s3://company-data/raw/2025/\n",
    "   ```\n",
    "\n",
    "   * If missing ‚Üí upstream team didn‚Äôt upload.\n",
    "\n",
    "2. **Check file size**\n",
    "\n",
    "   * If `0` bytes ‚Üí empty file, Snowflake won‚Äôt load.\n",
    "\n",
    "3. **Download and inspect file locally**\n",
    "\n",
    "   ```bash\n",
    "   aws s3 cp s3://company-data/raw/2025/sales_data_2025-08-01.csv .\n",
    "   cat sales_data_2025-08-01.csv | head -5\n",
    "   ```\n",
    "\n",
    "   * Maybe wrong delimiter (`|` instead of `,`) or header issues.\n",
    "\n",
    "4. **Check bucket region**\n",
    "\n",
    "   ```bash\n",
    "   aws s3api get-bucket-location --bucket company-data\n",
    "   ```\n",
    "\n",
    "   * If region doesn‚Äôt match Snowflake external stage ‚Üí you must fix stage definition.\n",
    "\n",
    "üëâ By combining these checks, you can quickly identify why Snowflake is failing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd09f39",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîë **Core AWS CLI Commands You Must Know (Grouped by Service)**\n",
    "\n",
    "### **1. General Utility**\n",
    "\n",
    "1. `aws configure` ‚Äì Configure credentials (access key, secret key, region, output).\n",
    "2. `aws configure list` ‚Äì See which credentials/region CLI is currently using.\n",
    "3. `aws sts get-caller-identity` ‚Äì Shows *who you are* (AWS account ID, user/role). Helps debug credential issues.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. S3 (Snowflake‚Äôs Best Friend)**\n",
    "\n",
    "üëâ These are **critical** since Snowflake loads/unloads data from S3.\n",
    "\n",
    "4. `aws s3 ls` ‚Äì List buckets or files inside a bucket.\n",
    "5. `aws s3 cp` ‚Äì Copy files between local ‚Üî S3 or S3 ‚Üî S3.\n",
    "6. `aws s3 mv` ‚Äì Move/rename files in S3 (removes from source).\n",
    "7. `aws s3 rm` ‚Äì Delete files in S3.\n",
    "8. `aws s3 sync` ‚Äì Synchronize folders between local and S3.\n",
    "9. `aws s3 presign` ‚Äì Generate a **presigned URL** for temporary file access (Snowflake can use this sometimes).\n",
    "10. `aws s3api get-bucket-location` ‚Äì Check which region the bucket is in.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. IAM (Security & Access Control)**\n",
    "\n",
    "üëâ Snowflake often uses **IAM Roles** or **IAM Users** for S3 integration.\n",
    "\n",
    "11. `aws iam list-users` ‚Äì List IAM users.\n",
    "12. `aws iam list-roles` ‚Äì List IAM roles.\n",
    "13. `aws iam get-user` ‚Äì See details of your IAM user.\n",
    "14. `aws iam attach-user-policy` ‚Äì Attach a policy to a user (like S3 read-only).\n",
    "15. `aws iam create-access-key` ‚Äì Generate access/secret key for a user.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. STS (Temporary Security Tokens)**\n",
    "\n",
    "üëâ Many companies don‚Äôt give static keys, instead they use **STS tokens** for security.\n",
    "\n",
    "16. `aws sts assume-role` ‚Äì Assume a role and get temporary credentials.\n",
    "17. `aws sts get-session-token` ‚Äì Get a temporary session (for MFA-enabled accounts).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. CloudWatch (Logs & Monitoring)**\n",
    "\n",
    "üëâ Snowflake won‚Äôt directly use this, but as a data engineer you may debug upstream AWS jobs here.\n",
    "\n",
    "18. `aws cloudwatch list-metrics` ‚Äì List available metrics.\n",
    "19. `aws cloudwatch get-metric-data` ‚Äì Retrieve metric data (like S3 request counts).\n",
    "20. `aws logs tail` ‚Äì Tail log groups in real time.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. EC2 (Infrastructure Basics)**\n",
    "\n",
    "üëâ Even if you‚Äôre Snowflake-focused, you‚Äôll often check EC2 because upstream/downstream apps may be running on it.\n",
    "\n",
    "21. `aws ec2 describe-instances` ‚Äì Show EC2 instances details.\n",
    "22. `aws ec2 start-instances` ‚Äì Start EC2 instance.\n",
    "23. `aws ec2 stop-instances` ‚Äì Stop EC2 instance.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Miscellaneous (Often Needed in Data Engineering)**\n",
    "\n",
    "24. `aws kms list-keys` ‚Äì List encryption keys (important for S3 + Snowflake when using SSE-KMS).\n",
    "25. `aws secretsmanager get-secret-value` ‚Äì Retrieve credentials/secrets (some Snowflake connectors store DB passwords here).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Why These Matter for Snowflake Engineers\n",
    "\n",
    "* **S3 commands (4‚Äì10)** ‚Üí Daily life when loading/unloading data to Snowflake.\n",
    "* **IAM commands (11‚Äì15)** ‚Üí You‚Äôll configure Snowflake external stages using IAM Roles/Users.\n",
    "* **STS commands (16‚Äì17)** ‚Üí Many companies enforce temporary tokens for Snowflake pipelines.\n",
    "* **CloudWatch & EC2 (18‚Äì23)** ‚Üí When debugging why data didn‚Äôt reach Snowflake (e.g., logs from ETL jobs).\n",
    "* **KMS & Secrets Manager (24‚Äì25)** ‚Üí For handling encrypted S3 buckets and managing DB credentials securely.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec15a34",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **When is it important to use AWS CLI instead of the web console?**\n",
    "\n",
    "Think of AWS CLI as the **toolbelt** of a data engineer, while the web console is like a **manual screwdriver**. Both can get the job done, but sometimes the CLI is the only efficient way.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Automation and Repeatable Processes**\n",
    "\n",
    "* **Scenario:** You have a Snowflake pipeline that loads **thousands of daily CSV files** into an external stage in S3.\n",
    "* If you use the web console: You‚Äôd have to manually upload each file or folder.\n",
    "* With CLI:\n",
    "\n",
    "```bash\n",
    "aws s3 sync ./daily_csv/ s3://company-data/raw/2025/\n",
    "```\n",
    "\n",
    "* ‚úÖ Outcome: All files uploaded automatically. You can schedule it with cron/airflow.\n",
    "\n",
    "**Key takeaway:** Whenever tasks are repetitive or scheduled, **CLI is essential**. Manual console clicks are error-prone and slow.\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Working in Remote or Headless Environments**\n",
    "\n",
    "* **Scenario:** Your Snowflake ETL server runs in AWS EC2 (Linux) and has no GUI.\n",
    "* To debug missing files in S3, you **cannot use a web browser**.\n",
    "* With CLI:\n",
    "\n",
    "```bash\n",
    "aws s3 ls s3://company-data/raw/2025/\n",
    "```\n",
    "\n",
    "* ‚úÖ Outcome: You can instantly see all files, sizes, and timestamps from the terminal.\n",
    "\n",
    "**Key takeaway:** CLI is critical for servers, automation scripts, or when GUI isn‚Äôt available.\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Large-Scale File Operations**\n",
    "\n",
    "* **Scenario:** Your company ingests **millions of rows per day**, spread across **hundreds of CSV files** in S3.\n",
    "* Web console: Uploading, moving, or deleting hundreds of files manually is impossible.\n",
    "* CLI:\n",
    "\n",
    "```bash\n",
    "aws s3 rm s3://company-data/raw/2025/ --recursive\n",
    "aws s3 cp ./new_files/ s3://company-data/raw/2025/ --recursive\n",
    "```\n",
    "\n",
    "* ‚úÖ Outcome: Bulk operations done in seconds.\n",
    "\n",
    "**Key takeaway:** CLI is **much faster and reliable** for large-scale data movement, which is common in Snowflake pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Debugging and Diagnostics**\n",
    "\n",
    "* **Scenario:** A Snowflake `COPY INTO` command fails with `access denied`.\n",
    "* Web console might let you check policies manually, but CLI gives precise answers:\n",
    "\n",
    "```bash\n",
    "aws s3api get-bucket-policy --bucket company-data\n",
    "aws iam simulate-principal-policy --policy-source-arn arn:aws:iam::123456789012:role/SnowflakeRole --action-names s3:GetObject\n",
    "```\n",
    "\n",
    "* ‚úÖ Outcome: You can see exactly **what permissions exist** and whether the Snowflake role can read/write files.\n",
    "\n",
    "**Key takeaway:** CLI lets you **inspect, debug, and validate permissions programmatically**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ Integration with Snowflake Scripts and Pipelines**\n",
    "\n",
    "* **Scenario:** You‚Äôre writing an Airflow DAG or a Python ETL script that loads files into Snowflake external stages.\n",
    "* CLI commands can be integrated directly:\n",
    "\n",
    "```bash\n",
    "os.system(\"aws s3 cp ./data.csv s3://company-data/raw/2025/\")\n",
    "```\n",
    "\n",
    "* ‚úÖ Outcome: Fully automated Snowflake pipeline without human intervention.\n",
    "\n",
    "**Key takeaway:** CLI is **mandatory for DevOps and CI/CD workflows**.\n",
    "\n",
    "---\n",
    "\n",
    "## **6Ô∏è‚É£ Security & Temporary Access**\n",
    "\n",
    "* **Scenario:** Your company uses **temporary STS credentials** for Snowflake external stages (no permanent keys).\n",
    "* Web console doesn‚Äôt let you assume temporary credentials programmatically.\n",
    "* CLI allows:\n",
    "\n",
    "```bash\n",
    "aws sts assume-role --role-arn arn:aws:iam::123456789012:role/SnowflakeRole --role-session-name SnowflakeSession\n",
    "```\n",
    "\n",
    "* ‚úÖ Outcome: You can use temporary credentials to access S3 from scripts safely.\n",
    "\n",
    "**Key takeaway:** CLI is essential when **security policies require ephemeral credentials**.\n",
    "\n",
    "---\n",
    "\n",
    "## **7Ô∏è‚É£ Versioning, Metadata, and Detailed Inspection**\n",
    "\n",
    "* **Scenario:** A Snowflake load failed because the wrong file version was uploaded.\n",
    "* CLI allows inspecting metadata:\n",
    "\n",
    "```bash\n",
    "aws s3api head-object --bucket company-data --key raw/2025/sales_data.csv\n",
    "```\n",
    "\n",
    "* ‚úÖ Outcome: Check **file size, last modified date, encryption** ‚Üí fix the pipeline.\n",
    "\n",
    "**Key takeaway:** CLI provides **fine-grained inspection** that web console often doesn‚Äôt show easily.\n",
    "\n",
    "---\n",
    "\n",
    "# **‚úÖ Summary Table: When CLI is Better**\n",
    "\n",
    "| Use Case                    | Why CLI is Preferred                                     |\n",
    "| --------------------------- | -------------------------------------------------------- |\n",
    "| Automation & Scheduling     | Repeatable tasks without manual clicks                   |\n",
    "| Remote servers / headless   | Works without GUI                                        |\n",
    "| Large-scale file operations | Faster for bulk upload/download/move/delete              |\n",
    "| Debugging / Permissions     | Inspect policies, simulate access, check bucket metadata |\n",
    "| Integration with pipelines  | Embeddable in scripts/DAGs for CI/CD                     |\n",
    "| Temporary credentials       | STS and ephemeral access keys work only via CLI          |\n",
    "| Detailed file inspection    | File metadata, size, last modified, encryption           |\n",
    "\n",
    "---\n",
    "\n",
    "**Story-Based Perspective:**\n",
    "\n",
    "Imagine it‚Äôs **3 AM** and a Snowflake pipeline failed. The web console is slow and requires a VPN. Using the CLI, you:\n",
    "\n",
    "1. Check which files are missing: `aws s3 ls ‚Ä¶`\n",
    "2. Confirm file size & encryption: `aws s3api head-object ‚Ä¶`\n",
    "3. Upload fixed files: `aws s3 cp ‚Ä¶`\n",
    "4. Test IAM permissions: `aws iam simulate-principal-policy ‚Ä¶`\n",
    "\n",
    "Within **10 minutes**, the pipeline is back online ‚Äî something you could never do through a web browser at that speed.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Bottom Line:**\n",
    "You use **CLI over web console** whenever you need **automation, speed, debugging, large-scale operations, or programmatic access**, which is almost daily in Snowflake pipelines.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27db6a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **1Ô∏è‚É£ Copying Through Staging (Snowflake Stage)**\n",
    "\n",
    "A **stage** in Snowflake is like a **temporary storage area** for files before loading into tables. You can use **internal stages** (inside Snowflake) or **external stages** (like S3).\n",
    "\n",
    "---\n",
    "\n",
    "### **Scenario:**\n",
    "\n",
    "You receive multiple daily CSV files in S3 and want to load them into a Snowflake table.\n",
    "\n",
    "### **Step 1 ‚Äì Create a Stage**\n",
    "\n",
    "```sql\n",
    "-- Internal stage\n",
    "CREATE OR REPLACE STAGE my_stage;\n",
    "\n",
    "-- OR External stage (S3)\n",
    "CREATE OR REPLACE STAGE my_s3_stage\n",
    "URL='s3://company-data/raw/2025/'\n",
    "STORAGE_INTEGRATION = my_s3_integration;\n",
    "```\n",
    "\n",
    "* **Internal stage** ‚Üí Snowflake manages storage.\n",
    "* **External stage** ‚Üí Snowflake reads files directly from S3 using an **integration**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2 ‚Äì Copy Files into Stage**\n",
    "\n",
    "If internal:\n",
    "\n",
    "```sql\n",
    "PUT file://C:\\local\\sales_data_2025.csv @my_stage;\n",
    "```\n",
    "\n",
    "If external ‚Üí Stage already points to S3, so you don‚Äôt need to `PUT`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3 ‚Äì Copy into Table**\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_stage\n",
    "FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"');\n",
    "```\n",
    "\n",
    "‚úÖ Advantages:\n",
    "\n",
    "* You can **inspect files** before loading.\n",
    "* Works with **multiple files**.\n",
    "* Supports **error handling** (`ON_ERROR = CONTINUE/ABORT_STATEMENT/ SKIP_FILE`).\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "\n",
    "* Extra step ‚Üí staging can increase latency.\n",
    "* Slightly more storage cost if using internal stage.\n",
    "\n",
    "---\n",
    "\n",
    "# **2Ô∏è‚É£ Direct Copy from S3 to Table**\n",
    "\n",
    "Snowflake also supports **direct copy from external stage** (S3 bucket) **without first uploading to internal stage**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Query Example:**\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM 's3://company-data/raw/2025/'\n",
    "STORAGE_INTEGRATION = my_s3_integration\n",
    "FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"')\n",
    "ON_ERROR = 'CONTINUE';\n",
    "```\n",
    "\n",
    "* Snowflake reads **directly from S3**.\n",
    "* Saves time because you **skip the PUT to internal stage**.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "\n",
    "* Faster for **large datasets**.\n",
    "* No extra internal storage cost.\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "\n",
    "* Harder to **pre-check file content**.\n",
    "* Error handling may require **re-downloading/uploading files** to fix issues.\n",
    "\n",
    "---\n",
    "\n",
    "# **3Ô∏è‚É£ Direct Unload (Snowflake ‚Üí S3)**\n",
    "\n",
    "Direct unload is used to **export table/query results from Snowflake to S3**, optionally compressed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Query Example:**\n",
    "\n",
    "```sql\n",
    "COPY INTO 's3://company-data/unload/2025/'\n",
    "FROM sales_table\n",
    "STORAGE_INTEGRATION = my_s3_integration\n",
    "FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"' COMPRESSION = GZIP)\n",
    "MAX_FILE_SIZE = 50000000;\n",
    "```\n",
    "\n",
    "* **COMPRESSION** ‚Üí GZIP, BZIP2, etc.\n",
    "* **MAX\\_FILE\\_SIZE** ‚Üí Control chunk size of files.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "\n",
    "* Efficient export to S3.\n",
    "* Supports **parallel unload**.\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "\n",
    "* If your query returns millions of rows, **many small files** may be generated ‚Üí requires later consolidation.\n",
    "* Only works **from Snowflake ‚Üí S3**, cannot push directly to local.\n",
    "\n",
    "---\n",
    "\n",
    "# **4Ô∏è‚É£ Zip Compression Format in Snowflake**\n",
    "\n",
    "* Snowflake **cannot load zip files** natively if they contain **multiple files**.\n",
    "* It can load **gzip (single file) or compressed files** directly.\n",
    "\n",
    "### **Important Notes:**\n",
    "\n",
    "| Compression    | Snowflake Support | Notes                      |\n",
    "| -------------- | ----------------- | -------------------------- |\n",
    "| GZIP (.gz)     | ‚úÖ Yes             | Single-file only           |\n",
    "| BZIP2 (.bz2)   | ‚úÖ Yes             | Single-file only           |\n",
    "| ZIP (.zip)     | ‚ùå Multiple files  | Only if single file inside |\n",
    "| Internal Stage | ‚úÖ                 | Can use PUT + staged file  |\n",
    "\n",
    "---\n",
    "\n",
    "# **5Ô∏è‚É£ How to Copy Zip Files from S3 to Snowflake**\n",
    "\n",
    "### **Scenario:** Single compressed CSV `.zip` in S3.\n",
    "\n",
    "```sql\n",
    "COPY INTO sales_table\n",
    "FROM @my_s3_stage\n",
    "FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = ZIP);\n",
    "```\n",
    "\n",
    "* `COMPRESSION = ZIP` tells Snowflake it‚Äôs compressed.\n",
    "* Works **only if ZIP has one CSV file inside**.\n",
    "\n",
    "---\n",
    "\n",
    "### **What happens with multiple files inside a ZIP?**\n",
    "\n",
    "* Snowflake **cannot read** multiple files inside a single ZIP.\n",
    "* Attempting this will result in:\n",
    "\n",
    "```\n",
    "Error: ZIP archive contains multiple files.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What happens if ZIP has a single file?**\n",
    "\n",
    "* ‚úÖ Snowflake reads it without problem.\n",
    "* Data loads normally.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to solve the multiple-file ZIP problem?**\n",
    "\n",
    "1. **Unzip before uploading:**\n",
    "\n",
    "   * Using AWS CLI or local unzip:\n",
    "\n",
    "```bash\n",
    "unzip my_data.zip -d ./unzipped_files/\n",
    "aws s3 cp ./unzipped_files/ s3://company-data/raw/2025/ --recursive\n",
    "```\n",
    "\n",
    "2. **Split ZIP into single-file archives** (if needed for automation).\n",
    "\n",
    "3. **Use internal staging + PUT** ‚Üí you can manage files before COPY.\n",
    "\n",
    "---\n",
    "\n",
    "# **6Ô∏è‚É£ Summary Table ‚Äì Copy Types & Notes**\n",
    "\n",
    "| Type          | Example                       | Pros                          | Cons                                  | Notes                               |\n",
    "| ------------- | ----------------------------- | ----------------------------- | ------------------------------------- | ----------------------------------- |\n",
    "| Through Stage | `COPY INTO table FROM @stage` | Inspect files, error handling | Extra step, storage cost              | PUT optional if internal stage      |\n",
    "| Direct Copy   | `COPY INTO table FROM S3`     | Fast, no extra storage        | Hard to pre-check files               | Ideal for large datasets            |\n",
    "| Direct Unload | `COPY INTO S3 FROM table`     | Parallel export, compressed   | Many small files, only Snowflake ‚Üí S3 | Use COMPRESSION and MAX\\_FILE\\_SIZE |\n",
    "\n",
    "---\n",
    "\n",
    "### **Story Perspective**\n",
    "\n",
    "Imagine a Snowflake pipeline:\n",
    "\n",
    "* Morning: Files arrive in **S3 ZIP archive** with **multiple CSVs inside**.\n",
    "* Direct copy fails ‚Üí Snowflake can‚Äôt read multiple files in ZIP.\n",
    "* Solution:\n",
    "\n",
    "  1. Unzip locally or in staging ‚Üí single files.\n",
    "  2. Upload to S3 ‚Üí use `COPY INTO` table.\n",
    "* End result: Table updated, error-free, ready for BI dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Takeaways:**\n",
    "\n",
    "1. Use **staging** when you want control, inspection, and error handling.\n",
    "2. Use **direct copy** when files are large and you want speed.\n",
    "3. **Direct unload** is great for pushing data back to S3 efficiently.\n",
    "4. Snowflake supports **ZIP only if single file inside**; multiple files ‚Üí unzip first.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d8770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
