{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb474685",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 1) Quick glossary (one-liner)\n",
    "\n",
    "* **Stage** = pointer (URL + credentials) to file(s) in S3 (or Snowflake internal storage). You can `LIST` and query the files directly. ([Snowflake Documentation][1])\n",
    "* **File format** = reusable object that describes CSV/JSON/PARQUET layout. ([Snowflake Documentation][2])\n",
    "* **Storage Integration** = Snowflake object that stores the managed identity / allowed locations used to access S3 (and that ties into the AWS IAM role). Stage can reference a storage integration. ([Snowflake Documentation][3])\n",
    "* **External table** = a Snowflake object that maps files in an external stage to columns, keeps metadata, and can be refreshed automatically (closer to a real table but data remains in S3). ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Complete demo: from file → query (replace placeholders with your values)\n",
    "\n",
    "### Assumptions / sample files\n",
    "\n",
    "S3 bucket: `s3://retailx-raw`\n",
    "Files under `orders/` with csv like `orders_20250828.csv` containing:\n",
    "\n",
    "```\n",
    "order_id,customer_id,created_at,total_usd\n",
    "1001,c_001,2025-08-28 10:01:02, 199.50\n",
    "1002,c_002,2025-08-28 10:05:34, 9.99\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step A — create a File Format (CSV)\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT retailx_csv_fmt\n",
    "  TYPE = CSV\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  SKIP_HEADER = 1\n",
    "  FIELD_DELIMITER = ','\n",
    "  TRIM_SPACE = TRUE\n",
    "  NULL_IF = ('', 'NULL', 'null')\n",
    "  COMPRESSION = 'AUTO';\n",
    "```\n",
    "\n",
    "(You can create JSON / PARQUET formats similarly.) ([Snowflake Documentation][2])\n",
    "\n",
    "---\n",
    "\n",
    "### Step B — (you already may have) create a Storage Integration\n",
    "\n",
    "> If you already created an integration & AWS role earlier, skip to Step C.\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STORAGE INTEGRATION retailx_s3_int\n",
    "  TYPE = EXTERNAL_STAGE\n",
    "  STORAGE_PROVIDER = 'S3'\n",
    "  ENABLED = TRUE\n",
    "  STORAGE_ALLOWED_LOCATIONS = ('s3://retailx-raw/');\n",
    "```\n",
    "\n",
    "After create, run:\n",
    "\n",
    "```sql\n",
    "DESC INTEGRATION retailx_s3_int;\n",
    "-- copy STORAGE_AWS_IAM_USER_ARN and STORAGE_AWS_EXTERNAL_ID and follow Snowflake/AWS steps to create AWS role/trust policy\n",
    "```\n",
    "\n",
    "(You need to finish the AWS side: create IAM role, trust the Snowflake ARN and external id, attach a least-privilege S3 policy — see the Snowflake storage integration docs.) ([Snowflake Documentation][5])\n",
    "\n",
    "---\n",
    "\n",
    "### Step C — create an external stage that uses the storage integration and the file format\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE retailx_orders_stage\n",
    "  URL = 's3://retailx-raw/orders/'\n",
    "  STORAGE_INTEGRATION = retailx_s3_int\n",
    "  FILE_FORMAT = retailx_csv_fmt;\n",
    "```\n",
    "\n",
    "(You can also omit `FILE_FORMAT` here and pass it when querying.) ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "### Step D — sanity checks\n",
    "\n",
    "```sql\n",
    "-- show files\n",
    "LIST @retailx_orders_stage;\n",
    "```\n",
    "\n",
    "If `LIST` works, you have connectivity. If `ACCESS_DENIED`, check IAM role trust policy, `STORAGE_ALLOWED_LOCATIONS` and the S3 bucket policy/role policy.\n",
    "\n",
    "---\n",
    "\n",
    "### Step E — ad-hoc query of CSV files in the stage (no COPY)\n",
    "\n",
    "When you query a staged CSV directly, Snowflake exposes column positions as `$1`, `$2`, ... — then you can cast/alias them:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  TRY_CAST(t.$1 AS INTEGER)        AS order_id,\n",
    "  t.$2::STRING                    AS customer_id,\n",
    "  TRY_TO_TIMESTAMP(t.$3, 'YYYY-MM-DD HH24:MI:SS') AS created_at,\n",
    "  TRY_CAST(t.$4 AS NUMBER(10,2))  AS total_usd\n",
    "FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt') t\n",
    "WHERE TRY_CAST(t.$4 AS NUMBER) > 50;\n",
    "```\n",
    "\n",
    "Notes:\n",
    "\n",
    "* You can query the whole stage, a path (`@stage/subpath/`), or a single file (`@stage/file.csv.gz`). Querying is *on-the-fly* parsing of files. ([Snowflake Documentation][6])\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Filters, joins, views, tables — examples\n",
    "\n",
    "### 3A — Filtering while reading files\n",
    "\n",
    "You saw `WHERE TRY_CAST(t.$4 AS NUMBER) > 50` above — predicate filters are allowed, and Snowflake will only scan files it needs if you restrict the stage path or use an external table with partitions. But note: when reading raw files directly from a stage, Snowflake usually scans files on the fly (less efficient than table pruning). ([Snowflake Documentation][4], [Stack Overflow][7])\n",
    "\n",
    "### 3B — Join the staged data with an internal table (works fine)\n",
    "\n",
    "```sql\n",
    "-- internal customer master table\n",
    "CREATE OR REPLACE TABLE retailx_customers (customer_id STRING, customer_name STRING);\n",
    "\n",
    "-- join ad-hoc stage data with internal table\n",
    "SELECT s.order_id, s.customer_id, c.customer_name, s.total_usd\n",
    "FROM (\n",
    "  SELECT\n",
    "    TRY_CAST(t.$1 AS INTEGER) AS order_id,\n",
    "    t.$2::STRING             AS customer_id,\n",
    "    TRY_CAST(t.$4 AS NUMBER(10,2)) AS total_usd\n",
    "  FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt') t\n",
    ") s\n",
    "JOIN retailx_customers c ON s.customer_id = c.customer_id\n",
    "WHERE s.total_usd > 100;\n",
    "```\n",
    "\n",
    "Yes — you can join staged data with tables. The staged side is parsed on the fly (positional columns), the table side benefits from Snowflake table optimizations.\n",
    "\n",
    "### 3C — Create a view that wraps the stage query\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE VIEW v_orders_stage AS\n",
    "SELECT\n",
    "  TRY_CAST(t.$1 AS INTEGER) AS order_id,\n",
    "  t.$2::STRING               AS customer_id,\n",
    "  TRY_TO_TIMESTAMP(t.$3,'YYYY-MM-DD HH24:MI:SS') AS created_at,\n",
    "  TRY_CAST(t.$4 AS NUMBER(10,2)) AS total_usd\n",
    "FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt') t;\n",
    "```\n",
    "\n",
    "A normal `VIEW` over a stage is allowed (view stores the query). When you query the view, files are re-read and re-parsed at query time. *You cannot create a materialized view on a stage query.* ([Snowflake Documentation][8], [Medium][9])\n",
    "\n",
    "### 3D — Create a table (persist/load the data into Snowflake)\n",
    "\n",
    "If you want the performance and Snowflake features, load into a table:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE retailx_orders AS\n",
    "SELECT\n",
    "  TRY_CAST(t.$1 AS INTEGER) AS order_id,\n",
    "  t.$2::STRING              AS customer_id,\n",
    "  TRY_TO_TIMESTAMP(t.$3,'YYYY-MM-DD HH24:MI:SS') AS created_at,\n",
    "  TRY_CAST(t.$4 AS NUMBER(10,2)) AS total_usd\n",
    "FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt') t;\n",
    "```\n",
    "\n",
    "Now data lives in Snowflake storage, will be micro-partitioned, and queries will typically be far faster and cheaper for repeated workloads. ([Snowflake Documentation][10])\n",
    "\n",
    "---\n",
    "\n",
    "# 4) External table (the “nice middle ground”)\n",
    "\n",
    "External tables create metadata in Snowflake and let you define named columns (mapping into `VALUE:c1`, `VALUE:c2`, etc.), and support automatic refresh (SNS/SQS) for S3 to pick up new files. They are useful when you need table-like access without copying data into Snowflake. Example:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE EXTERNAL TABLE retailx_orders_ext (\n",
    "  order_id    varchar as (value:c1::varchar),\n",
    "  customer_id varchar as (value:c2::varchar),\n",
    "  created_at  timestamp_ntz as (value:c3::timestamp_ntz),\n",
    "  total_usd   number(10,2) as (value:c4::number)\n",
    ")\n",
    "WITH LOCATION = @retailx_orders_stage\n",
    "FILE_FORMAT = (FORMAT_NAME = 'retailx_csv_fmt')\n",
    "AUTO_REFRESH = TRUE\n",
    "REFRESH_ON_CREATE = TRUE;\n",
    "```\n",
    "\n",
    "Key points:\n",
    "\n",
    "* External tables can be partitioned and their metadata refreshed automatically (SNS/SQS) to register new files. ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Differences vs “traditional” Snowflake table queries (detailed)\n",
    "\n",
    "### What internal Snowflake tables give you (advantages):\n",
    "\n",
    "* **Micro-partitions & pruning** (very fast scanning / min/max metadata) — Snowflake stores column min/max and can prune partitions. This massively reduces scan cost for selective filters. ([Snowflake Documentation][10])\n",
    "* **Clustering, time travel, cloning, DML, materialized views on tables** — advanced features you don’t get on stage reads or (some of) external tables. ([Snowflake Documentation][4])\n",
    "* **Faster repeated queries via micro-partition caching** and more cost-efficient compute for analytical workloads. ([Teej][11])\n",
    "\n",
    "### What stage / external reads **lack** or are weaker at:\n",
    "\n",
    "* **No micro-partition pruning** because files are not loaded into Snowflake’s micro-partition storage — queries must re-scan/parsing files (slower for heavy analytic workloads). ([Stack Overflow][7])\n",
    "* **Less pushdown & metadata**: stage queries return positional columns (\\$1,\\$2) unless you wrap them or use external tables to define columns. External tables can add metadata but still won’t have micro-partitions. ([Snowflake Documentation][6])\n",
    "* **No materialized views directly on stage queries** (external tables can have materialized views in some cases, but there are caveats). ([Medium][9])\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Benefits of keeping data in S3 & querying from Snowflake\n",
    "\n",
    "* **Storage cost savings** (S3 can be cheaper than Snowflake long-term storage for cold/raw data). ([phData][12])\n",
    "* **Single source of truth / interoperability** — other systems can read the same S3 files.\n",
    "* **Fast exploration & QA** — you can sample raw files without duplicating them into Snowflake.\n",
    "* **Immediate availability** for new raw files if using external tables + auto refresh or Snowpipe. ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Disadvantages / costs / caveats\n",
    "\n",
    "* **Query performance**: external queries are generally slower and can be more compute-heavy compared to native Snowflake tables. ([Stack Overflow][7])\n",
    "* **Limited Snowflake features**: no micro-partitioning, no time travel, no cloning, limited materialized view options, some DDL/DML limitations. ([Snowflake Documentation][4])\n",
    "* **Metadata maintenance**: external tables require refreshes (manual or event-driven) to reflect new/removed files; auto-refresh costs (Snowpipe charges) can appear. ([Snowflake Documentation][4])\n",
    "* **Potential egress costs**: if bucket and Snowflake are in *different* regions, you may pay cross-region data transfer. (Keep them same region where possible.) ([Medium][13])\n",
    "\n",
    "---\n",
    "\n",
    "# 8) When to use each option (rules of thumb)\n",
    "\n",
    "* **Query stage / external table**\n",
    "\n",
    "  * Use for: exploration, profiling raw data, QA, cross-platform sharing, and for rarely-queried historical datasets where you want to avoid copying.\n",
    "  * Use external table (not ad-hoc stage select) when you want table-like convenience (named columns, refresh) without loading. ([Snowflake Documentation][4])\n",
    "* **Load into internal Snowflake table**\n",
    "\n",
    "  * Use for: production analytics, frequent queries, joins/aggregations at scale, low latency dashboards, or any workload that benefits from micro-partitions, clustering, and Snowflake optimizations. ([Snowflake Documentation][10])\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Can I query *only* the S3 files I added to the Integration?\n",
    "\n",
    "Short answer: **No single magic permission in the integration alone controls the exact files — three things must align**:\n",
    "\n",
    "1. **Stage URL / external table LOCATION** — the stage or external table points to a specific bucket/path (that's your first limit). You can only query files under that stage path. ([Snowflake Documentation][4])\n",
    "2. **Storage Integration `STORAGE_ALLOWED_LOCATIONS`** — on the Snowflake side you can restrict allowed S3 URIs for that integration; stage URLs must align to those locations. ([Snowflake Documentation][3])\n",
    "3. **AWS IAM role / bucket policy** — AWS must grant the Snowflake role `s3:ListBucket` / `s3:GetObject` for the specific bucket & prefixes. If the IAM policy does not allow a prefix, Snowflake cannot read it even if the integration lists it. ([Snowflake Documentation][5])\n",
    "\n",
    "So: **you can only successfully query files that are (A) in the stage path, (B) allowed by STORAGE\\_ALLOWED\\_LOCATIONS, and (C) allowed by the AWS role/bucket policy**. All three must allow access.\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Short checklist / action items (practical)\n",
    "\n",
    "* For quick inspection: create FILE FORMAT + STAGE → `LIST` → `SELECT $1,$2... FROM @stage(...)` (good for QA). ([Snowflake Documentation][2])\n",
    "* For production queries: prefer loading data into a Snowflake table or create an EXTERNAL TABLE with `AUTO_REFRESH` and partition definitions. ([Snowflake Documentation][4])\n",
    "* If you must query external often: benchmark both (external table vs internal table CTAS) — the cost/latency difference can be large. ([Stack Overflow][7])\n",
    "\n",
    "---\n",
    "\n",
    "# 11) Quick Q\\&A recap (tiny)\n",
    "\n",
    "* Can you filter while querying a stage? **Yes.**\n",
    "* Can you join stage data with tables? **Yes** (but stage side parsed on the fly).\n",
    "* Can you create views over stage selects? **Yes** (but not materialized views over a stage query). ([Snowflake Documentation][8], [Medium][9])\n",
    "* Can you create tables from stage data? **Yes** (CTAS or `COPY INTO`), and this gives best performance. ([Snowflake Documentation][10])\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "[1]: https://docs.snowflake.com/en/sql-reference/sql/create-stage?utm_source=chatgpt.com \"CREATE STAGE - Snowflake Documentation\"\n",
    "[2]: https://docs.snowflake.com/en/sql-reference/sql/create-file-format?utm_source=chatgpt.com \"CREATE FILE FORMAT - Snowflake Documentation\"\n",
    "[3]: https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration?utm_source=chatgpt.com \"CREATE STORAGE INTEGRATION - Snowflake Documentation\"\n",
    "[4]: https://docs.snowflake.com/en/sql-reference/sql/create-external-table \"CREATE EXTERNAL TABLE | Snowflake Documentation\"\n",
    "[5]: https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration?utm_source=chatgpt.com \"Configuring a Snowflake storage integration to access Amazon S3\"\n",
    "[6]: https://docs.snowflake.com/en/user-guide/querying-stage?utm_source=chatgpt.com \"Querying Data in Staged Files - Snowflake Documentation\"\n",
    "[7]: https://stackoverflow.com/questions/70755218/snowflake-query-performance-is-unexpectedly-slower-for-external-parquet-tables-v?utm_source=chatgpt.com \"Snowflake query performance is unexpectedly slower for external ...\"\n",
    "[8]: https://docs.snowflake.com/en/sql-reference/sql/create-view?utm_source=chatgpt.com \"CREATE VIEW - Snowflake Documentation\"\n",
    "[9]: https://medium.com/snowflake/snowflake-external-table-vs-query-on-stage-pros-cons-a839b52dbab1?utm_source=chatgpt.com \"Snowflake External Table Vs Query on Stage…Pros & Cons - Medium\"\n",
    "[10]: https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions?utm_source=chatgpt.com \"Micro-partitions & Data Clustering - Snowflake Documentation\"\n",
    "[11]: https://teej.ghost.io/a-guide-to-the-snowflake-results-cache/?utm_source=chatgpt.com \"A Guide To The Snowflake Results Cache - Teej - Ghost\"\n",
    "[12]: https://www.phdata.io/blog/when-to-use-internal-versus-external-stages-in-snowflake/?utm_source=chatgpt.com \"When To Use Internal vs. External Stages in Snowflake - phData\"\n",
    "[13]: https://medium.com/%40zakary.leblanc/snowflake-cliff-notes-internal-external-stage-7a702bbe8748?utm_source=chatgpt.com \"Snowflake Cliff Notes: Internal/External Stage | by Zakary LeBlanc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540fc4d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1. Querying a CSV in External Stage + `$1,$2` notation**\n",
    "\n",
    "```sql\n",
    "-- Assume stage already exists\n",
    "SELECT $1 AS id, \n",
    "       $2 AS name, \n",
    "       $3 AS salary\n",
    "FROM @my_ext_stage/sales/ \n",
    "(FILE_FORMAT => my_csv_format);\n",
    "```\n",
    "\n",
    "* `$1`, `$2`, `$n` → column **positions** in the file (not table columns).\n",
    "* If CSV has no header, Snowflake doesn’t know column names → `$1` means \"first column in file\".\n",
    "* You can alias them to meaningful names.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What is a Storage Integration?**\n",
    "\n",
    "* A **Storage Integration** is a Snowflake object that stores an **IAM role ARN** instead of hardcoding AWS keys.\n",
    "* Why? → **Security**. You don’t drop permanent AWS keys into Snowflake, instead you trust a Snowflake-generated IAM role.\n",
    "* `STORAGE_ALLOWED_LOCATIONS` → restricts which S3 paths this integration can access. Example:\n",
    "\n",
    "  ```sql\n",
    "  CREATE STORAGE INTEGRATION my_s3_int\n",
    "    TYPE = EXTERNAL_STAGE\n",
    "    STORAGE_PROVIDER = S3\n",
    "    ENABLED = TRUE\n",
    "    STORAGE_ALLOWED_LOCATIONS = ('s3://my-company-bucket/data/');\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Create External Stage with Integration + AWS Trust**\n",
    "\n",
    "```sql\n",
    "CREATE STAGE my_ext_stage\n",
    "  STORAGE_INTEGRATION = my_s3_int\n",
    "  URL = 's3://my-company-bucket/data/'\n",
    "  FILE_FORMAT = my_csv_format;\n",
    "```\n",
    "\n",
    "* On **AWS side**:\n",
    "\n",
    "  * Create IAM role.\n",
    "  * Trust relationship → allow **Snowflake’s generated external ID + Snowflake AWS account** to assume the role.\n",
    "* Snowflake docs provide you Snowflake’s **AWS IAM principal ARN** for your region.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Performance tradeoffs External Table vs Native Table**\n",
    "\n",
    "* **External Table**:\n",
    "\n",
    "  * Reads directly from S3 each time.\n",
    "  * No micro-partitions → can’t do clustering, pruning efficiently.\n",
    "  * Slower queries if files are small/many.\n",
    "* **Native Table (after COPY INTO)**:\n",
    "\n",
    "  * Data ingested → stored in **Snowflake’s micro-partitions**.\n",
    "  * Partition pruning, clustering, stats, caching, materialized views → all work.\n",
    "    👉 Best practice: Use external tables for **discovery / staging**, COPY INTO for **production**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Partitioning semantics with METADATA\\$FILENAME**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE EXTERNAL TABLE sales_ext (\n",
    "  id STRING,\n",
    "  amount NUMBER,\n",
    "  sales_date DATE AS TO_DATE(SUBSTRING(METADATA$FILENAME, 12, 10), 'YYYY-MM-DD')\n",
    ")\n",
    "WITH LOCATION = @my_ext_stage/sales/\n",
    "FILE_FORMAT = my_csv_format\n",
    "AUTO_REFRESH = TRUE;\n",
    "```\n",
    "\n",
    "* Here Snowflake extracts `sales_date` from the **filename path**.\n",
    "* Allows partition pruning (e.g., only scan `2025-08-29/` files).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Can you run UPDATE on an external table?**\n",
    "\n",
    "* ❌ No. External tables are **read-only metadata layer** on top of files.\n",
    "* If you need updates → COPY data into a Snowflake table.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Auto-refresh with SNS/SQS**\n",
    "\n",
    "* Flow:\n",
    "\n",
    "  1. New files land in S3.\n",
    "  2. S3 event → SNS → SQS.\n",
    "  3. Snowflake subscribes to SQS → gets notified → auto-refreshes external table metadata.\n",
    "* Without this, you’d need manual `ALTER EXTERNAL TABLE … REFRESH`.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. PATTERN parameter**\n",
    "\n",
    "* Regex filter for staged files. Example:\n",
    "\n",
    "```sql\n",
    "SELECT * \n",
    "FROM @my_ext_stage\n",
    "(PATTERN => '.*2025-08.*.csv');\n",
    "```\n",
    "\n",
    "* Needed when your stage has mixed files but query should only read a subset.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. JSON Array file format (STRIP\\_OUTER\\_ARRAY)**\n",
    "\n",
    "```sql\n",
    "CREATE FILE FORMAT my_json_format \n",
    "  TYPE = JSON \n",
    "  STRIP_OUTER_ARRAY = TRUE;\n",
    "```\n",
    "\n",
    "* Makes each JSON array element a separate row.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Result Cache validity**\n",
    "\n",
    "* Snowflake **Result Cache = 24 hours** per user, per warehouse, if underlying data doesn’t change.\n",
    "* For external staged files: if files unchanged → repeated queries return from cache, no re-scan.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. COPY INTO vs CTAS**\n",
    "\n",
    "* **COPY INTO**:\n",
    "\n",
    "  * Standard for ingestion.\n",
    "  * Handles errors, can retry, supports validation, staging, incremental loads.\n",
    "* **CTAS**:\n",
    "\n",
    "  * Creates new table from query result.\n",
    "  * One-off operation, not designed for pipelines.\n",
    "    👉 Production ingestion = **COPY INTO**.\n",
    "\n",
    "---\n",
    "\n",
    "### **12. S3 bucket region placement**\n",
    "\n",
    "* Snowflake account in `us-east-1` → keep bucket in `us-east-1`.\n",
    "* If bucket in another region → **cross-region data transfer costs + latency**.\n",
    "\n",
    "---\n",
    "\n",
    "### **13. Inspect staged file metadata**\n",
    "\n",
    "```sql\n",
    "SELECT METADATA$FILENAME, \n",
    "       METADATA$FILE_ROW_NUMBER, \n",
    "       METADATA$FILE_LAST_MODIFIED\n",
    "FROM @my_ext_stage/sales/;\n",
    "```\n",
    "\n",
    "* Lets you see which file/row/time data came from.\n",
    "\n",
    "---\n",
    "\n",
    "### **14. STORAGE\\_BLOCKED\\_LOCATIONS**\n",
    "\n",
    "* Opposite of `STORAGE_ALLOWED_LOCATIONS`.\n",
    "* Prevents Snowflake from accessing certain S3 paths.\n",
    "* Use case: bucket has sensitive PII zone → block it in integration.\n",
    "\n",
    "---\n",
    "\n",
    "### **15. External Parquet Schema Inference**\n",
    "\n",
    "* Snowflake infers Parquet schema, but if evolving schema → use:\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE my_parquet_table\n",
    "  USING TEMPLATE (\n",
    "    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
    "    FROM TABLE(\n",
    "      INFER_SCHEMA(\n",
    "        LOCATION=>'@my_ext_stage/parquet/',\n",
    "        FILE_FORMAT=>'my_parquet_format'\n",
    "      )\n",
    "    )\n",
    "  );\n",
    "```\n",
    "\n",
    "* `USING TEMPLATE` locks schema based on inferred files.\n",
    "* Ensures stable schema for BI queries.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b1abf",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1) Named external stage vs direct S3 URL with storage integration**\n",
    "\n",
    "* **Named external stage**:\n",
    "\n",
    "  * You pre-create it once, storing the S3 path and integration.\n",
    "  * Centralized security: credentials are hidden, not exposed in every query.\n",
    "  * Reusable: many teams/queries can reference `@MY_STAGE/...`.\n",
    "  * Allows `LIST`, `READ`, `REMOVE` operations directly in Snowflake.\n",
    "\n",
    "* **Direct S3 URL with storage integration**:\n",
    "\n",
    "  ```sql\n",
    "  copy into 's3://my-bucket/path/'\n",
    "  from MYTABLE\n",
    "  storage_integration = MY_INT\n",
    "  file_format = (type=csv);\n",
    "  ```\n",
    "\n",
    "  * Quick and flexible.\n",
    "  * But path and integration must be repeated in every query.\n",
    "  * No way to `LIST` from Snowflake (since it’s not a named stage).\n",
    "\n",
    "👉 **Best practice**: use named stages for production pipelines (security + reusability).\n",
    "\n",
    "---\n",
    "\n",
    "### **2) Read consistency & `AT (TIMESTAMP => …)`**\n",
    "\n",
    "Snowflake queries always run on a **consistent snapshot** of data.\n",
    "\n",
    "* But if you run **two separate queries** (e.g., `COUNT(*)` and `COPY INTO`), the snapshots may differ unless you explicitly **lock them to the same time**.\n",
    "* Use `AT (TIMESTAMP => $SNAP_TS)` to ensure both queries see the *same frozen version* of the table.\n",
    "\n",
    "📌 Example:\n",
    "\n",
    "```sql\n",
    "set SNAP_TS = current_timestamp();\n",
    "\n",
    "select count(*) from ORDERS at (timestamp => $SNAP_TS);\n",
    "\n",
    "copy into @MY_STAGE/orders/\n",
    "from (select * from ORDERS at (timestamp => $SNAP_TS));\n",
    "```\n",
    "\n",
    "This guarantees the count and the unloaded files match exactly.\n",
    "\n",
    "---\n",
    "\n",
    "### **3) Why `INCLUDE_QUERY_ID` + `DETAILED_OUTPUT`**\n",
    "\n",
    "* **`INCLUDE_QUERY_ID=TRUE`**:\n",
    "\n",
    "  * Appends the Snowflake query ID into each filename → ensures uniqueness and traceability.\n",
    "  * Prevents overwriting when two processes export to the same prefix.\n",
    "  * Great for audit trails.\n",
    "\n",
    "* **`DETAILED_OUTPUT=TRUE`**:\n",
    "\n",
    "  * Makes `COPY INTO` return **one row per file unloaded**.\n",
    "  * Shows: file path, file size, rows unloaded.\n",
    "  * You can `SUM(rows_unloaded)` and prove row counts match expectations.\n",
    "\n",
    "📌 Example:\n",
    "\n",
    "```sql\n",
    "copy into @MY_STAGE/orders/\n",
    "from (select * from ORDERS)\n",
    "include_query_id = true\n",
    "detailed_output = true;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4) Why `PARTITION BY` can’t be combined with `SINGLE=TRUE` or `OVERWRITE=TRUE`**\n",
    "\n",
    "* **`PARTITION BY`** → Snowflake must create *multiple folders/files* (one per partition).\n",
    "* **`SINGLE=TRUE`** → forces *one file total*. These conflict.\n",
    "* **`OVERWRITE=TRUE`** → deletes and replaces everything under the prefix. Partitioning creates dynamic subfolders, so overwrite could break structure.\n",
    "\n",
    "👉 **Design workaround**:\n",
    "\n",
    "* Instead of `OVERWRITE=TRUE`, unload into a **date-stamped prefix**:\n",
    "\n",
    "  ```\n",
    "  s3://bucket/orders/dt=2025-08-30/\n",
    "  ```\n",
    "\n",
    "  Each run is isolated. Use S3 lifecycle rules to expire old folders.\n",
    "\n",
    "---\n",
    "\n",
    "### **5) Parquet vs CSV (and CSV gotchas)**\n",
    "\n",
    "* **Parquet**:\n",
    "\n",
    "  * Columnar, compressed, schema preserved.\n",
    "  * Best for big data tools (Athena, Spark, Glue).\n",
    "  * Much smaller files → saves cost.\n",
    "\n",
    "* **CSV**:\n",
    "\n",
    "  * Universal (every tool reads it).\n",
    "  * Human-readable.\n",
    "  * But bigger, and fragile with nulls, quotes, line breaks.\n",
    "\n",
    "**CSV fixes to avoid corruption**:\n",
    "\n",
    "```sql\n",
    "create or replace file format FF_CSV\n",
    "  type = csv\n",
    "  field_delimiter = ','\n",
    "  field_optionally_enclosed_by = '\"'\n",
    "  null_if = ('\\\\N','NULL')\n",
    "  empty_field_as_null = true\n",
    "  compression = gzip;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6) How to read back files from S3 (without loading into a table)**\n",
    "\n",
    "This is how you validate after unloading:\n",
    "\n",
    "📌 Parquet:\n",
    "\n",
    "```sql\n",
    "select count(*) \n",
    "from @MY_STAGE/orders/ (file_format => 'FF_PARQUET');\n",
    "```\n",
    "\n",
    "📌 CSV:\n",
    "\n",
    "```sql\n",
    "select count(*)\n",
    "from @MY_STAGE/orders/ (file_format => 'FF_CSV');\n",
    "```\n",
    "\n",
    "You can also spot check:\n",
    "\n",
    "```sql\n",
    "select metadata$filename, count(*)\n",
    "from @MY_STAGE/orders/ (file_format => 'FF_CSV')\n",
    "group by 1;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7) Permissions & storage integration (IAM + external ID)**\n",
    "\n",
    "* Snowflake **never stores AWS keys**. Instead, you set up a **storage integration**.\n",
    "\n",
    "* In Snowflake:\n",
    "\n",
    "  ```sql\n",
    "  create storage integration MY_INT\n",
    "    type = external_stage\n",
    "    storage_provider = s3\n",
    "    storage_aws_role_arn = 'arn:aws:iam::123456789012:role/my-snowflake-role'\n",
    "    storage_allowed_locations = ('s3://my-bucket/path/')\n",
    "    enabled = true;\n",
    "  ```\n",
    "\n",
    "* Then `DESCRIBE INTEGRATION` gives you:\n",
    "\n",
    "  * Snowflake **user ARN**\n",
    "  * **External ID**\n",
    "\n",
    "* In AWS IAM:\n",
    "\n",
    "  * Create a role with trust policy: “Allow Snowflake’s user ARN to assume this role, but only with this external ID.”\n",
    "  * Attach an S3 policy allowing `s3:PutObject`, `s3:ListBucket` (and optionally `DeleteObject`).\n",
    "\n",
    "This prevents cross-account hijacking.\n",
    "\n",
    "---\n",
    "\n",
    "### **8) Encryption options**\n",
    "\n",
    "When unloading, you can specify encryption:\n",
    "\n",
    "```sql\n",
    "copy into @MY_STAGE/orders/\n",
    "from MYTABLE\n",
    "file_format = (type=parquet)\n",
    "encryption = ( type='AWS_SSE_KMS', kms_key_id='arn:aws:kms:us-east-1:123:key/abcd...' );\n",
    "```\n",
    "\n",
    "Options:\n",
    "\n",
    "* `AWS_SSE_S3` → Default S3 server-side encryption (AES-256).\n",
    "* `AWS_SSE_KMS` → Use a customer-managed KMS key.\n",
    "* `NONE` → No encryption (rarely used).\n",
    "\n",
    "---\n",
    "\n",
    "### **9) What if query returns 0 rows?**\n",
    "\n",
    "* Snowflake won’t write any data file at all. The prefix may be empty.\n",
    "* This can break downstream jobs expecting at least one file.\n",
    "\n",
    "👉 Solutions:\n",
    "\n",
    "* Design consumer jobs to **tolerate empty directories**.\n",
    "* Or create a **marker file** manually (e.g., an empty `_SUCCESS` file in S3).\n",
    "* Or in the unload query, force a dummy row:\n",
    "\n",
    "  ```sql\n",
    "  select * from MYTABLE\n",
    "  union all\n",
    "  select null, null, null where not exists (select 1 from MYTABLE);\n",
    "  ```\n",
    "\n",
    "  (This ensures at least one file lands, even if empty.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1c7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
