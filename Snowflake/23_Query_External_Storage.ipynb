{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb474685",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 1) Quick glossary (one-liner)\n",
    "\n",
    "* **Stage** = pointer (URL + credentials) to file(s) in S3 (or Snowflake internal storage). You can `LIST` and query the files directly. ([Snowflake Documentation][1])\n",
    "* **File format** = reusable object that describes CSV/JSON/PARQUET layout. ([Snowflake Documentation][2])\n",
    "* **Storage Integration** = Snowflake object that stores the managed identity / allowed locations used to access S3 (and that ties into the AWS IAM role). Stage can reference a storage integration. ([Snowflake Documentation][3])\n",
    "* **External table** = a Snowflake object that maps files in an external stage to columns, keeps metadata, and can be refreshed automatically (closer to a real table but data remains in S3). ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Complete demo: from file â†’ query (replace placeholders with your values)\n",
    "\n",
    "### Assumptions / sample files\n",
    "\n",
    "S3 bucket: `s3://retailx-raw`\n",
    "Files under `orders/` with csv like `orders_20250828.csv` containing:\n",
    "\n",
    "```\n",
    "order_id,customer_id,created_at,total_usd\n",
    "1001,c_001,2025-08-28 10:01:02, 199.50\n",
    "1002,c_002,2025-08-28 10:05:34, 9.99\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step A â€” create a File Format (CSV)\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT retailx_csv_fmt\n",
    "  TYPE = CSV\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  SKIP_HEADER = 1\n",
    "  FIELD_DELIMITER = ','\n",
    "  TRIM_SPACE = TRUE\n",
    "  NULL_IF = ('', 'NULL', 'null')\n",
    "  COMPRESSION = 'AUTO';\n",
    "```\n",
    "\n",
    "(You can create JSON / PARQUET formats similarly.) ([Snowflake Documentation][2])\n",
    "\n",
    "---\n",
    "\n",
    "### Step B â€” (you already may have) create a Storage Integration\n",
    "\n",
    "> If you already created an integration & AWS role earlier, skip to Step C.\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STORAGE INTEGRATION retailx_s3_int\n",
    "  TYPE = EXTERNAL_STAGE\n",
    "  STORAGE_PROVIDER = 'S3'\n",
    "  ENABLED = TRUE\n",
    "  STORAGE_ALLOWED_LOCATIONS = ('s3://retailx-raw/');\n",
    "```\n",
    "\n",
    "After create, run:\n",
    "\n",
    "```sql\n",
    "DESC INTEGRATION retailx_s3_int;\n",
    "-- copy STORAGE_AWS_IAM_USER_ARN and STORAGE_AWS_EXTERNAL_ID and follow Snowflake/AWS steps to create AWS role/trust policy\n",
    "```\n",
    "\n",
    "(You need to finish the AWS side: create IAM role, trust the Snowflake ARN and external id, attach a least-privilege S3 policy â€” see the Snowflake storage integration docs.) ([Snowflake Documentation][5])\n",
    "\n",
    "---\n",
    "\n",
    "### Step C â€” create an external stage that uses the storage integration and the file format\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE retailx_orders_stage\n",
    "  URL = 's3://retailx-raw/orders/'\n",
    "  STORAGE_INTEGRATION = retailx_s3_int\n",
    "  FILE_FORMAT = retailx_csv_fmt;\n",
    "```\n",
    "\n",
    "(You can also omit `FILE_FORMAT` here and pass it when querying.) ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "### Step D â€” sanity checks\n",
    "\n",
    "```sql\n",
    "-- show files\n",
    "LIST @retailx_orders_stage;\n",
    "```\n",
    "\n",
    "If `LIST` works, you have connectivity. If `ACCESS_DENIED`, check IAM role trust policy, `STORAGE_ALLOWED_LOCATIONS` and the S3 bucket policy/role policy.\n",
    "\n",
    "---\n",
    "\n",
    "### Step E â€” ad-hoc query of CSV files in the stage (no COPY)\n",
    "\n",
    "When you query a staged CSV directly, Snowflake exposes column positions as `$1`, `$2`, ... â€” then you can cast/alias them:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  TRY_CAST(t.$1 AS INTEGER)        AS order_id,\n",
    "  t.$2::STRING                    AS customer_id,\n",
    "  TRY_TO_TIMESTAMP(t.$3, 'YYYY-MM-DD HH24:MI:SS') AS created_at,\n",
    "  TRY_CAST(t.$4 AS NUMBER(10,2))  AS total_usd\n",
    "FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt') t\n",
    "WHERE TRY_CAST(t.$4 AS NUMBER) > 50;\n",
    "```\n",
    "\n",
    "Notes:\n",
    "\n",
    "* You can query the whole stage, a path (`@stage/subpath/`), or a single file (`@stage/file.csv.gz`). Querying is *on-the-fly* parsing of files. ([Snowflake Documentation][6])\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Filters, joins, views, tables â€” examples\n",
    "\n",
    "### 3A â€” Filtering while reading files\n",
    "\n",
    "You saw `WHERE TRY_CAST(t.$4 AS NUMBER) > 50` above â€” predicate filters are allowed, and Snowflake will only scan files it needs if you restrict the stage path or use an external table with partitions. But note: when reading raw files directly from a stage, Snowflake usually scans files on the fly (less efficient than table pruning). ([Snowflake Documentation][4], [Stack Overflow][7])\n",
    "\n",
    "### 3B â€” Join the staged data with an internal table (works fine)\n",
    "\n",
    "```sql\n",
    "-- internal customer master table\n",
    "CREATE OR REPLACE TABLE retailx_customers (customer_id STRING, customer_name STRING);\n",
    "\n",
    "-- join ad-hoc stage data with internal table\n",
    "SELECT s.order_id, s.customer_id, c.customer_name, s.total_usd\n",
    "FROM (\n",
    "  SELECT\n",
    "    TRY_CAST(t.$1 AS INTEGER) AS order_id,\n",
    "    t.$2::STRING             AS customer_id,\n",
    "    TRY_CAST(t.$4 AS NUMBER(10,2)) AS total_usd\n",
    "  FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt') t\n",
    ") s\n",
    "JOIN retailx_customers c ON s.customer_id = c.customer_id\n",
    "WHERE s.total_usd > 100;\n",
    "```\n",
    "\n",
    "Yes â€” you can join staged data with tables. The staged side is parsed on the fly (positional columns), the table side benefits from Snowflake table optimizations.\n",
    "\n",
    "### 3C â€” Create a view that wraps the stage query\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE VIEW v_orders_stage AS\n",
    "SELECT\n",
    "  TRY_CAST(t.$1 AS INTEGER) AS order_id,\n",
    "  t.$2::STRING               AS customer_id,\n",
    "  TRY_TO_TIMESTAMP(t.$3,'YYYY-MM-DD HH24:MI:SS') AS created_at,\n",
    "  TRY_CAST(t.$4 AS NUMBER(10,2)) AS total_usd\n",
    "FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt') t;\n",
    "```\n",
    "\n",
    "A normal `VIEW` over a stage is allowed (view stores the query). When you query the view, files are re-read and re-parsed at query time. *You cannot create a materialized view on a stage query.* ([Snowflake Documentation][8], [Medium][9])\n",
    "\n",
    "### 3D â€” Create a table (persist/load the data into Snowflake)\n",
    "\n",
    "If you want the performance and Snowflake features, load into a table:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE retailx_orders AS\n",
    "SELECT\n",
    "  TRY_CAST(t.$1 AS INTEGER) AS order_id,\n",
    "  t.$2::STRING              AS customer_id,\n",
    "  TRY_TO_TIMESTAMP(t.$3,'YYYY-MM-DD HH24:MI:SS') AS created_at,\n",
    "  TRY_CAST(t.$4 AS NUMBER(10,2)) AS total_usd\n",
    "FROM @retailx_orders_stage (FILE_FORMAT => 'retailx_csv_fmt') t;\n",
    "```\n",
    "\n",
    "Now data lives in Snowflake storage, will be micro-partitioned, and queries will typically be far faster and cheaper for repeated workloads. ([Snowflake Documentation][10])\n",
    "\n",
    "---\n",
    "\n",
    "# 4) External table (the â€œnice middle groundâ€)\n",
    "\n",
    "External tables create metadata in Snowflake and let you define named columns (mapping into `VALUE:c1`, `VALUE:c2`, etc.), and support automatic refresh (SNS/SQS) for S3 to pick up new files. They are useful when you need table-like access without copying data into Snowflake. Example:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE EXTERNAL TABLE retailx_orders_ext (\n",
    "  order_id    varchar as (value:c1::varchar),\n",
    "  customer_id varchar as (value:c2::varchar),\n",
    "  created_at  timestamp_ntz as (value:c3::timestamp_ntz),\n",
    "  total_usd   number(10,2) as (value:c4::number)\n",
    ")\n",
    "WITH LOCATION = @retailx_orders_stage\n",
    "FILE_FORMAT = (FORMAT_NAME = 'retailx_csv_fmt')\n",
    "AUTO_REFRESH = TRUE\n",
    "REFRESH_ON_CREATE = TRUE;\n",
    "```\n",
    "\n",
    "Key points:\n",
    "\n",
    "* External tables can be partitioned and their metadata refreshed automatically (SNS/SQS) to register new files. ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Differences vs â€œtraditionalâ€ Snowflake table queries (detailed)\n",
    "\n",
    "### What internal Snowflake tables give you (advantages):\n",
    "\n",
    "* **Micro-partitions & pruning** (very fast scanning / min/max metadata) â€” Snowflake stores column min/max and can prune partitions. This massively reduces scan cost for selective filters. ([Snowflake Documentation][10])\n",
    "* **Clustering, time travel, cloning, DML, materialized views on tables** â€” advanced features you donâ€™t get on stage reads or (some of) external tables. ([Snowflake Documentation][4])\n",
    "* **Faster repeated queries via micro-partition caching** and more cost-efficient compute for analytical workloads. ([Teej][11])\n",
    "\n",
    "### What stage / external reads **lack** or are weaker at:\n",
    "\n",
    "* **No micro-partition pruning** because files are not loaded into Snowflakeâ€™s micro-partition storage â€” queries must re-scan/parsing files (slower for heavy analytic workloads). ([Stack Overflow][7])\n",
    "* **Less pushdown & metadata**: stage queries return positional columns (\\$1,\\$2) unless you wrap them or use external tables to define columns. External tables can add metadata but still wonâ€™t have micro-partitions. ([Snowflake Documentation][6])\n",
    "* **No materialized views directly on stage queries** (external tables can have materialized views in some cases, but there are caveats). ([Medium][9])\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Benefits of keeping data in S3 & querying from Snowflake\n",
    "\n",
    "* **Storage cost savings** (S3 can be cheaper than Snowflake long-term storage for cold/raw data). ([phData][12])\n",
    "* **Single source of truth / interoperability** â€” other systems can read the same S3 files.\n",
    "* **Fast exploration & QA** â€” you can sample raw files without duplicating them into Snowflake.\n",
    "* **Immediate availability** for new raw files if using external tables + auto refresh or Snowpipe. ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Disadvantages / costs / caveats\n",
    "\n",
    "* **Query performance**: external queries are generally slower and can be more compute-heavy compared to native Snowflake tables. ([Stack Overflow][7])\n",
    "* **Limited Snowflake features**: no micro-partitioning, no time travel, no cloning, limited materialized view options, some DDL/DML limitations. ([Snowflake Documentation][4])\n",
    "* **Metadata maintenance**: external tables require refreshes (manual or event-driven) to reflect new/removed files; auto-refresh costs (Snowpipe charges) can appear. ([Snowflake Documentation][4])\n",
    "* **Potential egress costs**: if bucket and Snowflake are in *different* regions, you may pay cross-region data transfer. (Keep them same region where possible.) ([Medium][13])\n",
    "\n",
    "---\n",
    "\n",
    "# 8) When to use each option (rules of thumb)\n",
    "\n",
    "* **Query stage / external table**\n",
    "\n",
    "  * Use for: exploration, profiling raw data, QA, cross-platform sharing, and for rarely-queried historical datasets where you want to avoid copying.\n",
    "  * Use external table (not ad-hoc stage select) when you want table-like convenience (named columns, refresh) without loading. ([Snowflake Documentation][4])\n",
    "* **Load into internal Snowflake table**\n",
    "\n",
    "  * Use for: production analytics, frequent queries, joins/aggregations at scale, low latency dashboards, or any workload that benefits from micro-partitions, clustering, and Snowflake optimizations. ([Snowflake Documentation][10])\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Can I query *only* the S3 files I added to the Integration?\n",
    "\n",
    "Short answer: **No single magic permission in the integration alone controls the exact files â€” three things must align**:\n",
    "\n",
    "1. **Stage URL / external table LOCATION** â€” the stage or external table points to a specific bucket/path (that's your first limit). You can only query files under that stage path. ([Snowflake Documentation][4])\n",
    "2. **Storage Integration `STORAGE_ALLOWED_LOCATIONS`** â€” on the Snowflake side you can restrict allowed S3 URIs for that integration; stage URLs must align to those locations. ([Snowflake Documentation][3])\n",
    "3. **AWS IAM role / bucket policy** â€” AWS must grant the Snowflake role `s3:ListBucket` / `s3:GetObject` for the specific bucket & prefixes. If the IAM policy does not allow a prefix, Snowflake cannot read it even if the integration lists it. ([Snowflake Documentation][5])\n",
    "\n",
    "So: **you can only successfully query files that are (A) in the stage path, (B) allowed by STORAGE\\_ALLOWED\\_LOCATIONS, and (C) allowed by the AWS role/bucket policy**. All three must allow access.\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Short checklist / action items (practical)\n",
    "\n",
    "* For quick inspection: create FILE FORMAT + STAGE â†’ `LIST` â†’ `SELECT $1,$2... FROM @stage(...)` (good for QA). ([Snowflake Documentation][2])\n",
    "* For production queries: prefer loading data into a Snowflake table or create an EXTERNAL TABLE with `AUTO_REFRESH` and partition definitions. ([Snowflake Documentation][4])\n",
    "* If you must query external often: benchmark both (external table vs internal table CTAS) â€” the cost/latency difference can be large. ([Stack Overflow][7])\n",
    "\n",
    "---\n",
    "\n",
    "# 11) Quick Q\\&A recap (tiny)\n",
    "\n",
    "* Can you filter while querying a stage? **Yes.**\n",
    "* Can you join stage data with tables? **Yes** (but stage side parsed on the fly).\n",
    "* Can you create views over stage selects? **Yes** (but not materialized views over a stage query). ([Snowflake Documentation][8], [Medium][9])\n",
    "* Can you create tables from stage data? **Yes** (CTAS or `COPY INTO`), and this gives best performance. ([Snowflake Documentation][10])\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "[1]: https://docs.snowflake.com/en/sql-reference/sql/create-stage?utm_source=chatgpt.com \"CREATE STAGE - Snowflake Documentation\"\n",
    "[2]: https://docs.snowflake.com/en/sql-reference/sql/create-file-format?utm_source=chatgpt.com \"CREATE FILE FORMAT - Snowflake Documentation\"\n",
    "[3]: https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration?utm_source=chatgpt.com \"CREATE STORAGE INTEGRATION - Snowflake Documentation\"\n",
    "[4]: https://docs.snowflake.com/en/sql-reference/sql/create-external-table \"CREATE EXTERNAL TABLE | Snowflake Documentation\"\n",
    "[5]: https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration?utm_source=chatgpt.com \"Configuring a Snowflake storage integration to access Amazon S3\"\n",
    "[6]: https://docs.snowflake.com/en/user-guide/querying-stage?utm_source=chatgpt.com \"Querying Data in Staged Files - Snowflake Documentation\"\n",
    "[7]: https://stackoverflow.com/questions/70755218/snowflake-query-performance-is-unexpectedly-slower-for-external-parquet-tables-v?utm_source=chatgpt.com \"Snowflake query performance is unexpectedly slower for external ...\"\n",
    "[8]: https://docs.snowflake.com/en/sql-reference/sql/create-view?utm_source=chatgpt.com \"CREATE VIEW - Snowflake Documentation\"\n",
    "[9]: https://medium.com/snowflake/snowflake-external-table-vs-query-on-stage-pros-cons-a839b52dbab1?utm_source=chatgpt.com \"Snowflake External Table Vs Query on Stageâ€¦Pros & Cons - Medium\"\n",
    "[10]: https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions?utm_source=chatgpt.com \"Micro-partitions & Data Clustering - Snowflake Documentation\"\n",
    "[11]: https://teej.ghost.io/a-guide-to-the-snowflake-results-cache/?utm_source=chatgpt.com \"A Guide To The Snowflake Results Cache - Teej - Ghost\"\n",
    "[12]: https://www.phdata.io/blog/when-to-use-internal-versus-external-stages-in-snowflake/?utm_source=chatgpt.com \"When To Use Internal vs. External Stages in Snowflake - phData\"\n",
    "[13]: https://medium.com/%40zakary.leblanc/snowflake-cliff-notes-internal-external-stage-7a702bbe8748?utm_source=chatgpt.com \"Snowflake Cliff Notes: Internal/External Stage | by Zakary LeBlanc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540fc4d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1. Querying a CSV in External Stage + `$1,$2` notation**\n",
    "\n",
    "```sql\n",
    "-- Assume stage already exists\n",
    "SELECT $1 AS id, \n",
    "       $2 AS name, \n",
    "       $3 AS salary\n",
    "FROM @my_ext_stage/sales/ \n",
    "(FILE_FORMAT => my_csv_format);\n",
    "```\n",
    "\n",
    "* `$1`, `$2`, `$n` â†’ column **positions** in the file (not table columns).\n",
    "* If CSV has no header, Snowflake doesnâ€™t know column names â†’ `$1` means \"first column in file\".\n",
    "* You can alias them to meaningful names.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What is a Storage Integration?**\n",
    "\n",
    "* A **Storage Integration** is a Snowflake object that stores an **IAM role ARN** instead of hardcoding AWS keys.\n",
    "* Why? â†’ **Security**. You donâ€™t drop permanent AWS keys into Snowflake, instead you trust a Snowflake-generated IAM role.\n",
    "* `STORAGE_ALLOWED_LOCATIONS` â†’ restricts which S3 paths this integration can access. Example:\n",
    "\n",
    "  ```sql\n",
    "  CREATE STORAGE INTEGRATION my_s3_int\n",
    "    TYPE = EXTERNAL_STAGE\n",
    "    STORAGE_PROVIDER = S3\n",
    "    ENABLED = TRUE\n",
    "    STORAGE_ALLOWED_LOCATIONS = ('s3://my-company-bucket/data/');\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Create External Stage with Integration + AWS Trust**\n",
    "\n",
    "```sql\n",
    "CREATE STAGE my_ext_stage\n",
    "  STORAGE_INTEGRATION = my_s3_int\n",
    "  URL = 's3://my-company-bucket/data/'\n",
    "  FILE_FORMAT = my_csv_format;\n",
    "```\n",
    "\n",
    "* On **AWS side**:\n",
    "\n",
    "  * Create IAM role.\n",
    "  * Trust relationship â†’ allow **Snowflakeâ€™s generated external ID + Snowflake AWS account** to assume the role.\n",
    "* Snowflake docs provide you Snowflakeâ€™s **AWS IAM principal ARN** for your region.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Performance tradeoffs External Table vs Native Table**\n",
    "\n",
    "* **External Table**:\n",
    "\n",
    "  * Reads directly from S3 each time.\n",
    "  * No micro-partitions â†’ canâ€™t do clustering, pruning efficiently.\n",
    "  * Slower queries if files are small/many.\n",
    "* **Native Table (after COPY INTO)**:\n",
    "\n",
    "  * Data ingested â†’ stored in **Snowflakeâ€™s micro-partitions**.\n",
    "  * Partition pruning, clustering, stats, caching, materialized views â†’ all work.\n",
    "    ðŸ‘‰ Best practice: Use external tables for **discovery / staging**, COPY INTO for **production**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Partitioning semantics with METADATA\\$FILENAME**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE EXTERNAL TABLE sales_ext (\n",
    "  id STRING,\n",
    "  amount NUMBER,\n",
    "  sales_date DATE AS TO_DATE(SUBSTRING(METADATA$FILENAME, 12, 10), 'YYYY-MM-DD')\n",
    ")\n",
    "WITH LOCATION = @my_ext_stage/sales/\n",
    "FILE_FORMAT = my_csv_format\n",
    "AUTO_REFRESH = TRUE;\n",
    "```\n",
    "\n",
    "* Here Snowflake extracts `sales_date` from the **filename path**.\n",
    "* Allows partition pruning (e.g., only scan `2025-08-29/` files).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Can you run UPDATE on an external table?**\n",
    "\n",
    "* âŒ No. External tables are **read-only metadata layer** on top of files.\n",
    "* If you need updates â†’ COPY data into a Snowflake table.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Auto-refresh with SNS/SQS**\n",
    "\n",
    "* Flow:\n",
    "\n",
    "  1. New files land in S3.\n",
    "  2. S3 event â†’ SNS â†’ SQS.\n",
    "  3. Snowflake subscribes to SQS â†’ gets notified â†’ auto-refreshes external table metadata.\n",
    "* Without this, youâ€™d need manual `ALTER EXTERNAL TABLE â€¦ REFRESH`.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. PATTERN parameter**\n",
    "\n",
    "* Regex filter for staged files. Example:\n",
    "\n",
    "```sql\n",
    "SELECT * \n",
    "FROM @my_ext_stage\n",
    "(PATTERN => '.*2025-08.*.csv');\n",
    "```\n",
    "\n",
    "* Needed when your stage has mixed files but query should only read a subset.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. JSON Array file format (STRIP\\_OUTER\\_ARRAY)**\n",
    "\n",
    "```sql\n",
    "CREATE FILE FORMAT my_json_format \n",
    "  TYPE = JSON \n",
    "  STRIP_OUTER_ARRAY = TRUE;\n",
    "```\n",
    "\n",
    "* Makes each JSON array element a separate row.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Result Cache validity**\n",
    "\n",
    "* Snowflake **Result Cache = 24 hours** per user, per warehouse, if underlying data doesnâ€™t change.\n",
    "* For external staged files: if files unchanged â†’ repeated queries return from cache, no re-scan.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. COPY INTO vs CTAS**\n",
    "\n",
    "* **COPY INTO**:\n",
    "\n",
    "  * Standard for ingestion.\n",
    "  * Handles errors, can retry, supports validation, staging, incremental loads.\n",
    "* **CTAS**:\n",
    "\n",
    "  * Creates new table from query result.\n",
    "  * One-off operation, not designed for pipelines.\n",
    "    ðŸ‘‰ Production ingestion = **COPY INTO**.\n",
    "\n",
    "---\n",
    "\n",
    "### **12. S3 bucket region placement**\n",
    "\n",
    "* Snowflake account in `us-east-1` â†’ keep bucket in `us-east-1`.\n",
    "* If bucket in another region â†’ **cross-region data transfer costs + latency**.\n",
    "\n",
    "---\n",
    "\n",
    "### **13. Inspect staged file metadata**\n",
    "\n",
    "```sql\n",
    "SELECT METADATA$FILENAME, \n",
    "       METADATA$FILE_ROW_NUMBER, \n",
    "       METADATA$FILE_LAST_MODIFIED\n",
    "FROM @my_ext_stage/sales/;\n",
    "```\n",
    "\n",
    "* Lets you see which file/row/time data came from.\n",
    "\n",
    "---\n",
    "\n",
    "### **14. STORAGE\\_BLOCKED\\_LOCATIONS**\n",
    "\n",
    "* Opposite of `STORAGE_ALLOWED_LOCATIONS`.\n",
    "* Prevents Snowflake from accessing certain S3 paths.\n",
    "* Use case: bucket has sensitive PII zone â†’ block it in integration.\n",
    "\n",
    "---\n",
    "\n",
    "### **15. External Parquet Schema Inference**\n",
    "\n",
    "* Snowflake infers Parquet schema, but if evolving schema â†’ use:\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE my_parquet_table\n",
    "  USING TEMPLATE (\n",
    "    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
    "    FROM TABLE(\n",
    "      INFER_SCHEMA(\n",
    "        LOCATION=>'@my_ext_stage/parquet/',\n",
    "        FILE_FORMAT=>'my_parquet_format'\n",
    "      )\n",
    "    )\n",
    "  );\n",
    "```\n",
    "\n",
    "* `USING TEMPLATE` locks schema based on inferred files.\n",
    "* Ensures stable schema for BI queries.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d45dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
