{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8bb629",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 🔷 1. What is a **Retriever** in LangChain?\n",
    "\n",
    "### ✅ Definition:\n",
    "\n",
    "A **Retriever** is a component that lets you **search and fetch the most relevant documents** from a data source (usually a **vector store**), based on a query.\n",
    "\n",
    "It does not generate answers. It just **retrieves documents** likely to contain the answer.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Why is a Retriever Needed?\n",
    "\n",
    "When you ask an LLM a question like:\n",
    "\n",
    "> *“What are the symptoms of keratoconus?”*\n",
    "\n",
    "The model may **hallucinate** if it lacks context.\n",
    "\n",
    "So instead, we:\n",
    "\n",
    "1. Store medical articles as vector embeddings.\n",
    "2. Use a retriever to fetch **top-k relevant chunks** based on semantic similarity.\n",
    "3. Feed those chunks into the LLM.\n",
    "\n",
    "➡ This technique is the foundation of **RAG (Retrieval-Augmented Generation)**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Relationship with Vector Store:\n",
    "\n",
    "* You store documents in a vector store like FAISS, Chroma, etc.\n",
    "* Then call `.as_retriever()` on it to turn it into a **Retriever** object.\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🔷 2. What is a **Retrieval Chain**?\n",
    "\n",
    "### ✅ Definition:\n",
    "\n",
    "A **Retrieval Chain** is a LangChain **chain** that:\n",
    "\n",
    "1. **Takes user input**\n",
    "2. Uses a **Retriever** to find relevant documents\n",
    "3. Passes them into an **LLM Prompt**\n",
    "4. Gets back the answer\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Purpose:\n",
    "\n",
    "To allow **custom, document-grounded answering** using retrieved knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Example:\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "result = qa_chain.run(\"What is LangChain?\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "Here, the chain retrieves relevant docs → sends them to an LLM → gets the final answer.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔷 3. What is a **Document Chain**?\n",
    "\n",
    "### ✅ Definition:\n",
    "\n",
    "A **Document Chain** is a chain that:\n",
    "\n",
    "* Accepts a set of documents\n",
    "* Passes them into an LLM via a prompt\n",
    "* Generates an answer\n",
    "\n",
    "> It doesn't do the retrieval; it's just the “answering” part.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ When is it used?\n",
    "\n",
    "Used **after** documents are already selected.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Example:\n",
    "\n",
    "```python\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain.invoke({\"input_documents\": documents, \"question\": \"What is LangChain?\"})\n",
    "```\n",
    "\n",
    "It’s often used **inside** a Retrieval Chain. First retrieve → then use document chain.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔷 4. What is `create_stuff_documents_chain()`?\n",
    "\n",
    "### ✅ Function:\n",
    "\n",
    "This is a utility to create a **Chain** that:\n",
    "\n",
    "* Takes a list of documents\n",
    "* “Stuffs” them into a prompt (i.e., inserts all at once)\n",
    "* Sends to an LLM\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ When do we use it?\n",
    "\n",
    "When:\n",
    "\n",
    "* You already have relevant documents (from retriever or manual filtering)\n",
    "* You want the LLM to answer using only those\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Real-World Scenario:\n",
    "\n",
    "You scrape product reviews of a laptop, chunk & embed them, and now want the model to answer:\n",
    "\n",
    "> “What are users saying about battery life?”\n",
    "\n",
    "You don’t need to generate embeddings again. Just:\n",
    "\n",
    "* Select top review chunks\n",
    "* Use `create_stuff_documents_chain` to summarize/answer\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Code Example:\n",
    "\n",
    "```python\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question based on the documents.\"),\n",
    "    (\"user\", \"Documents: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "stuff_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "stuff_chain.invoke({\n",
    "    \"input_documents\": docs,\n",
    "    \"question\": \"Summarize user feedback on battery life\"\n",
    "})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🔷 5. What is this line doing?\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "```\n",
    "\n",
    "### ✅ Purpose:\n",
    "\n",
    "LangChain uses a custom `Document` class to **wrap text along with metadata**.\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    page_content=\"This is a laptop review...\",\n",
    "    metadata={\"source\": \"Amazon\", \"rating\": \"4.5\"}\n",
    ")\n",
    "```\n",
    "\n",
    "### ✅ Why important?\n",
    "\n",
    "* Metadata helps with filtering later.\n",
    "* The `create_stuff_documents_chain()` expects a list of `Document` objects, not plain strings.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔷 6. How do we create a retriever from a vectorstore?\n",
    "\n",
    "### ✅ Step-by-step:\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "You now have:\n",
    "\n",
    "* A persistent DB (FAISS)\n",
    "* A retriever to use inside chains\n",
    "\n",
    "---\n",
    "\n",
    "# 🔷 7. What does `create_retrieval_chain()` do?\n",
    "\n",
    "### ✅ Function:\n",
    "\n",
    "This function combines:\n",
    "\n",
    "* a **retriever**\n",
    "* a **document chain** (like stuff chain)\n",
    "  into a **single retrieval-augmented generation (RAG)** chain.\n",
    "\n",
    "```python\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "rag_chain.invoke({\"input\": \"What are LLM agents?\"})\n",
    "```\n",
    "\n",
    "This is more flexible than `RetrievalQA`, which hides inner parts.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔷 8. Additional Important Concepts\n",
    "\n",
    "### 🔸 How documents are passed:\n",
    "\n",
    "LangChain passes documents to chains as `input_documents`, not raw text.\n",
    "\n",
    "### 🔸 `stuff`, `map_reduce`, `refine`:\n",
    "\n",
    "When using `combine_documents_chain`, you can choose how to combine multiple docs:\n",
    "\n",
    "* `stuff`: concatenate all (works for small docs)\n",
    "* `map_reduce`: map over docs then reduce summaries\n",
    "* `refine`: summarize and iteratively refine\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ Top 10 Important Summary Questions\n",
    "\n",
    "| #  | Question                                                                            |\n",
    "| -- | ----------------------------------------------------------------------------------- |\n",
    "| 1  | What is the difference between a Retriever and a Vector Store?                      |\n",
    "| 2  | How does LangChain retrieve context from documents using a retriever?               |\n",
    "| 3  | What is a Retrieval Chain and what does it do under the hood?                       |\n",
    "| 4  | What is a Document Chain and when do you use `create_stuff_documents_chain`?        |\n",
    "| 5  | What types of document combination strategies are available in LangChain?           |\n",
    "| 6  | How does FAISS store and retrieve documents in vector space?                        |\n",
    "| 7  | Why do we split text into chunks before storing in a vector DB?                     |\n",
    "| 8  | How does cosine similarity work for document retrieval?                             |\n",
    "| 9  | What’s the purpose of the `Document` class in LangChain?                            |\n",
    "| 10 | Compare `RetrievalQA` vs `create_retrieval_chain` — which is more flexible and why? |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0dc8ee",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1. What is the difference between a Retriever and a Vector Store?**\n",
    "\n",
    "* **Vector Store** is a **database** that stores high-dimensional embeddings (vector representations) of documents.\n",
    "* **Retriever** is a **wrapper** over the vector store that provides an easy interface to **search and fetch relevant documents** based on a user query.\n",
    "\n",
    "**In short**:\n",
    "\n",
    "> Vector Store = storage system.\n",
    "> Retriever = semantic search engine built on top of it.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. How does LangChain retrieve context from documents using a retriever?**\n",
    "\n",
    "LangChain uses this 3-step process:\n",
    "\n",
    "1. Converts user query → embedding using a model like OpenAIEmbeddings.\n",
    "2. Performs a similarity search in the **vector store** using **cosine similarity**.\n",
    "3. Returns top-k relevant `Document` objects with content and metadata.\n",
    "\n",
    "These documents are then passed to an LLM for answering.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What is a Retrieval Chain and what does it do under the hood?**\n",
    "\n",
    "A **Retrieval Chain**:\n",
    "\n",
    "* Takes user input\n",
    "* Uses a **Retriever** to fetch relevant docs\n",
    "* Passes the docs + question into a **document-answering chain**\n",
    "* Returns the LLM’s response\n",
    "\n",
    "**Under the hood**:\n",
    "\n",
    "```python\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "answer = document_chain.invoke({\"input_documents\": retrieved_docs, \"question\": query})\n",
    "```\n",
    "\n",
    "**LangChain abstraction**:\n",
    "\n",
    "```python\n",
    "rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. What is a Document Chain and when do you use `create_stuff_documents_chain`?**\n",
    "\n",
    "A **Document Chain** is responsible for:\n",
    "\n",
    "* Taking multiple documents\n",
    "* Combining them in a prompt\n",
    "* Passing to LLM to answer or summarize\n",
    "\n",
    "`create_stuff_documents_chain()` is used when:\n",
    "\n",
    "* You have **a small number of documents**\n",
    "* You want to **stuff them all into a single prompt**\n",
    "\n",
    "**Scenario**: If your documents are small reviews about a product and you want the LLM to answer:\n",
    "\n",
    "> *“Summarize battery feedback from all reviews.”*\n",
    "\n",
    "---\n",
    "\n",
    "### **5. What types of document combination strategies are available in LangChain?**\n",
    "\n",
    "LangChain supports:\n",
    "\n",
    "* **Stuff**: Stuff all docs into a prompt (best for small input).\n",
    "* **Map-Reduce**:\n",
    "\n",
    "  * `Map`: Run LLM on each doc individually.\n",
    "  * `Reduce`: Combine outputs.\n",
    "* **Refine**:\n",
    "\n",
    "  * Start with summary of one doc.\n",
    "  * Refine the summary using each following doc.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "```python\n",
    "from langchain.chains.combine_documents import create_map_reduce_documents_chain\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. How does FAISS store and retrieve documents in vector space?**\n",
    "\n",
    "* **FAISS (Facebook AI Similarity Search)** stores documents as **dense vector embeddings**.\n",
    "* Each embedding is stored in an index using **inner product or cosine distance**.\n",
    "* When a query is run:\n",
    "\n",
    "  * It's converted into an embedding\n",
    "  * FAISS compares it to stored vectors\n",
    "  * Returns closest documents by similarity\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Why do we split text into chunks before storing in a vector DB?**\n",
    "\n",
    "Because:\n",
    "\n",
    "* LLMs have **context window limits**\n",
    "* Long documents reduce retrieval accuracy\n",
    "* Smaller chunks ensure **fine-grained semantic matching**\n",
    "\n",
    "**If we don’t**:\n",
    "\n",
    "* Vector embeddings will represent **entire large texts**, making it harder to match specific queries.\n",
    "* Retrieval quality drops.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. How does cosine similarity work for document retrieval?**\n",
    "\n",
    "Cosine similarity calculates the **angle** between two vectors:\n",
    "\n",
    "* Closer angle = more similar.\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  \\text{cosine\\_sim}(A, B) = \\frac{A \\cdot B}{||A|| \\times ||B||}\n",
    "  $$\n",
    "\n",
    "In retrieval:\n",
    "\n",
    "* Compare query vector with document vectors\n",
    "* Return top-k most similar documents\n",
    "\n",
    "---\n",
    "\n",
    "### **9. What’s the purpose of the `Document` class in LangChain?**\n",
    "\n",
    "LangChain uses its own `Document` class to:\n",
    "\n",
    "* Wrap text content in `page_content`\n",
    "* Attach **metadata** like source, author, URL, etc.\n",
    "\n",
    "**Why important**:\n",
    "\n",
    "* Enables **filtering**\n",
    "* Enables **source citation**\n",
    "* Needed by most LangChain chains and retrievers\n",
    "\n",
    "**Example**:\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "Document(\n",
    "    page_content=\"Keratoconus is a progressive eye condition...\",\n",
    "    metadata={\"source\": \"WebMD\"}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Compare `RetrievalQA` vs `create_retrieval_chain` — which is more flexible and why?**\n",
    "\n",
    "| Feature          | `RetrievalQA` | `create_retrieval_chain` |\n",
    "| ---------------- | ------------- | ------------------------ |\n",
    "| Simplicity       | ✅ Easy setup  | ⚠️ More configuration    |\n",
    "| Custom Prompt    | ❌ Limited     | ✅ Fully customizable     |\n",
    "| Custom chains    | ❌ No          | ✅ Yes                    |\n",
    "| Granular control | ❌ No          | ✅ Yes                    |\n",
    "| Flexibility      | ❌ Low         | ✅ High                   |\n",
    "\n",
    "**Use `RetrievalQA`** for fast prototyping.\n",
    "**Use `create_retrieval_chain`** for production-grade apps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017984a1",
   "metadata": {},
   "source": [
    " **build a basic LangChain RAG (Retrieval-Augmented Generation) app** that fully demonstrates the following concepts:\n",
    "\n",
    "✅ **Retrieval Chain**\n",
    "✅ **Document Chain**\n",
    "✅ **create\\_stuff\\_documents\\_chain**\n",
    "✅ **Retriever and Vector DB connection**\n",
    "✅ **Working code**\n",
    "✅ **Explanation of each concept inline (like a proper classroom lecture)**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 STEP 1: Install Required Libraries\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-community langchain-openai faiss-cpu beautifulsoup4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📁 STEP 2: Folder Structure (Optional, for real apps)\n",
    "\n",
    "```\n",
    "basic_llm_rag_app/\n",
    "│\n",
    "├── app.py\n",
    "├── .env  (stores your OpenAI key)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📄 STEP 3: Full Working Code (`app.py`)\n",
    "\n",
    "```python\n",
    "# Step 1: Load Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Step 2: Import LangChain Components\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Step 3: Scrape a web page and load content\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Keratin\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Step 4: Split documents into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 5: Convert chunks into embeddings and store in FAISS\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Step 6: Convert vectorstore into retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Step 7: Create a simple system prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question using only the context provided.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\"\"\")\n",
    "\n",
    "# Step 8: Create the document chain (combines retrieved docs into one prompt)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Step 9: Create the retrieval chain (retriever + doc chain)\n",
    "retrieval_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=document_chain)\n",
    "\n",
    "# Step 10: Run the app\n",
    "query = \"What is the role of keratin in hair?\"\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"Response:\\n\", response['answer'])\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981a95c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🧱 Step-by-Step: Prompt Variable Filling in LangChain\n",
    "\n",
    "We’ll simulate a basic LangChain Retrieval Augmented Generation (RAG) app and **track how data flows** into the `{context}` and `{input}` variables.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Step 1: Setup the Prompt Template\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question using only the context provided.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "➡️ This defines a **template** that expects two inputs:\n",
    "\n",
    "* `{context}`: The retrieved documents\n",
    "* `{input}`: The user’s question\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Step 2: Prepare Documents (Simulated Web Page or Notes)\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"Keratin is a structural protein found in hair, nails, and skin.\"),\n",
    "    Document(page_content=\"In hair, keratin forms protective layers and prevents breakage.\"),\n",
    "    Document(page_content=\"Keratin treatments are used to smooth and straighten hair.\")\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Step 3: Split and Embed Documents\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# 2. Create vector embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embedding)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Step 4: Convert Vectorstore to Retriever\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "At this point, you can do:\n",
    "\n",
    "```python\n",
    "results = retriever.invoke(\"What does keratin do in hair?\")\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Step 5: Create the Document Chain\n",
    "\n",
    "We want to **combine documents into one string** (`context`) and feed it into the prompt.\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "```\n",
    "\n",
    "🧠 `create_stuff_documents_chain()` takes:\n",
    "\n",
    "* Your `llm`\n",
    "* A prompt template that expects `context` and `input`\n",
    "* It automatically combines `Document.page_content` into one `context` string.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Step 6: Create the Retrieval Chain\n",
    "\n",
    "```python\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "```\n",
    "\n",
    "Now you can pass just the user’s question:\n",
    "\n",
    "```python\n",
    "response = retrieval_chain.invoke({\"input\": \"What does keratin do in hair?\"})\n",
    "print(response[\"answer\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Debugging the Filling of `{context}` and `{input}`\n",
    "\n",
    "Let’s manually walk through what happens inside `invoke({\"input\": \"...\"})`.\n",
    "\n",
    "### ✳️ 1. You pass:\n",
    "\n",
    "```python\n",
    "{\"input\": \"What does keratin do in hair?\"}\n",
    "```\n",
    "\n",
    "### ✳️ 2. Retrieval happens:\n",
    "\n",
    "Retriever searches and returns documents:\n",
    "\n",
    "```python\n",
    "[\n",
    "  Document(page_content=\"Keratin is a structural protein found in hair, nails, and skin.\"),\n",
    "  Document(page_content=\"In hair, keratin forms protective layers and prevents breakage.\")\n",
    "]\n",
    "```\n",
    "\n",
    "### ✳️ 3. These `Document.page_content` values get combined into `context`:\n",
    "\n",
    "```python\n",
    "context = (\n",
    "    \"Keratin is a structural protein found in hair, nails, and skin.\\n\"\n",
    "    \"In hair, keratin forms protective layers and prevents breakage.\"\n",
    ")\n",
    "```\n",
    "\n",
    "### ✳️ 4. Prompt is filled:\n",
    "\n",
    "```text\n",
    "Answer the question using only the context provided.\n",
    "Context:\n",
    "Keratin is a structural protein found in hair, nails, and skin.\n",
    "In hair, keratin forms protective layers and prevents breakage.\n",
    "\n",
    "Question:\n",
    "What does keratin do in hair?\n",
    "```\n",
    "\n",
    "### ✳️ 5. This full prompt is sent to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Recap: How `{context}` and `{input}` Get Their Values\n",
    "\n",
    "| Variable    | Filled By                                                             | Content                    |\n",
    "| ----------- | --------------------------------------------------------------------- | -------------------------- |\n",
    "| `{input}`   | `invoke({\"input\": ...})`                                              | The user’s question        |\n",
    "| `{context}` | From retriever → documents → joined by `create_stuff_documents_chain` | Combined document contents |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Scenario to Remember This\n",
    "\n",
    "Imagine you are a librarian chatbot. When a user asks:\n",
    "\n",
    "> \"What is the role of keratin in hair?\"\n",
    "\n",
    "You:\n",
    "\n",
    "1. Search your book database using the retriever (vector search)\n",
    "2. Grab the most relevant pages (documents)\n",
    "3. Combine those pages into a `context`\n",
    "4. Ask your internal assistant (LLM) to answer using just this context\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Bonus Tip: Visualizing Prompt Fill\n",
    "\n",
    "You can manually simulate the filling for debugging:\n",
    "\n",
    "```python\n",
    "retrieved_docs = retriever.invoke(\"What does keratin do in hair?\")\n",
    "context_text = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "filled_prompt = prompt.format(\n",
    "    input=\"What does keratin do in hair?\",\n",
    "    context=context_text\n",
    ")\n",
    "\n",
    "print(\"Filled Prompt:\")\n",
    "print(filled_prompt)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee48ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
