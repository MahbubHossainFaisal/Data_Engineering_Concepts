{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a1c145",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  What is a LangSmith Experiment?\n",
    "\n",
    "### âœ… Definition:\n",
    "\n",
    "A **LangSmith Experiment** is a **test run** of your LLM app **against a dataset**, where **each input is run through your app**, and the **outputs are evaluated** by one or more **evaluators**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Diagram: Experiment Lifecycle\n",
    "\n",
    "```\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚ Dataset      â”‚ (List of examples: input + expected output)\n",
    "       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " â”‚  LLM Application     â”‚ (Your chain/tool/function)\n",
    " â”‚  Runs on each input  â”‚\n",
    " â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚ Each run produces...\n",
    "       â–¼\n",
    " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " â”‚ Prediction Output    â”‚\n",
    " â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼\n",
    " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " â”‚   Evaluators (Auto or Custom)  â”‚\n",
    " â”‚ Compare Prediction vs Golden   â”‚\n",
    " â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Scores + Feedback    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§ª Full Experiment Scenario\n",
    "\n",
    "Letâ€™s break it down in plain terms:\n",
    "\n",
    "### ğŸ”¸ Step 1: Prepare your dataset\n",
    "\n",
    "âœ… A list of test examples:\n",
    "\n",
    "```json\n",
    "{ \"input\": \"What is the capital of Japan?\", \"expected_output\": \"Tokyo\" }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¸ Step 2: Run your app over the dataset\n",
    "\n",
    "For each input, your app (LLM chain, tool, or function) will:\n",
    "\n",
    "* Run using that input\n",
    "* Create a **Run**\n",
    "* Log the `output`\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¸ Step 3: Evaluate the output\n",
    "\n",
    "Evaluators (defined via UI or Python) compare:\n",
    "\n",
    "* `prediction (output)`\n",
    "* `reference (golden output)`\n",
    "\n",
    "Evaluators then return:\n",
    "\n",
    "* `score`\n",
    "* `reasoning`\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Evaluators in Experiments\n",
    "\n",
    "### âœ… You can attach evaluators in two ways:\n",
    "\n",
    "| Method                  | How it Works                                     |\n",
    "| ----------------------- | ------------------------------------------------ |\n",
    "| **Auto-Evaluator (UI)** | Select in LangSmith UI from dropdown (no coding) |\n",
    "| **Custom Code**         | Define in Python and attach using SDK            |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Python Code to Run Experiment Locally\n",
    "\n",
    "Here's a **real example** using the LangSmith SDK:\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import ExactMatchEvaluator\n",
    "\n",
    "# Initialize\n",
    "client = Client()\n",
    "\n",
    "# Define evaluator(s)\n",
    "exact_match = ExactMatchEvaluator()\n",
    "\n",
    "# Define dataset and model (chain, tool, or wrapper)\n",
    "dataset_name = \"qa-dataset-v1\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "# Run experiment\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    model=model,\n",
    "    evaluation=[exact_match],\n",
    "    project_name=\"qa-experiment-01\"\n",
    ")\n",
    "```\n",
    "\n",
    "> ğŸ§  You can attach multiple evaluators to get richer feedback.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Running Experiments on Dataset **Versions / Splits**\n",
    "\n",
    "### âœ… Why?\n",
    "\n",
    "* Different versions may test different prompt variants\n",
    "* Dataset splits (e.g., `critical`, `edge`, `easy`) help you test smarter\n",
    "\n",
    "### ğŸ“Œ Example:\n",
    "\n",
    "Run only on version \"v2\" of dataset with tag `\"edge-cases\"`:\n",
    "\n",
    "```python\n",
    "client.run_on_dataset(\n",
    "    dataset_name=\"qa-dataset-v2\",\n",
    "    model=model,\n",
    "    evaluation=[exact_match],\n",
    "    project_name=\"qa-edge-eval\",\n",
    "    dataset_filters={\"tags\": [\"edge-cases\"]}\n",
    ")\n",
    "```\n",
    "---\n",
    "\n",
    "## ğŸ§  Must-Know Reflections:\n",
    "\n",
    "1. âœ… Whatâ€™s the benefit of attaching multiple evaluators to one experiment?\n",
    "2. âœ… How do dataset versions or tags help in testing safely?\n",
    "3. âœ… Why would you test only a subset of your examples (like edge cases)?\n",
    "4. âœ… Whatâ€™s the role of \"project\\_name\" when running an experiment?\n",
    "5. âœ… Can you re-use the same evaluators for different experiments? (â†’ Yes!)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e2550",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### âœ… **1. Whatâ€™s the benefit of attaching multiple evaluators to one experiment?**\n",
    "\n",
    "#### âœ… Answer:\n",
    "\n",
    "Attaching multiple evaluators gives you a **multi-dimensional view** of your LLM applicationâ€™s performance.\n",
    "\n",
    "| Evaluator Type       | What It Evaluates                                  |\n",
    "| -------------------- | -------------------------------------------------- |\n",
    "| `ExactMatch`         | Precise correctness                                |\n",
    "| `SemanticSimilarity` | Meaningful similarity, even if phrased differently |\n",
    "| `LLM-as-Judge`       | Subjective qualities like helpfulness, clarity     |\n",
    "| `Custom`             | Domain-specific metrics (e.g., legal correctness)  |\n",
    "\n",
    "ğŸ” **Why important?**\n",
    "Sometimes one metric is misleading â€” e.g., exact match fails even when the answer is good. Combining evaluators gives a **more reliable, nuanced understanding**.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **2. How do dataset versions or tags help in testing safely?**\n",
    "\n",
    "#### âœ… Answer:\n",
    "\n",
    "**Tags and versions** help you:\n",
    "\n",
    "* Test specific groups of examples (e.g., only â€œedge-casesâ€)\n",
    "* Track evaluation changes over time (e.g., v1 â†’ v2 â†’ v3)\n",
    "* Avoid running full datasets unnecessarily\n",
    "* **Pin evaluations to specific versions**, ensuring reproducibility\n",
    "\n",
    "ğŸ§  **Key idea:**\n",
    "Without versions, you might accidentally evaluate on outdated or changed data, leading to **misleading results**.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **3. Why would you test only a subset of your examples (like edge cases)?**\n",
    "\n",
    "#### âœ… Answer:\n",
    "\n",
    "Subsets (using tags or splits) let you:\n",
    "\n",
    "* Focus on **high-impact or high-risk scenarios** (e.g., medical or legal)\n",
    "* Run fast evaluations when time is limited\n",
    "* Identify if a new prompt/model **solves hard problems**\n",
    "\n",
    "ğŸ¯ **Example:**\n",
    "You add a new RAG prompt â€” instead of testing 1,000 examples, you tag 50 known â€œtrickyâ€ ones as `critical` and test those first.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **4. Whatâ€™s the role of `\"project_name\"` when running an experiment?**\n",
    "\n",
    "#### âœ… Answer:\n",
    "\n",
    "`project_name` in LangSmith:\n",
    "\n",
    "* **Groups your experiment runs**\n",
    "* Lets you **track results and scores** in the LangSmith dashboard\n",
    "* Helps compare experiments side by side (e.g., `prompt-v1` vs `prompt-v2`)\n",
    "* Makes it easy to **filter logs, scores, and traces**\n",
    "\n",
    "ğŸ’¡ Best Practice: Use descriptive project names like `\"summarization-v3-eval\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **5. Can you re-use the same evaluators for different experiments?**\n",
    "\n",
    "#### âœ… Answer:\n",
    "\n",
    "Yes â€” and **you should**!\n",
    "\n",
    "âœ… Evaluators like `ExactMatchEvaluator`, `SemanticSimilarity`, `LLM-as-Judge`, or your own custom logic are **modular** and **reusable**.\n",
    "\n",
    "This helps you:\n",
    "\n",
    "* Save time\n",
    "* Maintain consistency across experiments\n",
    "* Benchmark models across datasets using the **same evaluation criteria**\n",
    "\n",
    "ğŸ“Œ Example:\n",
    "\n",
    "```python\n",
    "semantic = RunEvaluator.for_type(\"semantic_similarity\")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name=\"news-summary-v1\",\n",
    "    model=\"gpt-4\",\n",
    "    evaluation=[semantic],\n",
    "    project_name=\"summarization-semantic-eval\"\n",
    ")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c521d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
