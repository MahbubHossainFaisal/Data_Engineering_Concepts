{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a1c145",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 🧠 What is a LangSmith Experiment?\n",
    "\n",
    "### ✅ Definition:\n",
    "\n",
    "A **LangSmith Experiment** is a **test run** of your LLM app **against a dataset**, where **each input is run through your app**, and the **outputs are evaluated** by one or more **evaluators**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Diagram: Experiment Lifecycle\n",
    "\n",
    "```\n",
    "       ┌──────────────┐\n",
    "       │ Dataset      │ (List of examples: input + expected output)\n",
    "       └─────┬────────┘\n",
    "             │\n",
    "             ▼\n",
    " ┌──────────────────────┐\n",
    " │  LLM Application     │ (Your chain/tool/function)\n",
    " │  Runs on each input  │\n",
    " └─────┬────────────────┘\n",
    "       │ Each run produces...\n",
    "       ▼\n",
    " ┌──────────────────────┐\n",
    " │ Prediction Output    │\n",
    " └─────┬────────────────┘\n",
    "       │\n",
    "       ▼\n",
    " ┌────────────────────────────────┐\n",
    " │   Evaluators (Auto or Custom)  │\n",
    " │ Compare Prediction vs Golden   │\n",
    " └────────────┬───────────────────┘\n",
    "              ▼\n",
    "    ┌──────────────────────┐\n",
    "    │ Scores + Feedback    │\n",
    "    └──────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 Full Experiment Scenario\n",
    "\n",
    "Let’s break it down in plain terms:\n",
    "\n",
    "### 🔸 Step 1: Prepare your dataset\n",
    "\n",
    "✅ A list of test examples:\n",
    "\n",
    "```json\n",
    "{ \"input\": \"What is the capital of Japan?\", \"expected_output\": \"Tokyo\" }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 Step 2: Run your app over the dataset\n",
    "\n",
    "For each input, your app (LLM chain, tool, or function) will:\n",
    "\n",
    "* Run using that input\n",
    "* Create a **Run**\n",
    "* Log the `output`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 Step 3: Evaluate the output\n",
    "\n",
    "Evaluators (defined via UI or Python) compare:\n",
    "\n",
    "* `prediction (output)`\n",
    "* `reference (golden output)`\n",
    "\n",
    "Evaluators then return:\n",
    "\n",
    "* `score`\n",
    "* `reasoning`\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Evaluators in Experiments\n",
    "\n",
    "### ✅ You can attach evaluators in two ways:\n",
    "\n",
    "| Method                  | How it Works                                     |\n",
    "| ----------------------- | ------------------------------------------------ |\n",
    "| **Auto-Evaluator (UI)** | Select in LangSmith UI from dropdown (no coding) |\n",
    "| **Custom Code**         | Define in Python and attach using SDK            |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Python Code to Run Experiment Locally\n",
    "\n",
    "Here's a **real example** using the LangSmith SDK:\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import ExactMatchEvaluator\n",
    "\n",
    "# Initialize\n",
    "client = Client()\n",
    "\n",
    "# Define evaluator(s)\n",
    "exact_match = ExactMatchEvaluator()\n",
    "\n",
    "# Define dataset and model (chain, tool, or wrapper)\n",
    "dataset_name = \"qa-dataset-v1\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "# Run experiment\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    model=model,\n",
    "    evaluation=[exact_match],\n",
    "    project_name=\"qa-experiment-01\"\n",
    ")\n",
    "```\n",
    "\n",
    "> 🧠 You can attach multiple evaluators to get richer feedback.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Running Experiments on Dataset **Versions / Splits**\n",
    "\n",
    "### ✅ Why?\n",
    "\n",
    "* Different versions may test different prompt variants\n",
    "* Dataset splits (e.g., `critical`, `edge`, `easy`) help you test smarter\n",
    "\n",
    "### 📌 Example:\n",
    "\n",
    "Run only on version \"v2\" of dataset with tag `\"edge-cases\"`:\n",
    "\n",
    "```python\n",
    "client.run_on_dataset(\n",
    "    dataset_name=\"qa-dataset-v2\",\n",
    "    model=model,\n",
    "    evaluation=[exact_match],\n",
    "    project_name=\"qa-edge-eval\",\n",
    "    dataset_filters={\"tags\": [\"edge-cases\"]}\n",
    ")\n",
    "```\n",
    "---\n",
    "\n",
    "## 🧠 Must-Know Reflections:\n",
    "\n",
    "1. ✅ What’s the benefit of attaching multiple evaluators to one experiment?\n",
    "2. ✅ How do dataset versions or tags help in testing safely?\n",
    "3. ✅ Why would you test only a subset of your examples (like edge cases)?\n",
    "4. ✅ What’s the role of \"project\\_name\" when running an experiment?\n",
    "5. ✅ Can you re-use the same evaluators for different experiments? (→ Yes!)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e2550",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ✅ **1. What’s the benefit of attaching multiple evaluators to one experiment?**\n",
    "\n",
    "#### ✅ Answer:\n",
    "\n",
    "Attaching multiple evaluators gives you a **multi-dimensional view** of your LLM application’s performance.\n",
    "\n",
    "| Evaluator Type       | What It Evaluates                                  |\n",
    "| -------------------- | -------------------------------------------------- |\n",
    "| `ExactMatch`         | Precise correctness                                |\n",
    "| `SemanticSimilarity` | Meaningful similarity, even if phrased differently |\n",
    "| `LLM-as-Judge`       | Subjective qualities like helpfulness, clarity     |\n",
    "| `Custom`             | Domain-specific metrics (e.g., legal correctness)  |\n",
    "\n",
    "🔍 **Why important?**\n",
    "Sometimes one metric is misleading — e.g., exact match fails even when the answer is good. Combining evaluators gives a **more reliable, nuanced understanding**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **2. How do dataset versions or tags help in testing safely?**\n",
    "\n",
    "#### ✅ Answer:\n",
    "\n",
    "**Tags and versions** help you:\n",
    "\n",
    "* Test specific groups of examples (e.g., only “edge-cases”)\n",
    "* Track evaluation changes over time (e.g., v1 → v2 → v3)\n",
    "* Avoid running full datasets unnecessarily\n",
    "* **Pin evaluations to specific versions**, ensuring reproducibility\n",
    "\n",
    "🧠 **Key idea:**\n",
    "Without versions, you might accidentally evaluate on outdated or changed data, leading to **misleading results**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **3. Why would you test only a subset of your examples (like edge cases)?**\n",
    "\n",
    "#### ✅ Answer:\n",
    "\n",
    "Subsets (using tags or splits) let you:\n",
    "\n",
    "* Focus on **high-impact or high-risk scenarios** (e.g., medical or legal)\n",
    "* Run fast evaluations when time is limited\n",
    "* Identify if a new prompt/model **solves hard problems**\n",
    "\n",
    "🎯 **Example:**\n",
    "You add a new RAG prompt — instead of testing 1,000 examples, you tag 50 known “tricky” ones as `critical` and test those first.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **4. What’s the role of `\"project_name\"` when running an experiment?**\n",
    "\n",
    "#### ✅ Answer:\n",
    "\n",
    "`project_name` in LangSmith:\n",
    "\n",
    "* **Groups your experiment runs**\n",
    "* Lets you **track results and scores** in the LangSmith dashboard\n",
    "* Helps compare experiments side by side (e.g., `prompt-v1` vs `prompt-v2`)\n",
    "* Makes it easy to **filter logs, scores, and traces**\n",
    "\n",
    "💡 Best Practice: Use descriptive project names like `\"summarization-v3-eval\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **5. Can you re-use the same evaluators for different experiments?**\n",
    "\n",
    "#### ✅ Answer:\n",
    "\n",
    "Yes — and **you should**!\n",
    "\n",
    "✅ Evaluators like `ExactMatchEvaluator`, `SemanticSimilarity`, `LLM-as-Judge`, or your own custom logic are **modular** and **reusable**.\n",
    "\n",
    "This helps you:\n",
    "\n",
    "* Save time\n",
    "* Maintain consistency across experiments\n",
    "* Benchmark models across datasets using the **same evaluation criteria**\n",
    "\n",
    "📌 Example:\n",
    "\n",
    "```python\n",
    "semantic = RunEvaluator.for_type(\"semantic_similarity\")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name=\"news-summary-v1\",\n",
    "    model=\"gpt-4\",\n",
    "    evaluation=[semantic],\n",
    "    project_name=\"summarization-semantic-eval\"\n",
    ")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c521d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
