{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7625485",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🧠 Before We Begin: Quick Overview of LangSmith\n",
    "\n",
    "**LangSmith** is an observability and debugging platform for **LLM (Large Language Model)** applications. It allows you to **trace**, **evaluate**, and **optimize** the logic and behavior of your app’s LLM calls.\n",
    "\n",
    "Think of LangSmith like a **\"Black Box Debugger\"** for LLM apps — especially useful in **multi-step workflows** like RAG, agents, chains, tools, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Let’s Begin Step-by-Step Like a Good Teacher\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 Part 1: **LangSmith Tracing Basics**\n",
    "\n",
    "### 🧪 What is \"Tracing\" in LangSmith?\n",
    "\n",
    "**Tracing** in LangSmith refers to **monitoring the execution flow** of your application, especially when it includes LLM calls.\n",
    "\n",
    "Every time a user **runs** your app (like asking a question), LangSmith can **trace**:\n",
    "\n",
    "* What inputs were used?\n",
    "* What functions were called?\n",
    "* How much time each part took?\n",
    "* What outputs were generated?\n",
    "* What was the intermediate state?\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Diagram: How LangSmith Trace Works (Conceptual Visualization)\n",
    "\n",
    "```\n",
    "User Input: “What are the benefits of Vitamin D?”\n",
    "\n",
    "┌──────────────┐\n",
    "│ Root Run     │ <--- A new trace is started!\n",
    "└─────┬────────┘\n",
    "      │\n",
    "      ├─> Retrieve Documents Run (e.g., from vector DB)\n",
    "      │    └─> Sub Run: Embedding Calculation\n",
    "      │    └─> Sub Run: Vector Similarity Search\n",
    "      │\n",
    "      └─> Generate Response Run (LLM generates reply)\n",
    "           └─> Sub Run: Prompt Formatting\n",
    "           └─> Sub Run: LLM Call to OpenAI/GPT\n",
    "```\n",
    "\n",
    "Each \"Run\" logs **metadata**, input/output, duration, errors, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Part 2: **Tracing with `@traceable` Decorator**\n",
    "\n",
    "LangSmith uses the `@traceable` decorator from the `langsmith.traceable` module.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 What is `@traceable`?\n",
    "\n",
    "It’s a **Python decorator** that you place above your functions to:\n",
    "\n",
    "* Mark them as **traceable** in LangSmith.\n",
    "* Log all inputs, outputs, and any **metadata** during execution.\n",
    "* Automatically **create nested traces** (or runs) under the root trace.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Purpose of `@traceable`\n",
    "\n",
    "| Purpose                       | Description                                                      |\n",
    "| ----------------------------- | ---------------------------------------------------------------- |\n",
    "| Enable function-level tracing | Logs how a function behaves inside a large app run               |\n",
    "| Debug faster                  | Helps you identify where logic failed or which response failed   |\n",
    "| View call hierarchy           | Shows **which function called what**, and how long each took     |\n",
    "| Trace custom logic            | You can trace **non-LLM parts**, like vector search or filtering |\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Example:\n",
    "\n",
    "```python\n",
    "from langsmith.traceable import traceable\n",
    "\n",
    "@traceable(name=\"retrieve_docs\", tags=[\"vector-search\"])\n",
    "def retrieve_documents(query: str):\n",
    "    # your logic\n",
    "    return documents\n",
    "```\n",
    "\n",
    "* This will show in LangSmith as a nested run inside the trace.\n",
    "* You’ll be able to click on “retrieve\\_docs” and inspect inputs/outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 Part 3: **Trace = Nested & Recursive Runs**\n",
    "\n",
    "LangSmith organizes a trace as a **tree of runs**.\n",
    "\n",
    "### ✅ Key Terms:\n",
    "\n",
    "| Term           | Description                                                            |\n",
    "| -------------- | ---------------------------------------------------------------------- |\n",
    "| **Trace**      | The overall execution instance (e.g., one user query = one trace)      |\n",
    "| **Root Run**   | The top-most call in a trace                                           |\n",
    "| **Run**        | Any function (decorated or not) that’s tracked by LangSmith            |\n",
    "| **Nested Run** | A run inside another run (like a child function call)                  |\n",
    "| **Recursive**  | Runs can contain more runs inside them (e.g., chain of calls or loops) |\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Example Scenario: Tracing in RAG App\n",
    "\n",
    "Let’s say a user asks:\n",
    "\n",
    "> \"Tell me about Albert Einstein's Nobel Prize.\"\n",
    "\n",
    "LangSmith will trace:\n",
    "\n",
    "1. **Root Run** = Entire user session\n",
    "2. `retrieve_documents()` = Retrieves relevant docs\n",
    "3. `generate_response()` = Generates the final answer\n",
    "4. Each of the above can call:\n",
    "\n",
    "   * Embedding calculation\n",
    "   * Vector DB call\n",
    "   * Prompt formatting\n",
    "   * LLM API call (e.g., OpenAI)\n",
    "\n",
    "All of this becomes visible in LangSmith — like **X-ray vision** into your GenAI app.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧷 Part 4: **Adding Metadata with `@traceable`**\n",
    "\n",
    "LangSmith allows you to **add metadata** to any run, which becomes super useful for:\n",
    "\n",
    "* **Debugging**\n",
    "* **Filtering traces later**\n",
    "* **Passing context**\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Example with Metadata\n",
    "\n",
    "```python\n",
    "@traceable(\n",
    "    name=\"retrieve_documents\",\n",
    "    metadata={\"retriever\": \"FAISS\", \"top_k\": 5},\n",
    "    tags=[\"retrieval\", \"vector\"]\n",
    ")\n",
    "def retrieve_documents(query: str):\n",
    "    # retrieval logic\n",
    "    return docs\n",
    "```\n",
    "\n",
    "### ✅ Why Add Metadata?\n",
    "\n",
    "| Benefit           | Description                                                                |\n",
    "| ----------------- | -------------------------------------------------------------------------- |\n",
    "| Adds context      | You can record what type of retriever or parameters were used              |\n",
    "| Enables filtering | Easily find all traces using `\"retriever\": \"FAISS\"`                        |\n",
    "| Helps evaluation  | Later, you can compare traces by different retrievers, prompt templates... |\n",
    "| Easy debugging    | Know what configurations led to what outputs                               |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧬 Part 5: **What is Metadata Passing at Runtime?**\n",
    "\n",
    "Sometimes you **don’t want to hardcode** metadata. Instead, you want to pass it dynamically at **runtime**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Example: Metadata Passing at Runtime\n",
    "\n",
    "```python\n",
    "@traceable(name=\"generate_response\")\n",
    "def generate_response(prompt, model_name=None, **kwargs):\n",
    "    metadata = {\"model_used\": model_name}\n",
    "    # Use LangSmith runtime context\n",
    "    from langsmith.run_helpers import get_current_run_tree\n",
    "    current_run = get_current_run_tree()\n",
    "    current_run.add_metadata(metadata)\n",
    "    # Generate response\n",
    "    ...\n",
    "```\n",
    "\n",
    "Now, this metadata will show up in your trace dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Why is Runtime Metadata Important?\n",
    "\n",
    "| Need                    | Benefit                                                           |\n",
    "| ----------------------- | ----------------------------------------------------------------- |\n",
    "| Config-driven logic     | Model, temperature, retriever might vary based on config          |\n",
    "| Multi-model experiments | Track which model is used during A/B testing                      |\n",
    "| Dynamic parameters      | Useful when passing top\\_k, document filter, custom scoring logic |\n",
    "| Better debugging        | Understand runtime decisions per trace                            |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Must-Know Questions (For Deep Understanding)\n",
    "\n",
    "1. **What is the difference between a trace and a run in LangSmith?**\n",
    "2. **What happens if you don’t decorate a function with `@traceable` in LangSmith?**\n",
    "3. **Why is nesting of runs important in LangSmith?**\n",
    "4. **How can LangSmith tracing help debug a faulty RAG application?**\n",
    "5. **What’s the benefit of attaching metadata to a run?**\n",
    "6. **How does LangSmith handle recursive or looped logic in tracing?**\n",
    "7. **How would you trace a multi-hop agent-based application in LangSmith?**\n",
    "8. **Can you add dynamic metadata in LangSmith? How?**\n",
    "9. **How does LangSmith help optimize latency or cost in GenAI apps?**\n",
    "10. **Give an example where metadata helped identify a bottleneck in RAG logic.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255a35b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🧠 1. **What is the difference between a trace and a run in LangSmith?**\n",
    "\n",
    "| Term      | Description                                                                        |\n",
    "| --------- | ---------------------------------------------------------------------------------- |\n",
    "| **Trace** | A **single execution session** of your GenAI app (e.g., user asks a question)      |\n",
    "| **Run**   | An **individual function call** or logic block within that session (can be nested) |\n",
    "\n",
    "📌 **Analogy**:\n",
    "\n",
    "* A **trace** is the full movie.\n",
    "* Each **run** is a scene in the movie.\n",
    "* Some scenes have **sub-scenes** (nested runs).\n",
    "\n",
    "📍 **Example**:\n",
    "\n",
    "* **Trace**: User asks “Who won the 2022 World Cup?”\n",
    "* **Runs**:\n",
    "\n",
    "  * `retrieve_documents()`\n",
    "\n",
    "    * Sub-run: `faiss_similarity_search()`\n",
    "  * `generate_response()`\n",
    "\n",
    "    * Sub-run: `openai_completion_call()`\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 2. **What happens if you don’t decorate a function with `@traceable` in LangSmith?**\n",
    "\n",
    "That function's behavior won’t be explicitly visible in the LangSmith UI unless it's:\n",
    "\n",
    "* A chain/tool/LLM call supported natively by LangChain\n",
    "* Or manually traced via API\n",
    "\n",
    "🚫 **You miss visibility** into that logic:\n",
    "\n",
    "* No inputs/outputs shown\n",
    "* No performance stats\n",
    "* No metadata context\n",
    "* Harder debugging\n",
    "\n",
    "📍**Example**:\n",
    "You forget to trace `score_documents()` — and you can't figure out why the final answer is off. If it were traceable, you'd immediately see scoring was misbehaving.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 3. **Why is nesting of runs important in LangSmith?**\n",
    "\n",
    "Nested runs give you a **tree structure** of logic that helps in:\n",
    "\n",
    "✅ Understanding full execution\n",
    "✅ Debugging logic step-by-step\n",
    "✅ Profiling time spent at each level\n",
    "✅ Tracking logic dependencies\n",
    "\n",
    "📍 **Example**:\n",
    "\n",
    "```\n",
    "Root: User Query\n",
    " └── retrieve_documents\n",
    "     └── embedding_model_call\n",
    "     └── vector_search_call\n",
    " └── generate_response\n",
    "     └── format_prompt\n",
    "     └── call_llm\n",
    "```\n",
    "\n",
    "If the root trace fails, you can walk down the tree to find **which node (run)** broke.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 4. **How can LangSmith tracing help debug a faulty RAG application?**\n",
    "\n",
    "LangSmith tracing provides:\n",
    "\n",
    "* **Input/output visibility**: Did the query go wrong? Did the vector DB return bad docs?\n",
    "* **Intermediate states**: What did the prompt look like?\n",
    "* **Duration of each step**: Which part was slow?\n",
    "* **Metadata**: Was it using the right retriever/config?\n",
    "\n",
    "📍 **Example**:\n",
    "A user query is generating a hallucinated answer.\n",
    "With tracing:\n",
    "\n",
    "* You realize `retrieve_documents()` returned nothing.\n",
    "* Why? `embedding_model` failed due to missing API key (visible in the sub-run error trace).\n",
    "\n",
    "✅ Without tracing, you'd just see \"LLM gave a bad answer.\"\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 5. **What’s the benefit of attaching metadata to a run?**\n",
    "\n",
    "Metadata helps you:\n",
    "\n",
    "* **Tag and filter** runs later\n",
    "* Track which **model/retriever/version** was used\n",
    "* Capture runtime config (like top\\_k, temperature)\n",
    "* Debug quickly by seeing context\n",
    "\n",
    "📍**Example**:\n",
    "\n",
    "```python\n",
    "@traceable(metadata={\"retriever\": \"FAISS\", \"embedding_model\": \"text-embedding-ada-002\"})\n",
    "```\n",
    "\n",
    "Now when performance drops, you can filter all traces that used `FAISS` retriever and compare them with `Weaviate`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 6. **How does LangSmith handle recursive or looped logic in tracing?**\n",
    "\n",
    "LangSmith traces nested calls recursively — whether **looped** or **recursively invoked functions** — and visualizes each as a **child run** under its parent.\n",
    "\n",
    "📍 **Example**:\n",
    "In an agent that retries 3 times:\n",
    "\n",
    "* LangSmith will trace all 3 tries as separate sub-runs\n",
    "* You can inspect each retry input/output pair\n",
    "\n",
    "It’s super useful for debugging tools or agents that make decisions based on feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 7. **How would you trace a multi-hop agent-based application in LangSmith?**\n",
    "\n",
    "Multi-hop agents make decisions in steps — and LangSmith is **built for this**.\n",
    "\n",
    "You’d trace:\n",
    "\n",
    "* Root run = agent start\n",
    "* Each **tool call** or **decision loop** = a child run\n",
    "* You can inspect:\n",
    "\n",
    "  * What input went into tool\n",
    "  * What came back\n",
    "  * What decision agent took next\n",
    "\n",
    "📍 **Case**:\n",
    "LangChain agent uses:\n",
    "\n",
    "1. Google search\n",
    "2. Wikipedia tool\n",
    "3. LLM summarizer\n",
    "\n",
    "LangSmith will show this trace step-by-step so you can debug incorrect hops.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 8. **Can you add dynamic metadata in LangSmith? How?**\n",
    "\n",
    "Yes!\n",
    "\n",
    "You use:\n",
    "\n",
    "```python\n",
    "from langsmith.run_helpers import get_current_run_tree\n",
    "\n",
    "run = get_current_run_tree()\n",
    "run.add_metadata({\"experiment_group\": \"prompt_v2\"})\n",
    "```\n",
    "\n",
    "✅ Helps when:\n",
    "\n",
    "* You want to tag runs based on **user config**\n",
    "* You’re A/B testing at runtime\n",
    "* Metadata is **not known at function definition time**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 9. **How does LangSmith help optimize latency or cost in GenAI apps?**\n",
    "\n",
    "LangSmith provides performance insights:\n",
    "\n",
    "* How long each run took\n",
    "* Cost of LLM calls (if integrated with OpenAI tracking)\n",
    "* Slow functions or duplicate logic\n",
    "\n",
    "📍 Example:\n",
    "You realize:\n",
    "\n",
    "* Retrieval is taking 0.5s\n",
    "* Prompt formatting is inefficient\n",
    "* LLM call is done twice due to bug\n",
    "\n",
    "✅ You can fix these by **observing traces**, not guessing.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 10. **Give an example where metadata helped identify a bottleneck in RAG logic.**\n",
    "\n",
    "📍 **Scenario**:\n",
    "You tag retriever runs with:\n",
    "\n",
    "```python\n",
    "@traceable(metadata={\"retriever\": \"FAISS\", \"embedding_model\": \"sentence-transformers-mpnet\"})\n",
    "```\n",
    "\n",
    "Later, you filter all traces with `retriever=FAISS` and compare:\n",
    "\n",
    "* Some runs return docs\n",
    "* Some return nothing\n",
    "\n",
    "💡 You discover that **queries with length > 100 tokens** are getting 0 results.\n",
    "\n",
    "So, your metadata lets you:\n",
    "\n",
    "* Identify the edge case\n",
    "* Fix vector DB filtering logic\n",
    "* Improve recall by 20%\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5368759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
