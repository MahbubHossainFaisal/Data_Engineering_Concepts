{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42d0217",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔵 1. What Do **Evaluators** Do in LangSmith?\n",
    "\n",
    "### ✅ Simple Explanation:\n",
    "\n",
    "Evaluators **measure the quality** of your LLM’s output by **comparing it with the expected output** (a.k.a. the golden output). They generate scores (like accuracy or relevance) and **reasoning**, so you know if your app is working well.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 2. What Problem Do They Solve?\n",
    "\n",
    "LLM outputs can:\n",
    "\n",
    "* ✅ Be good but not exact matches.\n",
    "* ❌ Miss subtle facts.\n",
    "* 🌀 Drift after prompt/model changes.\n",
    "\n",
    "So, without **automated, repeatable tests**, you risk:\n",
    "\n",
    "* Shipping regressions\n",
    "* Losing quality\n",
    "* Wasting time in manual checks\n",
    "\n",
    "Evaluators solve this by making your LLM app **testable like traditional software**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 3. How Do Evaluators Work?\n",
    "\n",
    "### 🔁 The Evaluation Flow:\n",
    "\n",
    "Each evaluator receives:\n",
    "\n",
    "* `inputs`: e.g., a user question\n",
    "* `prediction`: LLM-generated output\n",
    "* `reference`: Golden expected output\n",
    "\n",
    "And returns:\n",
    "\n",
    "* `score`: A number (0–1, or 1–5)\n",
    "* `reasoning`: Optional comment explaining the score\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 4. Visual Diagram of Evaluation Flow\n",
    "\n",
    "```\n",
    "                ┌─────────────────────┐\n",
    "                │  LangSmith Dataset  │\n",
    "                └────────┬────────────┘\n",
    "                         │\n",
    "            ┌────────────▼────────────┐\n",
    "            │ Your LLM App (Chain)    │\n",
    "            │ (Input → Output)        │\n",
    "            └────────────┬────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                ┌─────────────────────┐\n",
    "                │ Evaluator(s)        │\n",
    "                │  • Exact Match      │\n",
    "                │  • Similarity       │\n",
    "                │  • LLM-as-Judge     │\n",
    "                │  • Custom Code      │\n",
    "                └────────┬────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                ┌─────────────────────┐\n",
    "                │ Evaluation Results  │\n",
    "                │  • Score            │\n",
    "                │  • Reasoning        │\n",
    "                └─────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 5. Using Multiple Evaluators\n",
    "\n",
    "You can attach multiple evaluators in a single run.\n",
    "\n",
    "### 💡 Example:\n",
    "\n",
    "Evaluate chatbot answers with:\n",
    "\n",
    "* `ExactMatch`: to check deterministic tasks\n",
    "* `LLM-as-Judge`: to judge helpfulness\n",
    "* `SemanticSimilarity`: to check paraphrased but valid answers\n",
    "\n",
    "This gives you a **multi-dimensional view** of performance.\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import ExactMatchEvaluator, RunEvaluator\n",
    "\n",
    "client = Client()\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name=\"rag-qa-testset\",\n",
    "    evaluation=[\n",
    "        ExactMatchEvaluator(),\n",
    "        RunEvaluator.for_type(\"semantic_similarity\"),\n",
    "        RunEvaluator.for_type(\"llm_qa\")  # LLM-as-Judge\n",
    "    ],\n",
    "    model=\"gpt-4\",\n",
    "    project_name=\"test-qa-eval\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 6. Custom Evaluator Function in Python\n",
    "\n",
    "```python\n",
    "from langsmith.evaluation import RunEvaluator\n",
    "\n",
    "def contains_keyword(example, prediction, reference):\n",
    "    keywords = [\"diabetes\", \"insulin\", \"glucose\"]\n",
    "    found = any(k in prediction.lower() for k in keywords)\n",
    "    return {\n",
    "        \"score\": 1.0 if found else 0.0,\n",
    "        \"reasoning\": f\"Keywords found: {found}\"\n",
    "    }\n",
    "\n",
    "my_evaluator = RunEvaluator(\n",
    "    name=\"medical_keyword_checker\",\n",
    "    evaluation_fn=contains_keyword\n",
    ")\n",
    "```\n",
    "\n",
    "This is useful when you want **custom logic** that’s not covered by built-in evaluators.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 7. Types of Evaluators & How They Work in LangSmith UI\n",
    "\n",
    "### ✅ A. **LLM-as-Judge Evaluation**\n",
    "\n",
    "* Uses a separate LLM to **compare prediction and reference**\n",
    "* Judge gives:\n",
    "\n",
    "  * Score (0-1, 1-5, Yes/No)\n",
    "  * Reasoning\n",
    "* Useful for:\n",
    "\n",
    "  * Summaries\n",
    "  * Open-ended questions\n",
    "  * Style/tone\n",
    "\n",
    "### ✅ B. **Custom Code Evaluation**\n",
    "\n",
    "* Write your own Python logic (see above)\n",
    "* Use SDK or upload via UI\n",
    "* Evaluates inputs in your defined way\n",
    "\n",
    "### ✅ C. **LangSmith UI Usage**\n",
    "\n",
    "You can:\n",
    "\n",
    "* Attach built-in evaluators from dropdown\n",
    "* Configure parameters (scoring system, prompt)\n",
    "* Use your own LLM judge prompt\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 8. Auto Evaluators in LangSmith\n",
    "\n",
    "LangSmith’s **Auto Evaluators** are **ready-to-use evaluator templates** that:\n",
    "\n",
    "* Require **no coding**\n",
    "* Are powered by **LLMs**\n",
    "* Handle common evaluation needs (factuality, helpfulness, etc.)\n",
    "\n",
    "### ✅ Features:\n",
    "\n",
    "* Prebuilt prompts and scoring logic\n",
    "* Support multiple output types (text, JSON, etc.)\n",
    "* Configurable scoring range\n",
    "* Can be fine-tuned with **prompt override**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 9. Prompt Types in Auto Evaluators\n",
    "\n",
    "When you configure an LLM-based Auto Evaluator, you select:\n",
    "\n",
    "* `binary`: Yes/No → score 0 or 1\n",
    "* `scale`: Score from 1–5 (or other scale)\n",
    "* `categorical`: e.g., \\[\"Good\", \"Needs Work\", \"Bad\"]\n",
    "* `numeric`: e.g., BLEU score\n",
    "* `explanation`: Just give reasoning, no score\n",
    "\n",
    "This controls **how the LLM outputs evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 10. Why Schema Definition Is Important in Auto Evaluators\n",
    "\n",
    "Schema defines:\n",
    "\n",
    "* What fields are in the input/output\n",
    "* How LangSmith interprets your example data\n",
    "* Which fields should be evaluated\n",
    "\n",
    "### ✅ Why it's crucial:\n",
    "\n",
    "* Prevents wrong field mapping (e.g., comparing title to answer)\n",
    "* Enables automatic evaluators to function correctly\n",
    "* Adds structure for filtering and aggregation\n",
    "\n",
    "### ✅ Example Schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": {\n",
    "    \"question\": \"string\",\n",
    "    \"context\": \"string\"\n",
    "  },\n",
    "  \"output\": {\n",
    "    \"answer\": \"string\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Without this, evaluators might compare `context` vs `answer` by mistake.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Must-Know Questions\n",
    "\n",
    "1. What does an evaluator compare during testing?\n",
    "2. How does LLM-as-Judge differ from exact match evaluators?\n",
    "3. Why is semantic similarity important even when exact match fails?\n",
    "4. Why should you define schema for inputs/outputs in evaluations?\n",
    "5. What is the purpose of using multiple evaluators for one dataset?\n",
    "6. How can LangSmith evaluators help in CI/CD pipeline?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f8b0c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ **1. What does an evaluator compare during testing?**\n",
    "\n",
    "### ✅ **Answer:**\n",
    "\n",
    "An evaluator **compares the model's actual output (prediction)** with the **expected or golden output (reference)** for a given **input**.\n",
    "\n",
    "**The evaluation is based on:**\n",
    "\n",
    "* **Correctness** (e.g., does the answer match?)\n",
    "* **Semantic similarity** (e.g., is it different wording but the same meaning?)\n",
    "* **Relevance** (e.g., does the answer relate to the input?)\n",
    "* **Factual accuracy** (e.g., is the information true?)\n",
    "* **Custom logic** (e.g., does it include specific required keywords?)\n",
    "\n",
    "### 📌 Example:\n",
    "\n",
    "```python\n",
    "input: \"What is the capital of France?\"\n",
    "prediction: \"Paris\"\n",
    "reference: \"Paris\"\n",
    "```\n",
    "\n",
    "Evaluators check if `\"Paris\"` is:\n",
    "\n",
    "* Exactly equal (`ExactMatch`)\n",
    "* Similar in meaning (`SemanticSimilarity`)\n",
    "* Valid (`LLM-as-Judge`)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **2. How does LLM-as-Judge differ from exact match evaluators?**\n",
    "\n",
    "### ✅ **Answer:**\n",
    "\n",
    "| Evaluator Type   | What it does                                                             | Use Case                                                                         |\n",
    "| ---------------- | ------------------------------------------------------------------------ | -------------------------------------------------------------------------------- |\n",
    "| **Exact Match**  | Checks if `prediction == reference`                                      | For deterministic tasks like math, dates, names                                  |\n",
    "| **LLM-as-Judge** | Uses another LLM (like GPT-4) to **analyze the prediction and score it** | For creative, semantic, or fuzzy tasks like summarization, explanation, chatbots |\n",
    "\n",
    "### 🧠 Real-world analogy:\n",
    "\n",
    "* **Exact Match** is like checking if an answer matches the answer key exactly.\n",
    "* **LLM-as-Judge** is like a human teacher grading an essay based on its meaning, not exact words.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **3. Why is semantic similarity important even when exact match fails?**\n",
    "\n",
    "### ✅ **Answer:**\n",
    "\n",
    "LLMs can **paraphrase** correct answers. Exact match will fail even if the meaning is correct.\n",
    "\n",
    "### 📌 Example:\n",
    "\n",
    "```python\n",
    "prediction: \"The city of lights, Paris, is the capital.\"\n",
    "reference: \"Paris\"\n",
    "```\n",
    "\n",
    "* `ExactMatch` → ❌ fails\n",
    "* `SemanticSimilarity` → ✅ scores highly\n",
    "\n",
    "Semantic similarity ensures **we reward valid but creative answers**, not just exact strings.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **4. Why should you define schema for inputs/outputs in evaluations?**\n",
    "\n",
    "### ✅ **Answer:**\n",
    "\n",
    "Schemas tell LangSmith **what fields to evaluate** and how to **map the data** in your dataset.\n",
    "\n",
    "### 📌 Why it's important:\n",
    "\n",
    "* Prevents comparing wrong fields (e.g., summary vs. title)\n",
    "* Helps auto evaluators like `LLM-as-Judge` understand what is “input”, “output”, and “expected output”\n",
    "* Allows **UI-based evaluations**, filters, and aggregation to work properly\n",
    "\n",
    "### 🔧 Without schema:\n",
    "\n",
    "LangSmith might evaluate the wrong data or give an error.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **5. What is the purpose of using multiple evaluators for one dataset?**\n",
    "\n",
    "### ✅ **Answer:**\n",
    "\n",
    "No single evaluator can capture **all dimensions of quality** in LLM output.\n",
    "\n",
    "### 📌 Benefits:\n",
    "\n",
    "* **ExactMatch** for precision\n",
    "* **SemanticSimilarity** for flexibility\n",
    "* **LLM-as-Judge** for human-like assessment\n",
    "* **Custom Evaluator** for domain-specific logic (e.g., compliance, medical correctness)\n",
    "\n",
    "Using multiple evaluators provides a **full picture** of your model’s performance.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **6. How can LangSmith evaluators help in CI/CD pipeline?**\n",
    "\n",
    "### ✅ **Answer:**\n",
    "\n",
    "You can **automate evaluation** of your model/app after every change — just like traditional tests.\n",
    "\n",
    "### 📌 In CI/CD:\n",
    "\n",
    "* After a code/model update, you run your dataset.\n",
    "* Evaluators score the outputs.\n",
    "* If scores drop below thresholds → Fail the build → Prevent regression.\n",
    "* If scores improve → Track progress.\n",
    "\n",
    "LangSmith lets you **version, compare, and benchmark models continuously**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b3e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
