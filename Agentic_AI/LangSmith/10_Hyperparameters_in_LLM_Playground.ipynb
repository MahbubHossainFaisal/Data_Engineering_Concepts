{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0982126",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🎛️ Hyperparameters in LLM Playground (with In-Depth Use Cases)\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 1. **Temperature**\n",
    "\n",
    "#### 📌 What it does:\n",
    "\n",
    "Controls **randomness/creativity** of token selection.\n",
    "\n",
    "* Lower = more **predictable and focused**\n",
    "* Higher = more **diverse and creative**\n",
    "\n",
    "#### 🔢 Range:\n",
    "\n",
    "`0.0` (deterministic) → `1.0+` (more random)\n",
    "\n",
    "#### 🧠 Analogy:\n",
    "\n",
    "Think of **temperature as the “creative freedom”** of the model.\n",
    "\n",
    "#### 🧪 Use Cases:\n",
    "\n",
    "| Use Case                                 | Temperature |\n",
    "| ---------------------------------------- | ----------- |\n",
    "| Factual Q\\&A (e.g., “What is diabetes?”) | `0.0 – 0.3` |\n",
    "| Legal/medical summarization              | `0.2 – 0.4` |\n",
    "| Story generation                         | `0.8 – 1.0` |\n",
    "| Marketing slogans / poetry               | `0.9 – 1.2` |\n",
    "\n",
    "#### ⚠️ Tip:\n",
    "\n",
    "Too high a temperature may cause **hallucinations**. For enterprise use, stay under `0.7`.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 2. **Top-p (Nucleus Sampling)**\n",
    "\n",
    "#### 📌 What it does:\n",
    "\n",
    "Limits the token selection to the **top cumulative probability mass** (e.g., top 90% of likely tokens).\n",
    "\n",
    "> Instead of picking from all possibilities, it picks from a subset that’s most likely.\n",
    "\n",
    "#### 🔢 Range:\n",
    "\n",
    "`0.0` to `1.0`\n",
    "\n",
    "#### 🧪 Use Cases:\n",
    "\n",
    "| Use Case                        | Top-p        |\n",
    "| ------------------------------- | ------------ |\n",
    "| Precision-focused summarization | `0.8 – 0.9`  |\n",
    "| Conversational AI               | `0.9 – 1.0`  |\n",
    "| Creative writing                | `0.95 – 1.0` |\n",
    "\n",
    "✅ Top-p can be used **with or instead of temperature**.\n",
    "For fine control, lower **both** for more deterministic behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 3. **Max Tokens**\n",
    "\n",
    "#### 📌 What it does:\n",
    "\n",
    "Limits the **length of the model’s response** in tokens.\n",
    "(1 token ≈ 0.75 words)\n",
    "\n",
    "#### 🔢 Example:\n",
    "\n",
    "* `max_tokens = 50` → very short responses\n",
    "* `max_tokens = 500` → paragraphs\n",
    "* `max_tokens = 2000+` → full article\n",
    "\n",
    "#### 🧪 Use Cases:\n",
    "\n",
    "| Use Case                     | Max Tokens                                 |\n",
    "| ---------------------------- | ------------------------------------------ |\n",
    "| Chatbot replies              | `100 – 200`                                |\n",
    "| TL;DR summaries              | `50 – 150`                                 |\n",
    "| Long-form article generation | `800 – 3000`                               |\n",
    "| JSON output enforcement      | `100 – 400` (helps ensure valid structure) |\n",
    "\n",
    "✅ **Tip:** Higher `max_tokens` = more cost and latency. Always tune based on **expected output length**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 4. **Stop Sequences**\n",
    "\n",
    "#### 📌 What it does:\n",
    "\n",
    "Tells the model to **stop generating** when it hits a specified string.\n",
    "\n",
    "#### 🧪 Example:\n",
    "\n",
    "```python\n",
    "stop = [\"\\nUser:\", \"###\"]\n",
    "```\n",
    "\n",
    "Use this when:\n",
    "\n",
    "* Building a chatbot (`stop: [\"\\nUser:\"]`)\n",
    "* Structuring completions (`stop: [\"###\"]` to delimit sections)\n",
    "\n",
    "#### 🧠 Why important?\n",
    "\n",
    "It prevents the model from **over-generating or leaking into the next prompt section**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 5. **Frequency Penalty**\n",
    "\n",
    "#### 📌 What it does:\n",
    "\n",
    "Penalizes tokens that have already been used — discouraging **repetition**.\n",
    "\n",
    "#### 🔢 Range:\n",
    "\n",
    "`0.0` (no penalty) → `2.0` (high penalty)\n",
    "\n",
    "#### 🧪 Use Cases:\n",
    "\n",
    "| Use Case                          | Value       |\n",
    "| --------------------------------- | ----------- |\n",
    "| Short answer Q\\&A                 | `0.0 – 0.3` |\n",
    "| Reduce repetition in summaries    | `0.8 – 1.2` |\n",
    "| Fix repetitive phrases in stories | `1.0+`      |\n",
    "\n",
    "#### ⚠️ Example:\n",
    "\n",
    "Without penalty:\n",
    "\n",
    "> “AI is amazing. AI is powerful. AI is useful.”\n",
    "\n",
    "With penalty:\n",
    "\n",
    "> “AI is powerful and transformative across industries.”\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 6. **Presence Penalty**\n",
    "\n",
    "#### 📌 What it does:\n",
    "\n",
    "Encourages the model to **introduce new tokens** it hasn’t used before.\n",
    "\n",
    "* Useful to **increase diversity of ideas**\n",
    "* Works well for brainstorming, ideation tasks\n",
    "\n",
    "#### 🔢 Range:\n",
    "\n",
    "`0.0` (no encouragement) → `2.0` (strong encouragement)\n",
    "\n",
    "#### 🧪 Use Cases:\n",
    "\n",
    "| Use Case                     | Value       |\n",
    "| ---------------------------- | ----------- |\n",
    "| Ideation for marketing ideas | `1.0 – 1.5` |\n",
    "| Generate multiple titles     | `1.0 – 1.8` |\n",
    "| Avoid repetition of themes   | `1.0+`      |\n",
    "\n",
    "✅ Use this when you want **new, varied** content, not repetition.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Bonus: Choosing Between Temperature and Top-p\n",
    "\n",
    "| Feature         | Controls                              | Best For                             |\n",
    "| --------------- | ------------------------------------- | ------------------------------------ |\n",
    "| **Temperature** | **Randomness** of each token          | Simpler control for creativity       |\n",
    "| **Top-p**       | **Probability mass** of token choices | Fine-tuned sampling, avoids outliers |\n",
    "\n",
    "✅ You can use **both**, but don’t crank both too high.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Summary Table: Hyperparameters Overview\n",
    "\n",
    "| Parameter           | Purpose                 | Safe Range | When to Use                     |\n",
    "| ------------------- | ----------------------- | ---------- | ------------------------------- |\n",
    "| `temperature`       | Creativity / randomness | 0.2 – 0.8  | Q\\&A (low), storytelling (high) |\n",
    "| `top_p`             | Limit token pool        | 0.8 – 1.0  | Precision control               |\n",
    "| `max_tokens`        | Output length cap       | 50 – 3000  | Depends on output need          |\n",
    "| `stop`              | Stop generation         | n/a        | Chat, structured formats        |\n",
    "| `frequency_penalty` | Reduce repetition       | 0.8 – 1.2  | Summaries, clean output         |\n",
    "| `presence_penalty`  | Encourage novelty       | 1.0 – 2.0  | Brainstorming, idea diversity   |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Must-Know Questions\n",
    "\n",
    "1. ✅ When should you use `frequency_penalty` vs `presence_penalty`?\n",
    "2. ✅ What does `top_p = 0.9` do that `temperature = 0.9` doesn’t?\n",
    "3. ✅ What happens if `max_tokens` is too low?\n",
    "4. ✅ What’s the difference between temperature and top-p?\n",
    "5. ✅ Why are stop sequences important in a multi-turn chatbot?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f7415",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ✅ 1. **When should you use `frequency_penalty` vs `presence_penalty`?**\n",
    "\n",
    "| Hyperparameter      | Use it when you want to...                                        | Effect                                                                          |\n",
    "| ------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
    "| `frequency_penalty` | Reduce **repetition of words/phrases** that have already appeared | Lowers the score of tokens **repeated too frequently**                          |\n",
    "| `presence_penalty`  | Encourage the model to **introduce new topics or words**          | Adds penalty to tokens that have **already appeared once**, pushing for novelty |\n",
    "\n",
    "#### 🔍 Example:\n",
    "\n",
    "**Prompt:**\n",
    "\"Write a paragraph about AI and its benefits.\"\n",
    "\n",
    "**Without any penalty:**\n",
    "\"AI is powerful. AI is powerful because AI helps. AI is useful.\"\n",
    "\n",
    "**With `frequency_penalty = 1.2`:**\n",
    "\"AI is powerful and helps automate tasks across industries.\"\n",
    "\n",
    "**With `presence_penalty = 1.2`:**\n",
    "\"AI is powerful. It also improves decision-making and assists in healthcare.\"\n",
    "\n",
    "✅ **Use frequency penalty** to make output less **wordy** or **repetitive**.\n",
    "✅ **Use presence penalty** to **expand ideas** and **avoid staying on one point**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 2. **What does `top_p = 0.9` do that `temperature = 0.9` doesn’t?**\n",
    "\n",
    "| Parameter     | Works by...                                                                   | Main Difference                                          |\n",
    "| ------------- | ----------------------------------------------------------------------------- | -------------------------------------------------------- |\n",
    "| `temperature` | Adds randomness to **each token** selection                                   | Controls **how random** the token choice is              |\n",
    "| `top_p`       | Filters out tokens not in the **top cumulative probability mass (e.g., 90%)** | Controls **how many tokens** are considered at each step |\n",
    "\n",
    "#### 🔍 Key Difference:\n",
    "\n",
    "* **Temperature** controls **how adventurous** the model can be.\n",
    "* **Top-p** controls **how wide the decision pool** is before randomness applies.\n",
    "\n",
    "#### 🧠 Analogy:\n",
    "\n",
    "* **Temperature** = how much risk you allow the model to take.\n",
    "* **Top-p** = how many options you put on the table.\n",
    "\n",
    "✅ For **creative tasks**, use `temperature + top_p`.\n",
    "✅ For **tight control**, lower both slightly (e.g., `temperature=0.5`, `top_p=0.85`).\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 3. **What happens if `max_tokens` is too low?**\n",
    "\n",
    "#### 🔍 Answer:\n",
    "\n",
    "* The model **stops generating prematurely**.\n",
    "* You get **incomplete responses** or **cut-off JSON**, summaries, or answers.\n",
    "\n",
    "#### 🔴 Example:\n",
    "\n",
    "Prompt: \"Summarize the following article about the effects of sugar consumption.\"\n",
    "\n",
    "* **`max_tokens = 20`**\n",
    "\n",
    "  > \"Sugar can lead to... \\[cut off]\"\n",
    "\n",
    "* **`max_tokens = 100`**\n",
    "\n",
    "  > \"Sugar consumption has been linked to obesity, diabetes, and other health issues. It’s recommended to limit intake...\"\n",
    "\n",
    "✅ Always set `max_tokens` based on:\n",
    "\n",
    "* Length of expected answer\n",
    "* Structure (e.g., JSON output needs more space)\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 4. **What’s the difference between temperature and top-p?**\n",
    "\n",
    "| Feature       | Temperature                 | Top-p                                 |\n",
    "| ------------- | --------------------------- | ------------------------------------- |\n",
    "| Controls      | **Randomness of selection** | **Size of token pool** to choose from |\n",
    "| Value Effect  | Higher = more diverse       | Lower = more focused subset           |\n",
    "| Use Alone?    | ✅ Yes                       | ✅ Yes                                 |\n",
    "| Use Together? | ✅ Best practice             | ✅ Best practice                       |\n",
    "\n",
    "#### 🧪 Example:\n",
    "\n",
    "With the prompt: “Write a title for a blog post about AI and healthcare.”\n",
    "\n",
    "* `temperature = 0.9`, `top_p = 1.0`\n",
    "  → “Revolutionizing Medicine: The AI Takeover”\n",
    "\n",
    "* `temperature = 0.7`, `top_p = 0.85`\n",
    "  → “How AI is Transforming Healthcare”\n",
    "\n",
    "✅ **Top-p** filters; **temperature** selects within the filter.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 5. **Why are stop sequences important in a multi-turn chatbot?**\n",
    "\n",
    "#### 🔍 Answer:\n",
    "\n",
    "They **prevent the model from generating unintended parts of the next message**, or from running off with an overly long response.\n",
    "\n",
    "#### 🎯 Use Case:\n",
    "\n",
    "In a chatbot, you want the model to **stop when it’s done responding**, not generate a fake \"User:\" line or new turn.\n",
    "\n",
    "#### 🧪 Example:\n",
    "\n",
    "```python\n",
    "stop = [\"\\nUser:\", \"\\nAgent:\"]\n",
    "```\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "```\n",
    "User: What's your name?\n",
    "Agent:\n",
    "```\n",
    "\n",
    "**Without stop sequence:**\n",
    "→ \"I'm LangBot! User: What do you do? Agent: I help...\"\n",
    "\n",
    "**With stop sequence:**\n",
    "→ \"I'm LangBot!\"\n",
    "\n",
    "✅ Helps **maintain proper dialogue flow**\n",
    "✅ Prevents **model from hallucinating both sides** of a conversation\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Recap: Must-Know Questions Summary\n",
    "\n",
    "| Question                                  | Answer                                                       |\n",
    "| ----------------------------------------- | ------------------------------------------------------------ |\n",
    "| `frequency_penalty` vs `presence_penalty` | Frequency → avoid repeat words; Presence → push new ideas    |\n",
    "| `top_p` vs `temperature`                  | Top-p filters token choices; temperature controls randomness |\n",
    "| Low `max_tokens` effect                   | Output is incomplete or abruptly cut                         |\n",
    "| Main diff: temperature & top\\_p           | Temperature = risk; Top-p = scope of choices                 |\n",
    "| Importance of stop sequences              | Maintain dialogue flow, stop hallucinated turns              |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2850079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
