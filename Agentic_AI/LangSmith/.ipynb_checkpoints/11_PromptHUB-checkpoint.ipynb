{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62dfada",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. ‚úÖ What is PromptHub?\n",
    "\n",
    "**PromptHub** is a **centralized place to store, manage, test, and share your prompts**.\n",
    "\n",
    "> Think of PromptHub like a **GitHub for prompts** ‚Äî where instead of storing code, you're storing high-quality, reusable, version-controlled **prompt templates** used to talk to LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. üéØ Purpose and the Problem It Solves\n",
    "\n",
    "### ‚ùå The Problem (Before PromptHub):\n",
    "\n",
    "* Teams write different versions of the same prompt.\n",
    "* No versioning ‚Äî you can't track improvements.\n",
    "* Prompt logic is hardcoded deep in app code.\n",
    "* No central repository ‚Äî prompts become chaotic.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ The Solution (With PromptHub):\n",
    "\n",
    "* üéØ **Centralized Prompt Repository** ‚Äî one place to manage all prompts.\n",
    "* üì¶ **Prompt Templates** are reusable and modular.\n",
    "* üß™ **Built-in testing and versioning** ‚Äî try changes safely.\n",
    "* üë• **Collaboration** ‚Äî teams can share and refine prompts.\n",
    "* üõ†Ô∏è **Deployment-ready** ‚Äî export prompts as LangChain-compatible code.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. üß∞ Prompt Template in PromptHub Contains:\n",
    "\n",
    "| Component               | Description                                              |\n",
    "| ----------------------- | -------------------------------------------------------- |\n",
    "| **Template Messages**   | The prompt instructions & user/system/assistant messages |\n",
    "| **Model Configuration** | Model name, temperature, max tokens, etc.                |\n",
    "| **Output Schema**       | Expected format of the output (e.g., JSON structure)     |\n",
    "\n",
    "> This makes each prompt not just text, but a **full spec** ‚Äî ready to be used or tested instantly.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. üÜö Chat-Style Prompt vs Instruct-Style Prompt\n",
    "\n",
    "| Type               | Description                                    | Example                                                                  |\n",
    "| ------------------ | ---------------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| **Instruct-style** | A single instruction block                     | `\"Summarize this article in 3 bullet points\"`                            |\n",
    "| **Chat-style**     | Structured messages: system + user + assistant | `System: You are a helpful tutor`<br>`User: Explain Newton‚Äôs second law` |\n",
    "\n",
    "‚úÖ **PromptHub supports both** ‚Äî so you can build apps for **GPT-4 Turbo (chat-style)** or **older instruct-based models** easily.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. üíº Real-World Scenario + Code Walkthrough\n",
    "\n",
    "### üí° Scenario: You‚Äôre building an app to **convert customer complaints into polite responses**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Step-by-Step Plan\n",
    "\n",
    "1. Store the prompt in PromptHub.\n",
    "2. Include:\n",
    "\n",
    "   * Chat-style system & user messages.\n",
    "   * Output schema (JSON: greeting, response, closing).\n",
    "   * Model config (GPT-4, temperature=0.5).\n",
    "3. Load & call it in your app using LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "### üßë‚Äçüç≥ PromptHub Template (Chat-style):\n",
    "\n",
    "```python\n",
    "system: \"You are a polite customer support assistant.\"\n",
    "user: \"A customer says: {complaint_text}. Write a kind and helpful response.\"\n",
    "```\n",
    "\n",
    "### üß© Output Schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"greeting\": {\"type\": \"string\"},\n",
    "    \"response\": {\"type\": \"string\"},\n",
    "    \"closing\": {\"type\": \"string\"}\n",
    "  },\n",
    "  \"required\": [\"greeting\", \"response\", \"closing\"]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Code Walkthrough (Using LangChain + PromptHub)\n",
    "\n",
    "```python\n",
    "from langchain.prompthub import pull_prompt\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. üîÑ Pull prompt from PromptHub (public or private repo)\n",
    "prompt = pull_prompt(\"mahbub/politeness-helper\")  # You can create and version your own\n",
    "\n",
    "# 2. üîß Setup the LLM with same config used in PromptHub\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5)\n",
    "\n",
    "# 3. üîó Create the chain with the pulled prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 4. üß™ Run it\n",
    "result = chain.run({\"complaint_text\": \"Your app is slow and crashes constantly!\"})\n",
    "\n",
    "# 5. üì§ Print the structured response\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### üßæ Expected Output (Schema enforced):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"greeting\": \"Hi there!\",\n",
    "  \"response\": \"We sincerely apologize for the inconvenience you're experiencing. We're actively working to improve performance.\",\n",
    "  \"closing\": \"Thank you for your patience!\"\n",
    "}\n",
    "```\n",
    "\n",
    "‚úÖ This allows **clean integration** in a chatbot, email responder, or helpdesk app.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. ‚ùì Must-Know Questions\n",
    "\n",
    "* What are the benefits of storing prompts in PromptHub instead of source code?\n",
    "* How do you version and test prompts in PromptHub?\n",
    "* When should you use chat-style over instruct-style?\n",
    "* Can PromptHub templates be integrated into LangChain or other frameworks?\n",
    "* How do output schemas improve reliability in production AI systems?\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Key Takeaways\n",
    "\n",
    "| Concept                  | Why It Matters                                          |\n",
    "| ------------------------ | ------------------------------------------------------- |\n",
    "| **PromptHub**            | Like GitHub for managing prompts                        |\n",
    "| **Structured Templates** | Prompts + model settings + schema = production-ready    |\n",
    "| **Chat-style**           | Better for dialogue agents (system, user, assistant)    |\n",
    "| **Instruct-style**       | Good for single-shot tasks                              |\n",
    "| **Schema**               | Enforces output format, crucial for automated systems   |\n",
    "| **Real use**             | Easy integration with LangChain for dynamic Gen AI apps |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2dd15",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **Must-Know Questions ‚Äî Answered in Detail**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. What are the benefits of storing prompts in PromptHub instead of source code?**\n",
    "\n",
    "#### ‚úÖ Answer:\n",
    "\n",
    "Storing prompts in **PromptHub** (vs directly inside code) provides:\n",
    "\n",
    "| Benefit                           | Explanation                                                                                 |\n",
    "| --------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| üîÑ **Version Control**            | You can track changes to prompts like Git ‚Äî rollback, compare versions, etc.                |\n",
    "| üë• **Collaboration**              | Team members can test, improve, and comment on shared prompts.                              |\n",
    "| üî¨ **Rapid Experimentation**      | You can iterate and test different prompt styles/settings *without redeploying code*.       |\n",
    "| ‚ôªÔ∏è **Reusability**                | Same prompt can be reused across many pipelines or chains.                                  |\n",
    "| üì§ **Decoupling logic from code** | Keeps application logic clean ‚Äî prompts become configuration instead of hardcoded behavior. |\n",
    "\n",
    "#### üß† Analogy:\n",
    "\n",
    "> It‚Äôs like using a CMS for website content instead of embedding text directly into HTML ‚Äî **flexible, maintainable, and collaborative**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. How do you version and test prompts in PromptHub?**\n",
    "\n",
    "#### ‚úÖ Answer:\n",
    "\n",
    "**PromptHub has built-in versioning.** Every time you update a prompt template:\n",
    "\n",
    "* A new version is created\n",
    "* You can view and compare differences\n",
    "* You can tag versions (e.g., ‚Äúv1.0-production‚Äù)\n",
    "\n",
    "You can test prompts by:\n",
    "\n",
    "* Supplying different **inputs (variables)**\n",
    "* Changing model settings (temperature, top\\_p, etc.)\n",
    "* Enforcing output schema and seeing whether outputs match\n",
    "* Using **LangSmith Playground** to simulate end-to-end flow\n",
    "\n",
    "#### üöÄ Bonus:\n",
    "\n",
    "LangSmith also lets you **load prompts programmatically** by version tag in production code, like this:\n",
    "\n",
    "```python\n",
    "prompt = pull_prompt(\"mahbub/politeness-helper@v1.2\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. When should you use chat-style over instruct-style prompts?**\n",
    "\n",
    "#### ‚úÖ Answer:\n",
    "\n",
    "| Use Case                             | Chat-style                | Instruct-style     |\n",
    "| ------------------------------------ | ------------------------- | ------------------ |\n",
    "| Multi-turn dialogue                  | ‚úÖ Best suited             | ‚ùå Not designed for |\n",
    "| Role-based behavior                  | ‚úÖ Supports system message | ‚ùå No system role   |\n",
    "| Simpler tasks (e.g., summarize text) | ‚úÖ Can use                 | ‚úÖ Ideal            |\n",
    "| Complex LLM agents                   | ‚úÖ Required                | ‚ùå Limited context  |\n",
    "\n",
    "**Chat-style prompts** give you:\n",
    "\n",
    "* `system`, `user`, and `assistant` messages\n",
    "* More control over model behavior\n",
    "\n",
    "**Instruct-style prompts** are:\n",
    "\n",
    "* Just a block of text\n",
    "* Best for older models or when single input/output is enough\n",
    "\n",
    "> ‚ö†Ô∏è **Most modern models (like GPT-4 Turbo, Claude 3) are chat-first.**\n",
    "> So Chat-style prompts = industry default for LLM apps.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Can PromptHub templates be integrated into LangChain or other frameworks?**\n",
    "\n",
    "#### ‚úÖ Answer:\n",
    "\n",
    "Yes! PromptHub is designed for **integration into LangChain workflows** (and similar frameworks like LlamaIndex). You can:\n",
    "\n",
    "1. Pull a prompt directly using `pull_prompt()`\n",
    "2. Use it with `LLMChain`, `ChatChain`, or agent flows\n",
    "3. The prompt comes with:\n",
    "\n",
    "   * Prompt template\n",
    "   * Model config\n",
    "   * Optional output schema\n",
    "\n",
    "#### üîß Example:\n",
    "\n",
    "```python\n",
    "prompt = pull_prompt(\"mahbub/support-reply-template\")\n",
    "chain = LLMChain(llm=ChatOpenAI(), prompt=prompt)\n",
    "chain.run({\"complaint_text\": \"App keeps freezing\"})\n",
    "```\n",
    "\n",
    "> This gives you **clean code, clean prompts, and flexible upgrades** ‚Äî even in production.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. How do output schemas improve reliability in production AI systems?**\n",
    "\n",
    "#### ‚úÖ Answer:\n",
    "\n",
    "**Output schemas define the expected structure** of LLM responses, e.g., JSON objects with specific keys/types.\n",
    "\n",
    "### üéØ Why schemas matter:\n",
    "\n",
    "| Benefit                 | How it helps                                                   |\n",
    "| ----------------------- | -------------------------------------------------------------- |\n",
    "| ‚úÖ **Reliability**       | Validates that model output is what your app expects           |\n",
    "| üö´ **Error Prevention** | Prevents crashes in downstream logic (e.g., when parsing JSON) |\n",
    "| üîç **Validation**       | Catches bad or incomplete outputs at runtime                   |\n",
    "| üß™ **Testing**          | Lets you run bulk tests and flag schema violations             |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Real-World Example:\n",
    "\n",
    "Imagine a customer service agent bot. It expects output like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"greeting\": \"Hello!\",\n",
    "  \"response\": \"We're sorry to hear that...\",\n",
    "  \"closing\": \"Thank you for contacting us.\"\n",
    "}\n",
    "```\n",
    "\n",
    "Without an output schema:\n",
    "\n",
    "* A hallucinated model reply like ‚ÄúSorry üòî we‚Äôre on it!‚Äù might break your app‚Äôs renderer or downstream logic.\n",
    "\n",
    "With a schema:\n",
    "\n",
    "* You validate structure BEFORE passing it to UI/API/backend.\n",
    "* You get better **guardrails and observability** in production.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Recap Cheat Sheet\n",
    "\n",
    "| Question                      | Short Takeaway                                                    |\n",
    "| ----------------------------- | ----------------------------------------------------------------- |\n",
    "| Why PromptHub?                | Central repo for prompts = better collaboration, testing, scaling |\n",
    "| How to test prompts?          | Use playground + versioning + input variations                    |\n",
    "| Chat-style vs Instruct-style? | Use chat-style for LLM agents and complex behavior                |\n",
    "| LangChain integration?        | Seamless via `pull_prompt()` + `LLMChain`                         |\n",
    "| Why schemas?                  | Enforce structure, catch bad outputs, protect downstream logic    |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100382c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
