{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab348ed",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What is a **Summary Evaluator**?\n",
    "\n",
    "A **Summary Evaluator** is a specialized **LLM-as-Judge** evaluator that compares:\n",
    "\n",
    "* A **source document** (what needs to be summarized),\n",
    "* The **model-generated summary** (the prediction),\n",
    "* And the **reference/golden summary** (what a good summary should look like).\n",
    "\n",
    "Then it **automatically scores the summary** based on:\n",
    "\n",
    "* **Faithfulness** (is it factually grounded in the source?)\n",
    "* **Conciseness** (is it brief but informative?)\n",
    "* **Relevance** (does it include the important parts?)\n",
    "* **Fluency** (does it sound natural?)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Why Do We Need Summary Evaluators?\n",
    "\n",
    "Summarization is **subjective and fuzzy**:\n",
    "\n",
    "* Multiple valid summaries can exist.\n",
    "* A summary might look fine but **hallucinate facts**.\n",
    "* Traditional metrics like **BLEU** or **ROUGE** don’t capture **faithfulness** well.\n",
    "\n",
    "✅ So we need Summary Evaluators to:\n",
    "\n",
    "* Automatically **evaluate summaries** using an LLM.\n",
    "* Judge if summaries are **accurate and helpful**, not just similar.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ How Does It Work?\n",
    "\n",
    "### 🔁 The process:\n",
    "\n",
    "For each example, LangSmith passes:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": {\n",
    "    \"document\": \"Original article about AI...\",\n",
    "    \"question\": \"Summarize this\"\n",
    "  },\n",
    "  \"prediction\": \"AI is transforming many industries...\",\n",
    "  \"reference\": \"Artificial Intelligence impacts industries across sectors...\"\n",
    "}\n",
    "```\n",
    "\n",
    "The **Summary Evaluator** (LLM-powered) reviews:\n",
    "\n",
    "* 🔍 Is the summary **factually correct**?\n",
    "* 📋 Is it **complete and relevant**?\n",
    "* 🧠 Is it **fluent and clear**?\n",
    "\n",
    "Then it outputs:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"score\": 4,\n",
    "  \"reasoning\": \"Summary is accurate and relevant, but slightly verbose.\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 When Should You Use Summary Evaluators?\n",
    "\n",
    "Use it when:\n",
    "\n",
    "* ✅ You're building **summarization applications**\n",
    "* ✅ You need to **compare summary versions** (prompt A vs prompt B)\n",
    "* ✅ You want to **catch hallucinations** in generated summaries\n",
    "* ✅ You want **automated but human-like scoring** of summary quality\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 What Benefits Does It Provide?\n",
    "\n",
    "| Benefit                 | Description                                    |\n",
    "| ----------------------- | ---------------------------------------------- |\n",
    "| 🧠 Better Evaluation    | Judges based on **meaning**, not word match    |\n",
    "| ⚡ Automation            | Evaluates large datasets without manual review |\n",
    "| ✅ Reliability           | Helps detect **factual errors** in summaries   |\n",
    "| 📈 Rich Feedback        | Gives reasoning alongside numeric scores       |\n",
    "| 🧪 Supports A/B Testing | Can compare summary versions side-by-side      |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Real-World Scenario + Solution\n",
    "\n",
    "### 📚 Scenario:\n",
    "\n",
    "You’re building an LLM app that summarizes **healthcare articles** for patients.\n",
    "\n",
    "You want to test if your new summary prompt is better than the old one.\n",
    "\n",
    "You’ve created a dataset:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"document\": \"Diabetes is a chronic condition ...\",\n",
    "  \"expected_summary\": \"Diabetes affects how the body processes sugar.\"\n",
    "}\n",
    "```\n",
    "\n",
    "You run:\n",
    "\n",
    "* **Prompt v1 → Summary A**\n",
    "* **Prompt v2 → Summary B**\n",
    "\n",
    "### 💡 Problem:\n",
    "\n",
    "* Both summaries are fluent.\n",
    "* You’re not sure which one is **more faithful** or **complete**.\n",
    "\n",
    "### ✅ Solution: Use Summary Evaluators\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import RunEvaluator\n",
    "\n",
    "client = Client()\n",
    "\n",
    "summary_eval = RunEvaluator.for_type(\"summary_quality\")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name=\"health-summaries-v1\",\n",
    "    model=\"gpt-4-summary-v1\",\n",
    "    evaluation=[summary_eval],\n",
    "    project_name=\"summary-eval-gpt4-v1\"\n",
    ")\n",
    "```\n",
    "\n",
    "> The evaluator will score each summary for:\n",
    ">\n",
    "> * ✅ Accuracy (no hallucination)\n",
    "> * ✅ Coverage (includes key points)\n",
    "> * ✅ Clarity\n",
    "\n",
    "And then you’ll see **per-example scores**, **justifications**, and **aggregated performance** in LangSmith.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Must-Know Summary\n",
    "\n",
    "| Topic         | Key Point                                                 |\n",
    "| ------------- | --------------------------------------------------------- |\n",
    "| What is it?   | Evaluator for checking summary quality                    |\n",
    "| Why needed?   | Summarization is subjective and error-prone               |\n",
    "| How it works? | Compares document, prediction, and reference via LLM      |\n",
    "| When to use?  | When building, testing, or comparing summaries            |\n",
    "| Benefits?     | Human-like scoring, automation, reasoning, accuracy       |\n",
    "| Scenario?     | Summarizing articles and evaluating factuality & coverage |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b194b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
