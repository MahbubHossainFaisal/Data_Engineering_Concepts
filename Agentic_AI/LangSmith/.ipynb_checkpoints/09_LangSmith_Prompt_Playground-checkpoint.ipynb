{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef0671a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üéØ What Is LangSmith Playground?\n",
    "\n",
    "LangSmith Playground is an **interactive development space** where you can:\n",
    "\n",
    "* Design, test, and debug prompts\n",
    "* Configure LLM parameters like temperature, max tokens, etc.\n",
    "* Visualize responses and output schema\n",
    "* Iterate quickly without coding\n",
    "* Save prompt versions for reuse and A/B testing\n",
    "\n",
    "It‚Äôs similar in spirit to the OpenAI Playground, but built for **evaluating, refining, and integrating prompts in real LangChain apps**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Hardcoded Prompts vs Prompt Templates\n",
    "\n",
    "| Type                 | Description                                  | Pros              | Cons                           |\n",
    "| -------------------- | -------------------------------------------- | ----------------- | ------------------------------ |\n",
    "| **Hardcoded Prompt** | Plain text prompt with fixed content         | Quick to test     | Not dynamic or reusable        |\n",
    "| **Prompt Template**  | Prompt with placeholders (like `{question}`) | Reusable, dynamic | Needs variable injection logic |\n",
    "\n",
    "### üìå Example:\n",
    "\n",
    "**Hardcoded Prompt:**\n",
    "\n",
    "```text\n",
    "Summarize the following article about diabetes.\n",
    "```\n",
    "\n",
    "**Prompt Template:**\n",
    "\n",
    "```text\n",
    "Summarize the following article:\\n\\n{article}\n",
    "```\n",
    "\n",
    "‚úÖ Prompt templates are preferred in production for flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Purpose of LangSmith Playground (In Detail)\n",
    "\n",
    "The Playground lets you:\n",
    "\n",
    "1. **Experiment interactively**: Test prompts with different inputs\n",
    "2. **Fine-tune LLM behavior** using hyperparameters\n",
    "3. **Debug outputs** by inspecting LLM responses, traces, and errors\n",
    "4. **Save & version prompts** (great for prompt engineering workflow)\n",
    "5. **Connect your chains/tools to test end-to-end flow**\n",
    "6. **Evaluate output with built-in evaluators**\n",
    "\n",
    "üë®‚Äçüíª Think of it as your **prompt development IDE**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Hyperparameters of LLMs in Playground (with Use Cases)\n",
    "\n",
    "| Hyperparameter        | Description                                          | Example Use Case                                                      |\n",
    "| --------------------- | ---------------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| **Temperature**       | Controls randomness. 0 = deterministic, 1 = creative | `0.0` ‚Üí factual Q\\&A <br> `0.8` ‚Üí story generation                    |\n",
    "| **Top-k / Top-p**     | Controls diversity of token selection                | Use `top-p = 0.9` for balanced creativity                             |\n",
    "| **Max tokens**        | Limits response length                               | `max_tokens = 50` for summaries; `max_tokens = 1000` for long answers |\n",
    "| **Stop sequences**    | String(s) that stop the model from continuing        | Useful in chat-like applications (e.g., stop at `\\nUser:`)            |\n",
    "| **Frequency Penalty** | Penalizes repetition                                 | Reduce duplicate words in summarization                               |\n",
    "| **Presence Penalty**  | Encourages new topics                                | Boosts novelty in brainstorming tasks                                 |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Use Cases for Playground\n",
    "\n",
    "### 1. **Prompt Design**\n",
    "\n",
    "Iterate quickly on how to ask the LLM a question.\n",
    "\n",
    "> Example: ‚ÄúRewrite this legal paragraph in plain English‚Äù\n",
    "\n",
    "### 2. **Chain Testing**\n",
    "\n",
    "Run your LangChain chain inside the Playground with a mock input.\n",
    "\n",
    "### 3. **Prompt Comparison**\n",
    "\n",
    "Try two prompt variants with the same input and compare outputs.\n",
    "\n",
    "### 4. **Parameter Tuning**\n",
    "\n",
    "Adjust temperature and max tokens to optimize output behavior.\n",
    "\n",
    "### 5. **Schema Alignment**\n",
    "\n",
    "Ensure output is structured and follows your defined schema (e.g., JSON).\n",
    "\n",
    "### 6. **A/B Testing**\n",
    "\n",
    "Save multiple prompt versions and run experiments against datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ What Is an Output Schema?\n",
    "\n",
    "> ‚úÖ **Definition**: An **Output Schema** in LangSmith defines the **expected structure of the model's output**.\n",
    "\n",
    "### üîç Why It Matters:\n",
    "\n",
    "* Helps validate whether the output is **correctly formatted** (especially for structured outputs like JSON)\n",
    "* Enables downstream tools/chains to parse the output safely\n",
    "* Useful for automated evaluators to match prediction vs reference fields\n",
    "\n",
    "### üìå Example Schema (for Q\\&A):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"answer\": { \"type\": \"string\" },\n",
    "    \"source\": { \"type\": \"string\" }\n",
    "  },\n",
    "  \"required\": [\"answer\"]\n",
    "}\n",
    "```\n",
    "\n",
    "### üîÅ Scenario:\n",
    "\n",
    "You‚Äôre building a RAG app. You want your LLM to return:\n",
    "\n",
    "```json\n",
    "{ \"answer\": \"Yes\", \"source\": \"https://cdc.gov\" }\n",
    "```\n",
    "\n",
    "Without an output schema, the model might return just plain text. With the schema, you enforce structure and correctness.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary Recap\n",
    "\n",
    "| Feature             | Explanation                                   |\n",
    "| ------------------- | --------------------------------------------- |\n",
    "| Hardcoded Prompt    | Fixed string, no dynamic input                |\n",
    "| Prompt Template     | Reusable prompt with variables                |\n",
    "| Playground          | Interactive space for testing LLM behavior    |\n",
    "| LLM Hyperparameters | Control randomness, length, structure         |\n",
    "| Use Cases           | Design, test, evaluate, tune prompts          |\n",
    "| Output Schema       | Defines structured format for model responses |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Must-Know Questions for Mastery:\n",
    "\n",
    "1. ‚úÖ When should you use a hardcoded prompt vs a prompt template?\n",
    "2. ‚úÖ What happens if you increase the temperature from 0.2 to 0.9?\n",
    "3. ‚úÖ Why is output schema critical in structured LLM applications?\n",
    "4. ‚úÖ How does LangSmith Playground help you debug a broken chain?\n",
    "5. ‚úÖ Which parameter helps prevent repetitive answers from the model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd99323",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 1. **When should you use a hardcoded prompt vs a prompt template?**\n",
    "\n",
    "| Prompt Type          | Use When...                                 | Why                                                                       |\n",
    "| -------------------- | ------------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| **Hardcoded Prompt** | You‚Äôre quickly testing a one-off idea       | Faster, no setup needed                                                   |\n",
    "| **Prompt Template**  | You‚Äôre building a reusable LLM app or chain | Allows injecting dynamic input variables like `{question}` or `{context}` |\n",
    "\n",
    "üß† **Example**:\n",
    "\n",
    "* Hardcoded: `\"Summarize the article about diabetes.\"`\n",
    "* Template: `\"Summarize the following article: {article}\"` ‚Üí used in production\n",
    "\n",
    "> üîë **Use prompt templates** in all production-ready LLM workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. **What happens if you increase the temperature from 0.2 to 0.9?**\n",
    "\n",
    "| Temperature | Model Behavior                                              |\n",
    "| ----------- | ----------------------------------------------------------- |\n",
    "| **0.2**     | More **focused and deterministic** output. Less creativity. |\n",
    "| **0.9**     | **Creative and diverse** responses. More variability.       |\n",
    "\n",
    "üß™ **Example:**\n",
    "Prompt: `\"Write a story about a dog.\"`\n",
    "\n",
    "* Temp = 0.2 ‚Üí ‚ÄúA dog named Max went to the park.‚Äù\n",
    "* Temp = 0.9 ‚Üí ‚ÄúMax, the talking dog, organized a canine jazz festival.‚Äù\n",
    "\n",
    "> üî• Use **low temperature** for tasks like Q\\&A, summaries.\n",
    "> Use **high temperature** for brainstorming, storytelling.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 3. **Why is output schema critical in structured LLM applications?**\n",
    "\n",
    "#### ‚úÖ Answer:\n",
    "\n",
    "An **output schema** ensures the model's output is:\n",
    "\n",
    "* **Predictable**\n",
    "* **Parseable** (especially for JSON or API integration)\n",
    "* **Validatable** (you can check if required fields exist)\n",
    "\n",
    "### üéØ Example:\n",
    "\n",
    "You want the output like:\n",
    "\n",
    "```json\n",
    "{ \"answer\": \"Yes\", \"source\": \"cdc.gov\" }\n",
    "```\n",
    "\n",
    "But without a schema, the model might return:\n",
    "\n",
    "```\n",
    "Yes, according to CDC.\n",
    "```\n",
    "\n",
    "‚úÖ Schema enforces structure ‚Üí necessary for tools, APIs, or downstream functions.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 4. **How does LangSmith Playground help you debug a broken chain?**\n",
    "\n",
    "#### ‚úÖ Answer:\n",
    "\n",
    "In the Playground:\n",
    "\n",
    "* You can **manually run the chain or tool** with sample inputs\n",
    "* Visualize all **intermediate steps** (sub-runs, prompts, responses)\n",
    "* Inspect **inputs and outputs** of every node in the trace\n",
    "* Modify parameters (e.g., temperature, prompt content) without writing code\n",
    "\n",
    "‚úÖ This speeds up debugging when something in your LangChain logic isn‚Äôt working as expected.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 5. **Which parameter helps prevent repetitive answers from the model?**\n",
    "\n",
    "#### ‚úÖ Answer: `frequency_penalty`\n",
    "\n",
    "| Parameter              | Effect                                                      |\n",
    "| ---------------------- | ----------------------------------------------------------- |\n",
    "| **frequency\\_penalty** | Penalizes token repetition. Higher value ‚Üí less repetition. |\n",
    "\n",
    "üß™ **Example:**\n",
    "Without penalty:\n",
    "\n",
    "> ‚ÄúAI is great. AI is powerful. AI is transforming the world.‚Äù\n",
    "\n",
    "With `frequency_penalty = 1.2`:\n",
    "\n",
    "> ‚ÄúAI is transforming the world across industries.‚Äù\n",
    "\n",
    "‚úÖ Helps generate **more natural**, **less redundant** answers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f18e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
