{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae04aca",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üì¶ 1. **Why Testing & Datasets Matter in LLM Applications**\n",
    "\n",
    "LLM applications are **non-deterministic** (i.e., same prompt ‚Üí can give different responses).\n",
    "That means:\n",
    "\n",
    "* You **cannot write fixed unit tests** like classical software\n",
    "* Instead, you need **datasets** to test quality, coherence, accuracy, etc.\n",
    "\n",
    "> üî• LangSmith helps evaluate how well your LLM performs across real or synthetic inputs, with or without human-labeled \"correct\" outputs.\n",
    "\n",
    "---\n",
    "\n",
    "# üìö 2. **What Is a Dataset in LangSmith?**\n",
    "\n",
    "A **dataset** is a **list of examples** (inputs and optionally expected outputs), used for **evaluation** of your LLM application.\n",
    "\n",
    "Each example is structured like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": {\n",
    "    \"question\": \"Who is Ada Lovelace?\"\n",
    "  },\n",
    "  \"output\": {\n",
    "    \"answer\": \"Ada Lovelace is considered the first computer programmer...\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üõ†Ô∏è 3. **Ways to Generate Evaluation Datasets**\n",
    "\n",
    "There are **4 main ways** to create LangSmith datasets:\n",
    "\n",
    "### 1. üìù **Manually Created Examples**\n",
    "\n",
    "Write input/output pairs yourself. Best for critical use cases.\n",
    "\n",
    "### 2. üß† **AI-Generated Examples**\n",
    "\n",
    "Seed a few examples ‚Üí LangSmith will generate more using LLMs.\n",
    "\n",
    "### 3. üåê **Harvest From Production Traces**\n",
    "\n",
    "Take real user inputs/outputs from past traces ‚Üí upload to dataset.\n",
    "\n",
    "### 4. üìÇ **Import from CSV/JSON via Code or UI**\n",
    "\n",
    "Load structured examples from files.\n",
    "\n",
    "---\n",
    "\n",
    "# üè∑Ô∏è 4. **What Are Resource Tags in LangSmith Datasets?**\n",
    "\n",
    "Tags help **organize and version** your datasets.\n",
    "You can use tags like:\n",
    "\n",
    "* `\"baseline\"`\n",
    "* `\"version-2\"`\n",
    "* `\"priority-urgent\"`\n",
    "* `\"retriever-only\"`\n",
    "\n",
    "> üìå Tags make it easier to **filter datasets**, **run evaluations** on specific sets, or **compare performance** across different versions.\n",
    "\n",
    "---\n",
    "\n",
    "# üîß 5. **How to Create an Empty Dataset + Upload via Python (Jupyter)**\n",
    "\n",
    "Let‚Äôs go step-by-step.\n",
    "\n",
    "### üõ†Ô∏è Step 1: Create an Empty Dataset\n",
    "\n",
    "```python\n",
    "from langsmith.client import Client\n",
    "\n",
    "client = Client()\n",
    "dataset = client.create_dataset(\n",
    "    name=\"rag-eval-dataset\",\n",
    "    description=\"Dataset to evaluate RAG responses\"\n",
    ")\n",
    "print(dataset.id)  # You'll use this in the next step\n",
    "```\n",
    "\n",
    "### üõ†Ô∏è Step 2: Add Examples from Notebook\n",
    "\n",
    "```python\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": {\"question\": \"What is the capital of France?\"},\n",
    "        \"output\": {\"answer\": \"Paris\"}\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"Who discovered gravity?\"},\n",
    "        \"output\": {\"answer\": \"Isaac Newton\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "client.create_examples(\n",
    "    inputs_outputs=examples,\n",
    "    dataset_id=dataset.id\n",
    ")\n",
    "```\n",
    "\n",
    "### üõ†Ô∏è Step 3: Tag the Dataset Version\n",
    "\n",
    "```python\n",
    "client.tag_resource(resource_type=\"dataset\", resource_id=dataset.id, tags=[\"v1.0\", \"baseline\"])\n",
    "```\n",
    "\n",
    "‚úÖ Now you can filter by `\"v1.0\"` tag in the LangSmith UI.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ 6. **What‚Äôs the Purpose of Tags?**\n",
    "\n",
    "* **Version control**: You may change examples later ‚Äî tags mark versions\n",
    "* **Subset evaluation**: Run eval only on examples with `\"hard\"` or `\"baseline\"`\n",
    "* **Clarity**: Track progress across experiments (e.g., `v1`, `v2`, `fine-tuned`)\n",
    "\n",
    "---\n",
    "\n",
    "# üîÅ 7. **Adding Trace Input/Output as Golden Examples**\n",
    "\n",
    "Sometimes, your production app does something really well. You want that:\n",
    "\n",
    "* Input/Output pair\n",
    "* Saved as a **golden example** for evaluation\n",
    "\n",
    "### ‚úÖ Why do this?\n",
    "\n",
    "* You convert **real success cases** into **tests**\n",
    "* Ensures future model versions don‚Äôt regress\n",
    "\n",
    "```python\n",
    "trace = client.read_run(\"trace_id\")\n",
    "client.create_example(\n",
    "    inputs=trace.inputs,\n",
    "    outputs=trace.outputs,\n",
    "    dataset_id=your_dataset_id\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîÑ 8. **Create Separate Datasets for Subruns (Retriever / Generator)**\n",
    "\n",
    "LangSmith lets you evaluate **components separately**:\n",
    "\n",
    "* Dataset A ‚Üí only for retriever (input = query, output = expected docs)\n",
    "* Dataset B ‚Üí only for generator (input = retrieved docs + question, output = final answer)\n",
    "\n",
    "### üß† Why?\n",
    "\n",
    "* Helps you **pinpoint where failures occur**\n",
    "* You might find retriever is weak, but LLM is fine (or vice versa)\n",
    "\n",
    "‚úÖ Use **Input schema / Output schema** to enforce structure per dataset.\n",
    "\n",
    "---\n",
    "\n",
    "# üß© 9. **What Are Input/Output Schemas in Datasets?**\n",
    "\n",
    "A **schema** is like a contract: defines the fields, types, and structure of `input` and `output`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```json\n",
    "Input schema:\n",
    "{\n",
    "  \"question\": \"string\",\n",
    "  \"user_id\": \"string\"\n",
    "}\n",
    "\n",
    "Output schema:\n",
    "{\n",
    "  \"answer\": \"string\",\n",
    "  \"confidence\": \"float\"\n",
    "}\n",
    "```\n",
    "\n",
    "‚úÖ Benefits:\n",
    "\n",
    "* Prevent malformed examples\n",
    "* UI shows structured fields\n",
    "* Better validations in evals\n",
    "\n",
    "---\n",
    "\n",
    "# üß† 10. **Generate New Examples Using Existing Dataset (AI-Assisted)**\n",
    "\n",
    "LangSmith can **generate more examples** using:\n",
    "\n",
    "* Seed dataset\n",
    "* A prompting LLM\n",
    "* Specified variation type (e.g., \"generate 5 harder questions\")\n",
    "\n",
    "```python\n",
    "client.generate_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    num_examples=5,\n",
    "    generation_mode=\"diverse\",\n",
    "    tags=[\"auto-generated\"]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üìö 11. **Dataset Versions & Growth Over Time**\n",
    "\n",
    "* You can keep **tagged versions** like `v1.0`, `v2.0`\n",
    "* Add new examples over time as your app evolves\n",
    "* Helps with **progress tracking** and **regression testing**\n",
    "\n",
    "üß† Example:\n",
    "\n",
    "* `v1.0` ‚Üí 20 simple Q\\&A\n",
    "* `v2.0` ‚Üí adds 30 medium + 10 hard questions\n",
    "\n",
    "You can compare performance between versions.\n",
    "\n",
    "---\n",
    "\n",
    "# üìä 12. **Splitting Dataset by Priority**\n",
    "\n",
    "You can tag examples as:\n",
    "\n",
    "* `\"crucial\"`\n",
    "* `\"important\"`\n",
    "* `\"optional\"`\n",
    "\n",
    "### ‚úÖ Why?\n",
    "\n",
    "* Focus evals on the most mission-critical items\n",
    "* Score model on `\"crucial\"` subset first\n",
    "* Avoid wasting compute on low-value tests\n",
    "\n",
    "---\n",
    "\n",
    "# üîÅ 13. **Clone, Download, Share Datasets Across Projects**\n",
    "\n",
    "* **Clone** a dataset within the UI\n",
    "* **Download** examples as JSON\n",
    "* **Use in other projects** (like fine-tuning or external evaluation)\n",
    "\n",
    "```python\n",
    "client.export_dataset(dataset_id)\n",
    "client.import_dataset(name=\"cloned-dataset\", examples=exported_examples)\n",
    "```\n",
    "\n",
    "‚úÖ This makes it easy to **reuse proven datasets** across different apps.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Must-Know Questions to Be Ready\n",
    "\n",
    "1. Why are datasets essential in evaluating LLM apps?\n",
    "2. What are different ways to create datasets in LangSmith?\n",
    "3. What is the purpose of tagging a dataset or example?\n",
    "4. How can you create a dataset and upload examples via code?\n",
    "5. Why would you split your dataset into Retriever and Generator tests?\n",
    "6. What is the purpose of schema in a dataset?\n",
    "7. How can AI generate more dataset examples in LangSmith?\n",
    "8. How do versions help in maintaining dataset quality?\n",
    "9. How can you reuse datasets across different projects?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30403389",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. **Why is evaluation necessary for LLM apps?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "LLMs are **non-deterministic** ‚Äî the same input may return different outputs across runs. Without proper evaluation:\n",
    "\n",
    "* You cannot **reliably measure performance**.\n",
    "* You won't catch **regressions**.\n",
    "* You won‚Äôt know if changes are **helping or hurting**.\n",
    "\n",
    "### ‚úÖ Example:\n",
    "\n",
    "Suppose you fine-tune a prompt for better accuracy, but suddenly hallucinations increase. Evaluation is how you **catch** that problem before deploying.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. **What are LangSmith Datasets used for?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "LangSmith Datasets are **collections of input/output examples** used to:\n",
    "\n",
    "* **Test and benchmark** your app‚Äôs behavior\n",
    "* Run **automated and manual evaluations**\n",
    "* Use **real data or synthetic examples** for repeatable tests\n",
    "\n",
    "These datasets can be tagged, versioned, and reused across models or prompt versions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. **How do you create a dataset from production traces?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "You can convert actual app runs (traces) into evaluation examples.\n",
    "\n",
    "### ‚úÖ Code Example:\n",
    "\n",
    "```python\n",
    "run = client.read_run(\"trace_id\")  # From your logs\n",
    "client.create_example(\n",
    "    inputs=run.inputs,\n",
    "    outputs=run.outputs,\n",
    "    dataset_id=\"your-dataset-id\",\n",
    "    tags=[\"production\", \"gold\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* Because real user interactions are **high-quality test cases**.\n",
    "* They help detect **real-world regressions** after changes.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. **Why separate retriever and generator datasets in RAG applications?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "A RAG system has two key components:\n",
    "\n",
    "* **Retriever** (fetches relevant documents)\n",
    "* **Generator** (forms a final answer using context)\n",
    "\n",
    "Creating separate datasets helps:\n",
    "\n",
    "* **Isolate failure points**: is it bad retrieval or bad generation?\n",
    "* **Individually fine-tune** and evaluate each component\n",
    "\n",
    "### ‚úÖ Scenario:\n",
    "\n",
    "* `Retriever Dataset`: Input is `query`, output is `retrieved_docs`\n",
    "* `Generator Dataset`: Input is `retrieved_docs + query`, output is `final answer`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. **How do schema definitions help in dataset quality?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "Input/output schemas define **structure and field types** for your examples. It helps:\n",
    "\n",
    "* Prevent malformed inputs (e.g., missing keys)\n",
    "* Ensure consistency during evaluations\n",
    "* Make automated tools work correctly\n",
    "\n",
    "### ‚úÖ Example:\n",
    "\n",
    "If input schema requires:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"string\",\n",
    "  \"metadata\": \"object\"\n",
    "}\n",
    "```\n",
    "\n",
    "Then LangSmith will **validate each example** to match this. It‚Äôs like a contract for evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 6. **What is the role of tags in LangSmith datasets?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "Tags are **labels** to help:\n",
    "\n",
    "* **Version** datasets (`v1`, `v2`)\n",
    "* **Prioritize** examples (`critical`, `edge`, `simple`)\n",
    "* **Filter** them during evaluation runs\n",
    "\n",
    "### ‚úÖ Example:\n",
    "\n",
    "Tag 10 examples as `\"edge-case\"` and run evaluation **only on them** before deploying a new model.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 7. **How do you manage dataset versioning and growth?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "* Use **tags** like `v1`, `v2.1`, `baseline`, `optimized` to mark dataset stages\n",
    "* Keep old versions for **comparison**\n",
    "* Add new examples or improve outputs over time without deleting old data\n",
    "\n",
    "### ‚úÖ Tip:\n",
    "\n",
    "You can use `\"versions\"` to **compare how a model or prompt performs** on the same examples across time.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 8. **How can you prioritize or filter dataset examples?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "Use **tags** like:\n",
    "\n",
    "* `critical`: absolutely must pass\n",
    "* `medium`: nice-to-have correct\n",
    "* `low`: basic sanity\n",
    "\n",
    "Then use LangSmith UI or SDK to run evaluation on only selected **tag groups**.\n",
    "\n",
    "### ‚úÖ Scenario:\n",
    "\n",
    "Run `critical` set every night, full dataset every weekend.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 9. **How do you use AI to generate dataset examples?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "LangSmith can **auto-generate synthetic examples** from a few seed examples.\n",
    "\n",
    "```python\n",
    "client.generate_examples(\n",
    "    dataset_id=your_dataset_id,\n",
    "    num_examples=5,\n",
    "    generation_mode=\"diverse\",\n",
    "    tags=[\"ai-generated\"]\n",
    ")\n",
    "```\n",
    "\n",
    "### ‚úÖ Use Cases:\n",
    "\n",
    "* Expand coverage\n",
    "* Get variations for robustness testing\n",
    "* Save time in curating data\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 10. **How would you export a dataset and reuse in another project?**\n",
    "\n",
    "### ‚ú≥Ô∏è Answer:\n",
    "\n",
    "#### To export:\n",
    "\n",
    "```python\n",
    "data = client.export_dataset(dataset_id=\"your-id\")\n",
    "with open(\"my_dataset.json\", \"w\") as f:\n",
    "    json.dump(data, f)\n",
    "```\n",
    "\n",
    "#### To clone or use elsewhere:\n",
    "\n",
    "* Upload it via UI or SDK\n",
    "* Use in other chains, evaluations, or tools\n",
    "\n",
    "LangSmith makes datasets **portable**, which is helpful for:\n",
    "\n",
    "* Cross-project sharing\n",
    "* Collaboration\n",
    "* Benchmarking on same test set across multiple LLMs\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92544067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
