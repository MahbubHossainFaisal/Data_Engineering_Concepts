{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b051a1d7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ” What is a \"Run Type\" in LangSmith?\n",
    "\n",
    "A **run** in LangSmith represents a **step** in the execution of your GenAI application â€” this could be calling an LLM, retrieving documents, parsing a response, etc.\n",
    "\n",
    "LangSmith classifies each of these runs by **type** to help:\n",
    "\n",
    "* Visualize trace trees meaningfully\n",
    "* Debug parts more easily\n",
    "* Group similar runs for performance/cost analysis\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© 1. **LLM Run**\n",
    "\n",
    "### ğŸ“˜ **What it is**:\n",
    "\n",
    "A run that represents a **Large Language Model** (LLM) call â€” like OpenAI, Anthropic, Mistral, etc.\n",
    "\n",
    "### ğŸ§  **Use Case**:\n",
    "\n",
    "Every time you **call an LLM to generate** output (e.g., summary, response, SQL generation), LangSmith creates an **LLM-type run**.\n",
    "\n",
    "### ğŸ”§ **How it looks in LangSmith**:\n",
    "\n",
    "* Clearly labeled as `LLM`\n",
    "* Shows input messages or prompts\n",
    "* Shows temperature, model name, tokens, latency\n",
    "\n",
    "### ğŸ“ **Example**:\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "llm.predict(\"Summarize the following article...\")\n",
    "```\n",
    "\n",
    "This creates a **LLM Run** in the trace tree under your main Chain.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š 2. **Retriever Run**\n",
    "\n",
    "### ğŸ“˜ **What it is**:\n",
    "\n",
    "This represents a **retrieval call** to fetch documents from a vector database or search index (like FAISS, Pinecone, Weaviate, etc.)\n",
    "\n",
    "### ğŸ§  **Use Case**:\n",
    "\n",
    "In **RAG (Retrieval-Augmented Generation)**, this run tells you what docs were fetched and why.\n",
    "\n",
    "### ğŸ”§ **How LangSmith handles it**:\n",
    "\n",
    "* Shows query passed\n",
    "* Shows the list of retrieved documents\n",
    "* Time taken to retrieve\n",
    "\n",
    "### ğŸ“ **Example**:\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "retriever = faiss.as_retriever()\n",
    "retriever.get_relevant_documents(\"What is LangSmith?\")\n",
    "```\n",
    "\n",
    "LangSmith creates a **Retriever Run**, showing:\n",
    "\n",
    "* Query string\n",
    "* Retrieved docs\n",
    "* Embedding model info (if passed via metadata)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ 3. **Tool Run**\n",
    "\n",
    "### ğŸ“˜ **What it is**:\n",
    "\n",
    "A **Tool Run** occurs when an **Agent calls a tool** during decision-making.\n",
    "\n",
    "### ğŸ§  **Use Case**:\n",
    "\n",
    "When using LangChainâ€™s Agent architecture, the agent often decides to call tools like:\n",
    "\n",
    "* Calculator\n",
    "* Search\n",
    "* Weather\n",
    "* Custom API tools\n",
    "\n",
    "### ğŸ”§ **How LangSmith visualizes it**:\n",
    "\n",
    "* Shows **input to tool**\n",
    "* Shows **output from tool**\n",
    "* Captures **tool name**\n",
    "\n",
    "### ğŸ“ **Example**:\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "tools = [Tool(name=\"Calculator\", func=my_calc_func, description=\"...\")]\n",
    "agent = initialize_agent(tools=tools, llm=llm, agent_type=\"zero-shot-react-description\")\n",
    "\n",
    "agent.run(\"What is 5 * 11?\")\n",
    "```\n",
    "\n",
    "Youâ€™ll see:\n",
    "\n",
    "* `Tool Run` for `Calculator`\n",
    "* Input: `\"5 * 11\"`\n",
    "* Output: `\"55\"`\n",
    "\n",
    "---\n",
    "\n",
    "## â›“ï¸ 4. **Chain Run** (Default Type)\n",
    "\n",
    "### ğŸ“˜ **What it is**:\n",
    "\n",
    "Any **LangChain chain or function** you decorate with `@traceable` becomes a **Chain run** by default.\n",
    "\n",
    "### ğŸ§  **Use Case**:\n",
    "\n",
    "* You write a function that glues other parts: retrieval + LLM + formatting â†’ that's a chain.\n",
    "* This is often the **root** of your trace.\n",
    "\n",
    "### ğŸ”§ **Why Important**:\n",
    "\n",
    "Chains are the **structural units** in your trace tree â€” they often contain:\n",
    "\n",
    "* LLM Runs\n",
    "* Retriever Runs\n",
    "* Prompt Runs\n",
    "* Tool Runs\n",
    "\n",
    "### ğŸ“ **Example**:\n",
    "\n",
    "```python\n",
    "from langsmith.traceable import traceable\n",
    "\n",
    "@traceable(name=\"answer_question_chain\")\n",
    "def answer_question(user_input: str) -> str:\n",
    "    docs = retriever.get_relevant_documents(user_input)\n",
    "    return llm.predict(f\"Answer based on: {docs}\")\n",
    "```\n",
    "\n",
    "This entire logic gets traced as a **Chain Run**, which contains nested **Retriever + LLM runs**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ 5. **Prompt Run**\n",
    "\n",
    "### ğŸ“˜ **What it is**:\n",
    "\n",
    "A **Prompt Run** represents the stage where a **prompt template is being filled or formatted** before being sent to the LLM.\n",
    "\n",
    "### ğŸ§  **Use Case**:\n",
    "\n",
    "You might have complex templates with variables. If something goes wrong, prompt run helps debug it.\n",
    "\n",
    "### ğŸ”§ **What it captures**:\n",
    "\n",
    "* Template structure\n",
    "* Final filled prompt\n",
    "* Variables used\n",
    "\n",
    "### ğŸ“ **Example**:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Answer based on: {context}\\nQuestion: {question}\")\n",
    "formatted = prompt.format(context=\"Earth is round\", question=\"What is Earthâ€™s shape?\")\n",
    "```\n",
    "\n",
    "LangSmith records:\n",
    "\n",
    "* Template: `\"Answer based on: {context}\\nQuestion: {question}\"`\n",
    "* Final Prompt: `\"Answer based on: Earth is round\\nQuestion: What is Earthâ€™s shape?\"`\n",
    "\n",
    "Useful for prompt debugging!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ 6. **Parser Run**\n",
    "\n",
    "### ğŸ“˜ **What it is**:\n",
    "\n",
    "When you **parse** the raw output from an LLM (JSON, text, SQL), LangSmith can log it as a **Parser Run**.\n",
    "\n",
    "### ğŸ§  **Use Case**:\n",
    "\n",
    "* LLM outputs structured data\n",
    "* You extract key fields\n",
    "* If parsing fails, LangSmith shows it clearly\n",
    "\n",
    "### ğŸ”§ **Typical scenario**:\n",
    "\n",
    "* Parsing with `StructuredOutputParser`\n",
    "* Custom parsing logic\n",
    "\n",
    "### ğŸ“ **Example**:\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schema(schema)\n",
    "result = parser.parse(llm_output)\n",
    "```\n",
    "\n",
    "LangSmith captures:\n",
    "\n",
    "* Raw LLM output\n",
    "* Parsing function name\n",
    "* Final structured result\n",
    "\n",
    "If parsing fails (e.g., invalid JSON), you can debug **directly in the trace**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Summary Table\n",
    "\n",
    "| **Run Type** | **What It Traces**                        | **Why Itâ€™s Useful**                                     |\n",
    "| ------------ | ----------------------------------------- | ------------------------------------------------------- |\n",
    "| `LLM`        | LLM generation calls                      | See prompts, responses, cost, and latency               |\n",
    "| `Retriever`  | Document retrieval logic                  | Debug RAG performance, trace missing docs               |\n",
    "| `Tool`       | Agent tool calls                          | Monitor tool decisions and outputs                      |\n",
    "| `Chain`      | Composite logic (default with @traceable) | Structure the full flow, contains nested runs           |\n",
    "| `Prompt`     | Template formatting                       | Ensure prompt accuracy and debug inputs                 |\n",
    "| `Parser`     | Parsing model outputs                     | Trace data transformation, spot malformed output issues |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Case Study: Full Trace Tree\n",
    "\n",
    "```\n",
    "Root (Chain): answer_question_chain\n",
    "â”œâ”€â”€ Prompt: filled user + system prompt\n",
    "â”œâ”€â”€ Retriever: get_relevant_documents(query)\n",
    "â”‚   â””â”€â”€ Metadata: {\"retriever\": \"Weaviate\"}\n",
    "â”œâ”€â”€ LLM: OpenAI GPT-4\n",
    "â”‚   â””â”€â”€ Input: Prompt text\n",
    "â”‚   â””â”€â”€ Output: Model answer\n",
    "â””â”€â”€ Parser: JSON to dict (StructuredOutputParser)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Must-Know Questions\n",
    "\n",
    "1. What are the 6 run types in LangSmith and their roles?\n",
    "2. What run type does `@traceable` default to, and how can you customize it?\n",
    "3. How do nested run types help you debug a complex agent?\n",
    "4. If your RAG pipeline is not returning documents, which run type should you inspect?\n",
    "5. How would you debug prompt formatting issues using LangSmith?\n",
    "6. How does a parser run help avoid silent failures in structured output?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be6dc2a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### âœ… **1. What are the 6 run types in LangSmith and their roles?**\n",
    "\n",
    "| **Run Type**  | **Role**                                                                                                   |\n",
    "| ------------- | ---------------------------------------------------------------------------------------------------------- |\n",
    "| **LLM**       | Traces calls to a language model (e.g., GPT-4, Claude) to generate text.                                   |\n",
    "| **Retriever** | Traces document retrieval logic â€” when documents are pulled from vector DBs or search engines.             |\n",
    "| **Tool**      | Traces external tools used by agents, e.g., calculator, search tool, or custom APIs.                       |\n",
    "| **Chain**     | Represents a composite logical unit like a pipeline â€” default for `@traceable`-decorated functions.        |\n",
    "| **Prompt**    | Captures prompt template rendering â€” i.e., what was the final prompt given to the LLM.                     |\n",
    "| **Parser**    | Captures output parsing logic â€” e.g., converting JSON strings or structured responses into usable objects. |\n",
    "\n",
    "ğŸ“˜ **Why important?**\n",
    "LangSmith categorizes these run types so developers can isolate and debug individual stages in a multi-step GenAI pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **2. What run type does `@traceable` default to, and how can you customize it?**\n",
    "\n",
    "* By default, `@traceable` logs a run as type **`chain`**.\n",
    "* This makes sense because a decorated function usually wraps a *pipeline* or composite step.\n",
    "\n",
    "#### ğŸ› ï¸ Customize it like this:\n",
    "\n",
    "```python\n",
    "@traceable(name=\"parse_tool_output\", run_type=\"parser\")\n",
    "def parse_output(output):\n",
    "    ...\n",
    "```\n",
    "\n",
    "âœ”ï¸ This flexibility allows you to **accurately represent the functionâ€™s intent** in your trace tree, improving readability and debugging.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **3. How do nested run types help you debug a complex agent?**\n",
    "\n",
    "When using LangSmith, **nested runs** give you a **call hierarchy**.\n",
    "\n",
    "Imagine a multi-step agent:\n",
    "\n",
    "```\n",
    "Root Chain (Agent Execution)\n",
    "â”œâ”€â”€ Tool Run (Search API)\n",
    "â”‚   â””â”€â”€ LLM Run (Rephrasing the search result)\n",
    "â”œâ”€â”€ Tool Run (Calculator)\n",
    "â”œâ”€â”€ LLM Run (Final synthesis)\n",
    "```\n",
    "\n",
    "With nesting:\n",
    "\n",
    "* You **see the tool calls** and their responses.\n",
    "* You **see which LLM calls** are being made at which steps.\n",
    "* You **track latency per component**.\n",
    "\n",
    "âœ… This helps isolate bugs like:\n",
    "\n",
    "* Tool call failed? Go to Tool Run.\n",
    "* Response format broke? Go to Parser Run.\n",
    "* Wrong reasoning? Go to LLM Run.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **4. If your RAG pipeline is not returning documents, which run type should you inspect?**\n",
    "\n",
    "ğŸ‘‰ You should **inspect the `Retriever` run**.\n",
    "\n",
    "This will show:\n",
    "\n",
    "* What query was passed to the retriever\n",
    "* What documents were returned (if any)\n",
    "* If there was a timeout or API error\n",
    "* Metadata like retriever type or vector DB used\n",
    "\n",
    "#### ğŸ“ Example:\n",
    "\n",
    "```python\n",
    "@traceable\n",
    "def retrieve(user_query):\n",
    "    return retriever.get_relevant_documents(user_query)\n",
    "```\n",
    "\n",
    "ğŸ§  If zero documents are returned, maybe:\n",
    "\n",
    "* The embedding is off\n",
    "* The query is poorly formatted\n",
    "* The DB is misconfigured\n",
    "\n",
    "LangSmith helps by **logging all of this visibly** under the Retriever Run node.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **5. How would you debug prompt formatting issues using LangSmith?**\n",
    "\n",
    "Youâ€™d inspect the **Prompt Run**.\n",
    "\n",
    "It will show:\n",
    "\n",
    "* The prompt **template** (with placeholders)\n",
    "* The final **rendered prompt** (after variable substitution)\n",
    "* All **inputs passed** for formatting\n",
    "\n",
    "#### ğŸ“ Example:\n",
    "\n",
    "```python\n",
    "Prompt Template:\n",
    "\"Answer using context: {context}\\nQuestion: {question}\"\n",
    "\n",
    "Filled Prompt:\n",
    "\"Answer using context: \\nQuestion: What is LangSmith?\"\n",
    "```\n",
    "\n",
    "ğŸ¤¯ A-ha! `context` was empty â€” which is why the LLM hallucinated. Without prompt runs, this would be hard to catch.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **6. How does a parser run help avoid silent failures in structured output?**\n",
    "\n",
    "LLMs often return **structured responses** like JSON. If the format is invalid and your parser crashes, you may not catch the bug unless it's logged.\n",
    "\n",
    "The **Parser Run**:\n",
    "\n",
    "* Traces your parsing logic\n",
    "* Captures the **raw LLM output**\n",
    "* Captures the **parsed result**\n",
    "* Logs any **errors** (e.g., `JSONDecodeError`)\n",
    "\n",
    "#### ğŸ“ Example:\n",
    "\n",
    "```python\n",
    "@traceable(run_type=\"parser\")\n",
    "def parse_json_response(text):\n",
    "    return json.loads(text)\n",
    "```\n",
    "\n",
    "LangSmith will show:\n",
    "\n",
    "* Input: `'{\"answer\": \"42\"}'`\n",
    "* Output: `{\"answer\": \"42\"}`\n",
    "\n",
    "Or, if LLM returns: `\"The answer is 42\"` â†’ âŒ JSON parsing fails\n",
    "ğŸ‘‰ You now know exactly **where** and **why** the error occurred.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Final Summary of All Answers\n",
    "\n",
    "| **Question**     | **Main Takeaway**                                                              |\n",
    "| ---------------- | ------------------------------------------------------------------------------ |\n",
    "| Run types        | LLM, Retriever, Tool, Chain, Prompt, Parser â€” each has a clear debugging role. |\n",
    "| Default run type | `@traceable` = Chain by default, can be customized.                            |\n",
    "| Nested runs      | Help trace and debug complex agents or RAG pipelines.                          |\n",
    "| RAG bug?         | Check `Retriever` run to debug document fetch issues.                          |\n",
    "| Prompt bug?      | Check `Prompt` run to inspect final rendered prompts.                          |\n",
    "| Parsing bug?     | Use `Parser` run to debug structured output issues.                            |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be824f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
