{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5780571a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üëá **What We'll Learn**\n",
    "\n",
    "1. What are Pairwise Experiments?\n",
    "2. Why and when they are **crucial**\n",
    "3. What you need to **run them effectively**\n",
    "4. Full Python code example with explanations\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ 1. What Are Pairwise Experiments?\n",
    "\n",
    "> **Definition:**\n",
    "> A **Pairwise Experiment** is a method of evaluation where **two outputs (A & B)** from different versions of your LLM app are **compared against each other**, not just scored in isolation.\n",
    "\n",
    "Instead of asking:\n",
    "\n",
    "> ‚ÄúIs A good?‚Äù\n",
    "\n",
    "We ask:\n",
    "\n",
    "> ‚ÄúIs A better than B?‚Äù\n",
    "\n",
    "üîç This mimics how **humans compare choices** in real life (and in A/B testing).\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 2. Why Are Pairwise Experiments Crucial?\n",
    "\n",
    "### ‚úÖ In real-world GenAI apps:\n",
    "\n",
    "* Outputs may **both seem okay**, but one may be **subtly better**.\n",
    "* LLM scoring in isolation may **not catch nuances**.\n",
    "* Helps when outputs are **non-deterministic**, e.g.,:\n",
    "\n",
    "  * Summaries\n",
    "  * Answers to complex questions\n",
    "  * Chat responses\n",
    "  * RAG results\n",
    "\n",
    "### üî¥ Problems with isolated scoring:\n",
    "\n",
    "| Method        | Problem                                   |\n",
    "| ------------- | ----------------------------------------- |\n",
    "| LLM-as-Judge  | May score both A and B highly             |\n",
    "| Exact Match   | May fail if both are semantically correct |\n",
    "| Embedding Sim | Cannot tell which phrasing is clearer     |\n",
    "\n",
    "üí° **Pairwise helps** answer: *\"Which version should I ship?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ 3. What Do You Need for Pairwise Experiments?\n",
    "\n",
    "### ‚úÖ A. **Two versions of your app**\n",
    "\n",
    "You compare:\n",
    "\n",
    "* Old prompt vs new prompt\n",
    "* Model v1 vs v2\n",
    "* Retrieval method A vs B\n",
    "\n",
    "Each version should output predictions **for the same input dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ B. **The same dataset**\n",
    "\n",
    "Use **one dataset**, and evaluate:\n",
    "\n",
    "* Version A ‚Üí Prediction A\n",
    "* Version B ‚Üí Prediction B\n",
    "* Now compare Prediction A vs B\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ C. **A pairwise evaluator**\n",
    "\n",
    "Most commonly:\n",
    "\n",
    "* `LLM-as-Judge` evaluator prompt:\n",
    "\n",
    "  > ‚ÄúBetween Output A and Output B, which is more helpful, accurate, and fluent?‚Äù\n",
    "\n",
    "LangSmith handles the internal scoring as:\n",
    "\n",
    "* `A wins`, `B wins`, or `Tie`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ D. **Run group metadata**\n",
    "\n",
    "To tell LangSmith:\n",
    "\n",
    "* ‚ÄúThese two runs belong to the same input, but different versions.‚Äù\n",
    "\n",
    "You use:\n",
    "\n",
    "```python\n",
    "run_group_id = uuid.uuid4().hex\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. Code: Running Pairwise Experiments in LangSmith\n",
    "\n",
    "```python\n",
    "import uuid\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import RunEvaluator\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Your dataset name\n",
    "dataset_name = \"chatbot-qa-dataset\"\n",
    "\n",
    "# Choose your models or chains\n",
    "model_a = \"gpt-4-prompt-v1\"\n",
    "model_b = \"gpt-4-prompt-v2\"\n",
    "\n",
    "# Shared ID to group paired runs\n",
    "run_group_id = uuid.uuid4().hex\n",
    "\n",
    "# Step 1: Run both versions\n",
    "project_a = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    model=model_a,\n",
    "    project_name=\"version-a\",\n",
    "    metadata={\"version\": \"A\", \"run_group\": run_group_id}\n",
    ")\n",
    "\n",
    "project_b = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    model=model_b,\n",
    "    project_name=\"version-b\",\n",
    "    metadata={\"version\": \"B\", \"run_group\": run_group_id}\n",
    ")\n",
    "\n",
    "# Step 2: Run pairwise comparison\n",
    "pairwise_evaluator = RunEvaluator.for_type(\"pairwise_string_comparison\")\n",
    "\n",
    "client.evaluate_pairwise(\n",
    "    run_groups=[[project_a.project_name, project_b.project_name]],\n",
    "    evaluator=pairwise_evaluator,\n",
    "    dataset_name=dataset_name,\n",
    "    metadata={\"experiment_type\": \"pairwise\", \"run_group\": run_group_id},\n",
    "    project_name=\"pairwise-compare-v1-v2\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç What Happens Internally?\n",
    "\n",
    "1. LangSmith finds matching runs by `input` across both versions.\n",
    "2. Sends output A and B to the evaluator prompt.\n",
    "3. Evaluator (usually GPT-4) returns:\n",
    "\n",
    "   * \"A is better\"\n",
    "   * \"B is better\"\n",
    "   * \"Tie\"\n",
    "4. You get results in LangSmith dashboard:\n",
    "\n",
    "   * Win rate %\n",
    "   * Score distribution\n",
    "   * Comments / justifications\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary: When Should You Use Pairwise Experiments?\n",
    "\n",
    "| Scenario                                                | Should you use pairwise? |\n",
    "| ------------------------------------------------------- | ------------------------ |\n",
    "| You changed a **prompt**?                               | ‚úÖ Yes                    |\n",
    "| You upgraded the **model** (GPT-3.5 ‚Üí GPT-4)?           | ‚úÖ Yes                    |\n",
    "| You optimized your **retrieval chain**?                 | ‚úÖ Yes                    |\n",
    "| You just want **exact match accuracy**?                 | ‚ùå Not needed             |\n",
    "| You are doing **creative generation or summarization**? | ‚úÖ Definitely yes         |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Must-Know Reflection Questions:\n",
    "\n",
    "1. ‚úÖ Why is pairwise comparison better than isolated scoring in LLM evaluations?\n",
    "2. ‚úÖ What is the role of `run_group_id` in pairwise experiments?\n",
    "3. ‚úÖ Can you run pairwise on more than two versions?\n",
    "4. ‚úÖ When would you prefer traditional evaluation over pairwise?\n",
    "5. ‚úÖ How does LangSmith group predictions for comparison?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c53e80",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **1. Why is pairwise comparison better than isolated scoring in LLM evaluations?**\n",
    "\n",
    "### ‚úÖ Answer:\n",
    "\n",
    "Because **LLM outputs are often ambiguous**, it's hard to tell if one is ‚Äúright‚Äù or ‚Äúwrong‚Äù in isolation. But it's **much easier to say which of two options is better**.\n",
    "\n",
    "### üîç Example:\n",
    "\n",
    "| Input: \"Summarize this paragraph\"                                                        |\n",
    "| ---------------------------------------------------------------------------------------- |\n",
    "| Version A: \"AI helps automate tasks and save time.\"                                      |\n",
    "| Version B: \"Artificial intelligence improves efficiency by automating repetitive tasks.\" |\n",
    "\n",
    "* Both are **correct**.\n",
    "* A standard evaluator might give both a score of 0.9.\n",
    "* But **pairwise comparison** lets you say:\n",
    "  üîπ ‚ÄúB is more detailed and clear than A.‚Äù\n",
    "\n",
    "### üí° Pairwise:\n",
    "\n",
    "* Mimics **real-world human judgment**\n",
    "* Captures **subtle quality differences**\n",
    "* More helpful in **creative or open-ended outputs**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **2. What is the role of `run_group_id` in pairwise experiments?**\n",
    "\n",
    "### ‚úÖ Answer:\n",
    "\n",
    "`run_group_id` is a **unique identifier** used to **link multiple runs** of the **same input** from different app versions.\n",
    "\n",
    "LangSmith uses it to:\n",
    "\n",
    "* Group predictions from **version A** and **version B**\n",
    "* Compare their outputs **side by side**\n",
    "* Evaluate them **together** rather than independently\n",
    "\n",
    "üîß Without `run_group_id`, LangSmith won‚Äôt know which outputs belong to the **same input**, and the pairwise evaluation would fail or mismatch.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **3. Can you run pairwise on more than two versions?**\n",
    "\n",
    "### ‚úÖ Answer:\n",
    "\n",
    "Yes ‚Äî **indirectly**.\n",
    "\n",
    "LangSmith officially supports **A vs B** pairwise comparison at a time. But if you have **3 or more versions**, you can run **multiple pairwise experiments**:\n",
    "\n",
    "* A vs B\n",
    "* A vs C\n",
    "* B vs C\n",
    "\n",
    "Then you **aggregate results** to find:\n",
    "\n",
    "* Which version wins the most?\n",
    "* Which version loses the most?\n",
    "* Which one ties most often?\n",
    "\n",
    "This is often called a **tournament-style evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **4. When would you prefer traditional evaluation over pairwise?**\n",
    "\n",
    "### ‚úÖ Answer:\n",
    "\n",
    "Use **traditional (isolated) evaluation** when:\n",
    "\n",
    "| Situation                              | Use traditional evaluators |\n",
    "| -------------------------------------- | -------------------------- |\n",
    "| Deterministic tasks (math, names)      | ‚úÖ Yes                      |\n",
    "| You care about **exact correctness**   | ‚úÖ Yes                      |\n",
    "| You want **individual quality scores** | ‚úÖ Yes                      |\n",
    "| Need to automate CI/CD scoring         | ‚úÖ Yes                      |\n",
    "\n",
    "Use **pairwise** when:\n",
    "\n",
    "* Outputs are **non-deterministic** (summaries, chat)\n",
    "* You want to know **which version is better**\n",
    "* You want **human-style comparison**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **5. How does LangSmith group predictions for comparison?**\n",
    "\n",
    "### ‚úÖ Answer:\n",
    "\n",
    "LangSmith uses:\n",
    "\n",
    "* The same **input example** from the dataset\n",
    "* The **run\\_group\\_id** (if provided) or matches by input content\n",
    "* The **project names or metadata tags** to identify which output belongs to which version\n",
    "\n",
    "Then it pairs:\n",
    "\n",
    "* `prediction_A` (from version A)\n",
    "* `prediction_B` (from version B)\n",
    "\n",
    "‚Ä¶and passes them to an **LLM-based evaluator** (or custom logic) that compares and returns:\n",
    "\n",
    "* A wins ‚úÖ\n",
    "* B wins ‚úÖ\n",
    "* Tie ü§ù\n",
    "* Reasoning üí¨\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd507f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
