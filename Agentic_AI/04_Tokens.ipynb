{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48f7d5d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What is a **Token**?\n",
    "\n",
    "### âœ… Definition:\n",
    "\n",
    "> A **token** is the smallest unit of text that a language model understands and processes.\n",
    "\n",
    "It could be:\n",
    "\n",
    "* A word: `dog`\n",
    "* Part of a word: `unbelievable` â†’ `un`, `believ`, `able`\n",
    "* Punctuation: `.`, `,`, `!`\n",
    "* Even whitespace: ` ` is a token.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¦ Analogy: Token = Lego brick\n",
    "\n",
    "Imagine building text like Lego blocks.\n",
    "The sentence:\n",
    "\n",
    "> \"Natural language is fun!\"\n",
    "> Becomes these blocks (tokens):\n",
    "\n",
    "* `\"Natural\"`, `\" language\"`, `\" is\"`, `\" fun\"`, `\"!\"`\n",
    "\n",
    "Why the space in `\" language\"`? Because **tokenizers** preserve **space and punctuation** differently!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ How Are Tokens Counted?\n",
    "\n",
    "Large Language Models (LLMs) donâ€™t see sentences as words.\n",
    "They see a sequence of tokens like this:\n",
    "\n",
    "| Text             | Tokens                                   |\n",
    "| ---------------- | ---------------------------------------- |\n",
    "| \"I love AI\"      | `[\"I\", \" love\", \" AI\"]`                  |\n",
    "| \"Don't\"          | `[\"Don\", \"'\", \"t\"]`                      |\n",
    "| \"GPT-4 is cool.\" | `[\"GPT\", \"-\", \"4\", \" is\", \" cool\", \".\"]` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ How many tokens can LLMs handle?\n",
    "\n",
    "Each LLM has a **maximum context window**:\n",
    "\n",
    "| Model       | Max Tokens |\n",
    "| ----------- | ---------- |\n",
    "| GPT-3.5     | 4,096      |\n",
    "| GPT-4 Turbo | 128,000    |\n",
    "| Claude 2    | 100,000    |\n",
    "| Gemini Pro  | \\~32,000   |\n",
    "\n",
    "> This includes both your **prompt tokens** + **response tokens**!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Try it Yourself: Count Tokens\n",
    "\n",
    "Use `tiktoken` (OpenAI tokenizer):\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # for GPT-4\n",
    "tokens = encoding.encode(\"I love LangChain and LLMs ðŸ’¡\")\n",
    "\n",
    "print(\"Token count:\", len(tokens))\n",
    "print(\"Tokens:\", tokens)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Token count: 9\n",
    "Tokens: [40, 1655, 18202, 290, 16673, 105, 139, 3347, 520]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Why Tokens Matter in GenAI?\n",
    "\n",
    "| Use Case        | Why Tokens Are Critical                       |\n",
    "| --------------- | --------------------------------------------- |\n",
    "| LLM Prompting   | Prompt length limit is in tokens              |\n",
    "| Cost Estimation | You pay per token (for GPT, Claude, etc.)     |\n",
    "| Text Chunking   | Chunks must stay under token limits           |\n",
    "| Embedding       | Embedding models take max tokens (e.g., 8192) |\n",
    "| Performance     | Too many tokens = slow & expensive inference  |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Real Example: Tokens vs. Characters\n",
    "\n",
    "```python\n",
    "text = \"LangChain is amazing! ðŸ”¥\"\n",
    "\n",
    "print(\"Characters:\", len(text))  # 24\n",
    "print(\"Tokens:\", len(encoding.encode(text)))  # Could be 6-9\n",
    "```\n",
    "\n",
    "âš ï¸ Even if two sentences have same **character count**, they can have very **different token counts**!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Interview Questions to Prepare\n",
    "\n",
    "> These are MUST-KNOW for LLM Engineers and GenAI roles:\n",
    "\n",
    "1. **What is a token in an LLM?**\n",
    "2. **How do tokens differ from words and characters?**\n",
    "3. **Why is tokenization important in LLM pipelines?**\n",
    "4. **How do you count tokens before sending a prompt to GPT-4?**\n",
    "5. **What are the token limits of popular models?**\n",
    "6. **What happens if you exceed token limits?**\n",
    "7. **How does chunking relate to tokens and why not just use character-based chunks?**\n",
    "8. **How do emojis and Unicode characters affect token count?**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Bonus: Online Tokenizers\n",
    "\n",
    "Use these tools to visualize tokenization:\n",
    "\n",
    "* [OpenAI Tokenizer](https://platform.openai.com/tokenizer)\n",
    "* [tiktoken preview](https://tiktokenizer.vercel.app/)\n",
    "* [Anthropic tokenizer (Claude)](https://console.anthropic.com/tokenizer)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”š Summary\n",
    "\n",
    "| Concept            | Summary                                           |\n",
    "| ------------------ | ------------------------------------------------- |\n",
    "| Token              | Smallest unit of text processed by LLMs           |\n",
    "| Tokenizer          | Tool that converts text â†’ tokens                  |\n",
    "| Why Important?     | Cost, model limits, embedding, performance        |\n",
    "| Tools to Use       | `tiktoken`, OpenAI Tokenizer UI                   |\n",
    "| LLM-aware chunking | Always split based on token count, not characters |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06c11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
