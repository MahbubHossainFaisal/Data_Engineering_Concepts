{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8587d9b0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Part 1: LCEL â€” LangChain Expression Language\n",
    "\n",
    "### ðŸ”¹ **Definition of LCEL**\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is a **declarative**, **composable**, and **structured** way to define **prompt workflows** in LangChain.\n",
    "\n",
    "> ðŸ§  Think of LCEL as a mini-language or DSL (domain-specific language) **within Python** that allows you to build **modular AI pipelines** using building blocks like prompt templates, chains, messages, retrievers, tools, etc.\n",
    "\n",
    "It abstracts away the complexity of orchestration and **lets you define chains like a dataflow pipeline**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **Core Components of LCEL**\n",
    "\n",
    "Here are the essential components you must understand deeply:\n",
    "\n",
    "| Component                                           | Purpose                                                                                         |\n",
    "| --------------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| `PromptTemplate`                                    | Used to define templates for user/system messages.                                              |\n",
    "| `ChatPromptTemplate`                                | Combines different prompt messages (like system, human) into a single coherent chat input.      |\n",
    "| `SystemMessage`, `HumanMessage`, `AIMessage`        | Define the **role** of each message to control how LLMs behave.                                 |\n",
    "| `Runnable`                                          | Base interface for all chainable components. Anything \"runnable\" can be composed in a pipeline. |\n",
    "| `RunnableMap`, `RunnableLambda`, `RunnableSequence` | Used for more advanced composition: branching, mapping, sequencing, logic, etc.                 |\n",
    "| `.from_messages()` and `.to_messages()`             | Create or extract message sequences.                                                            |\n",
    "| `LCEL Chain`                                        | Composition of all above components to create a pipeline.                                       |\n",
    "\n",
    "> ðŸ§ª LCEL is designed with **composability** and **declarativity** in mindâ€”this makes debugging, testing, and managing workflows easier.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŸ© Part 2: GROQ Platform\n",
    "\n",
    "### ðŸ”¹ What is GROQ?\n",
    "\n",
    "**Groq** is a **hardware and software platform** built to deliver **ultra-low latency AI inference**, especially for LLMs and Gen AI workloads.\n",
    "\n",
    "Think of Groq as:\n",
    "\n",
    "* A **competitor to NVIDIA GPUs and Google TPUs**, but optimized for deterministic inference latency.\n",
    "* It uses its own **LPU (Language Processing Unit)**â€”a custom architecture purpose-built for inference.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ What Services Does Groq Provide?\n",
    "\n",
    "| Service                 | Description                                                                              |\n",
    "| ----------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| **GroqCloud**           | Access to Groq-powered inference as a service (API-based).                               |\n",
    "| **Groq LPU Hardware**   | Dedicated chips optimized for low-latency, high-throughput AI inference.                 |\n",
    "| **Developer SDK/API**   | Tools to integrate models like LLaMA, Mistral, or even custom models with Groq's engine. |\n",
    "| **Streaming Inference** | Near-instant token streaming, beating GPU inference in latency.                          |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ What Problem Does Groq Solve?\n",
    "\n",
    "Traditional GPU inference has **non-deterministic latency**, **batch dependencies**, and **latency spikes**.\n",
    "\n",
    "Groq solves:\n",
    "\n",
    "* â±ï¸ **Sub-10ms latency** per token.\n",
    "* âœ… **Deterministic output**: You get a predictable, real-time response.\n",
    "* ðŸŒ **Stateless Streaming**: Useful for real-time agents or APIs with Gen AI.\n",
    "* âš™ï¸ **Reduced costs for real-time inference** compared to GPU overload.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¸ What is AI Inference?\n",
    "\n",
    "> **Inference** is the **process of running a trained model on new data** to make predictions.\n",
    "\n",
    "In Gen AI:\n",
    "\n",
    "* You pass a **prompt** to an LLM (like GPT, Mistral, or LLaMA).\n",
    "* The LLM **infers the next token(s)** based on its training.\n",
    "\n",
    "Groq focuses on making **this inference process lightning-fast and predictable**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¸ Delivering Fast AI Inference with LPU\n",
    "\n",
    "### ðŸ’¡ LPU (Language Processing Unit)\n",
    "\n",
    "A **Language Processing Unit (LPU)** is Groq's specialized chip designed specifically for LLM inference.\n",
    "\n",
    "#### Key Innovations:\n",
    "\n",
    "1. **Deterministic Single-core Execution:**\n",
    "\n",
    "   * Unlike GPUs (which batch jobs), LPU runs everything **on a single core** with no context-switching.\n",
    "   * That means **no queue delays**.\n",
    "\n",
    "2. **Pipelined Execution:**\n",
    "\n",
    "   * Each token generation is pipelined like an assembly line, keeping throughput high.\n",
    "\n",
    "3. **Streaming-first Architecture:**\n",
    "\n",
    "   * Designed for **real-time streaming** of tokens instead of waiting for batch generation.\n",
    "   * Reduces latency for use-cases like chatbots, agents, or autonomous control.\n",
    "\n",
    "4. **Scales Horizontally:**\n",
    "\n",
    "   * Multiple LPUs can work together, making it cloud-scale.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš¦ Part 3: Deep Dive into LangChain + LCEL Components\n",
    "\n",
    "### ðŸ”¹ What is `langchain_core`?\n",
    "\n",
    "* `langchain_core` is a **minimal, foundational** package of LangChain.\n",
    "* It includes all the **LCEL interfaces, prompt/message definitions, and base classes** like `Runnable`, `ChatMessage`, etc.\n",
    "* It is intentionally kept **lean** to be **LLM-provider agnostic** and to support minimal dependencies.\n",
    "\n",
    "> Use `langchain_core` when you're building low-level chains or embedding LCEL into custom apps without bloated dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ `system_message` vs `human_message`\n",
    "\n",
    "#### âœ… What are they?\n",
    "\n",
    "* `system_message`: Sets behavior, tone, and instruction context for the model.\n",
    "* `human_message`: Actual input or query from the user.\n",
    "\n",
    "> **You cannot combine them into one message** because modern chat-based LLMs (like OpenAI, Anthropic) **expect separate roles** for proper context handling.\n",
    "\n",
    "#### ðŸ¤” Why not a single prompt?\n",
    "\n",
    "Because:\n",
    "\n",
    "* The model treats **system** and **human** differently during token attention.\n",
    "* System message is **not scored** for response generationâ€”it guides behavior.\n",
    "* If you merge them, the model wonâ€™t **differentiate between instruction vs question**, leading to unpredictable outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ Difference between using `SystemMessage` & `HumanMessage` separately vs in `from_messages()`\n",
    "\n",
    "#### ðŸ§ª Separate Construction (Manual):\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is LCEL?\")\n",
    "]\n",
    "```\n",
    "\n",
    "#### âœ… Using `ChatPromptTemplate.from_messages()`:\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"What is LCEL?\")\n",
    "])\n",
    "```\n",
    "\n",
    "**Advantage of `from_messages()`**:\n",
    "\n",
    "* Clean, declarative.\n",
    "* Composable and chainable.\n",
    "* Easier to manage long multi-turn dialogues.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ What does `from_messages()` do?\n",
    "\n",
    "* Converts a list of message roles and strings into a `ChatPromptTemplate`.\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant.\"),\n",
    "  (\"human\", \"Who is Alan Turing?\")\n",
    "])\n",
    "```\n",
    "\n",
    "### ðŸ”¸ What does `to_messages()` do?\n",
    "\n",
    "* Converts a prompt with variables to **runtime-ready `ChatMessages`**, filling in dynamic values.\n",
    "\n",
    "```python\n",
    "prompt.to_messages({\"name\": \"Alan Turing\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— What is the Need for Chains?\n",
    "\n",
    "Chains allow you to:\n",
    "\n",
    "* Break down your pipeline into **modular blocks**.\n",
    "* Compose:\n",
    "\n",
    "  * Prompt â†’ Model â†’ Output Parser\n",
    "  * Prompt â†’ Model â†’ Tool â†’ Follow-up Prompt\n",
    "* Add logic, memory, retrieval, tools in steps.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ Why chains make life easier (With Example):\n",
    "\n",
    "Without chains:\n",
    "\n",
    "```python\n",
    "# Manually do all steps\n",
    "prompt = \"Translate {text} to French\"\n",
    "formatted = prompt.format(text=\"Hello\")\n",
    "output = model.invoke(formatted)\n",
    "parsed = parse(output)\n",
    "```\n",
    "\n",
    "With LCEL Chain:\n",
    "\n",
    "```python\n",
    "chain = prompt_template | model | output_parser\n",
    "result = chain.invoke({\"text\": \"Hello\"})\n",
    "```\n",
    "\n",
    "Now your code:\n",
    "\n",
    "* Is readable ðŸ§¾\n",
    "* Is reusable ðŸ”\n",
    "* Can scale into agents and tool-using AI ðŸ¤–\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Important Questions\n",
    "\n",
    "1. **What is LCEL and how does it differ from traditional function composition?**\n",
    "2. **Why do we separate system and human messages in a chat prompt?**\n",
    "3. **Explain the lifecycle of a LangChain chain from prompt creation to output parsing.**\n",
    "4. **How does Groqâ€™s LPU differ from a traditional GPU in inference?**\n",
    "5. **How do `from_messages()` and `to_messages()` contribute to composability?**\n",
    "6. **What are the trade-offs of using `langchain_core` vs full `langchain` package?**\n",
    "7. **What is a Runnable in LCEL and how does it enable chaining?**\n",
    "8. **What makes Groq ideal for agentic Gen AI applications?**\n",
    "9. **Can LCEL be used without LangChainâ€™s full ecosystem?**\n",
    "10. **Explain with example how youâ€™d build a multi-step Gen AI workflow using LCEL.**\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c2d69",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### âœ… **1. What is LCEL and how does it differ from traditional function composition?**\n",
    "\n",
    "**Answer:**\n",
    "**LCEL (LangChain Expression Language)** is a **declarative and composable way** to define LLM workflows using `Runnable` interfaces in LangChain.\n",
    "\n",
    "* In traditional function composition, you manually define how data flows from one function to another (`f(g(x))`).\n",
    "* In **LCEL**, you chain prompt templates, models, retrievers, tools, and parsers using the `|` pipe operator, making it readable and modular.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "This is cleaner than writing each step manually.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **2. Why do we separate `system` and `human` messages in a chat prompt?**\n",
    "\n",
    "**Answer:**\n",
    "LLMs like GPT-4 are trained to process different roles:\n",
    "\n",
    "* `system` messages set behavior and context.\n",
    "* `human` messages are interpreted as actual user input.\n",
    "\n",
    "**Reason for separation:**\n",
    "\n",
    "* It ensures that the model understands **which part is instruction** vs **which part is the user query**.\n",
    "* It mirrors the format of training data, leading to better and more predictable responses.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **3. Explain the lifecycle of a LangChain chain from prompt creation to output parsing.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. **Prompt Template** is defined (e.g., \"Translate {text} to French\").\n",
    "2. **Input Variables** are filled in (`{text}` = \"Hello\").\n",
    "3. **LLM Call**: Prompt is passed to the model.\n",
    "4. **Raw Output** is returned.\n",
    "5. **Output Parser** extracts or formats the result.\n",
    "\n",
    "All steps are `Runnable` and can be composed as:\n",
    "\n",
    "```python\n",
    "chain = prompt | model | parser\n",
    "output = chain.invoke({\"text\": \"Hello\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **4. How does Groqâ€™s LPU differ from a traditional GPU in inference?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| Feature           | Groq LPU                     | Traditional GPU (e.g., NVIDIA) |\n",
    "| ----------------- | ---------------------------- | ------------------------------ |\n",
    "| Execution Model   | Deterministic, single-core   | Parallel, batched              |\n",
    "| Latency           | Sub-10ms/token               | 100ms+ (batch-dependent)       |\n",
    "| Streaming Support | Native                       | Not optimized                  |\n",
    "| Resource Sharing  | Minimal/no context-switching | Context-switching delays       |\n",
    "\n",
    "Groq is purpose-built for **real-time, low-latency LLM inference**.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **5. How do `from_messages()` and `to_messages()` contribute to composability?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* `from_messages()` creates a **structured prompt template** by clearly defining roles like system, human, AI.\n",
    "* `to_messages()` **renders the actual list of messages** by filling in template variables.\n",
    "\n",
    "This makes prompt construction:\n",
    "\n",
    "* Reusable âœ…\n",
    "* Maintainable âœ…\n",
    "* Modular âœ…\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **6. What are the trade-offs of using `langchain_core` vs full `langchain` package?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| Feature       | `langchain_core`                | `langchain` Full                   |\n",
    "| ------------- | ------------------------------- | ---------------------------------- |\n",
    "| Dependencies  | Minimal                         | Heavy (includes integrations)      |\n",
    "| Customization | High (low-level building)       | Medium (abstracted)                |\n",
    "| Use-case      | Build lightweight, minimal apps | Ready-to-use chains, tools, agents |\n",
    "\n",
    "Use `langchain_core` for custom apps, and `langchain` full for out-of-the-box tools and integrations.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **7. What is a `Runnable` in LCEL and how does it enable chaining?**\n",
    "\n",
    "**Answer:**\n",
    "A **`Runnable`** is a LangChain interface that represents any step in a pipeline (prompt, model, parser, retriever, etc.).\n",
    "\n",
    "Because all components implement `Runnable`, they can be **chained using `|`** and **executed** using `.invoke()` or `.stream()`.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "final_chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "This makes the workflow **declarative and composable**.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **8. What makes Groq ideal for agentic Gen AI applications?**\n",
    "\n",
    "**Answer:**\n",
    "Agent-based applications (like copilots or RAG) need:\n",
    "\n",
    "* Fast token-by-token response (for real-time UX).\n",
    "* Deterministic latency (to plan tool calls).\n",
    "* Scalability (for concurrent users).\n",
    "\n",
    "Groqâ€™s LPUs provide:\n",
    "\n",
    "* Sub-10ms/token streaming.\n",
    "* Stateless, fast inference.\n",
    "* Scalable performance without batching issues.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **9. Can LCEL be used without LangChainâ€™s full ecosystem?**\n",
    "\n",
    "**Answer:**\n",
    "Yes.\n",
    "\n",
    "You can use **only `langchain_core`** with LCEL to:\n",
    "\n",
    "* Build minimal pipelines.\n",
    "* Avoid vendor lock-in.\n",
    "* Integrate your own models (like OpenAI, HuggingFace) using wrappers.\n",
    "\n",
    "This gives you flexibility without relying on external tools or retrievers.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **10. Explain with example how youâ€™d build a multi-step Gen AI workflow using LCEL.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Use-case**: Translate a query, summarize it, and format the final answer.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: Translate\n",
    "translate_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Translate the following to French.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# Step 2: Summarize\n",
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize the French text in 1 line.\"),\n",
    "    (\"human\", \"{translated_text}\")\n",
    "])\n",
    "\n",
    "# LLM and parser\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain it all\n",
    "full_chain = (\n",
    "    translate_prompt | model | parser\n",
    ") >> (lambda translated: {\"translated_text\": translated}) >> (\n",
    "    summarize_prompt | model | parser\n",
    ")\n",
    "\n",
    "# Run\n",
    "output = full_chain.invoke({\"text\": \"Hello, how are you today?\"})\n",
    "print(output)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025e3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
