{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adf749f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🧠 **LANGSERVE: COMPLETE BREAKDOWN**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **1. What is LangServe?**\n",
    "\n",
    "**LangServe** is a framework built by LangChain to **serve LangChain `Runnable` chains as web APIs**, typically REST endpoints, using **FastAPI** as the web framework.\n",
    "\n",
    "> Think of LangServe as the bridge between your **LLM pipelines (chains)** and **external consumers (web apps, other services, UIs)**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **2. What is the need for LangServe?**\n",
    "\n",
    "Building powerful LLM chains is great, but in production, you often need to:\n",
    "\n",
    "* **Access those chains remotely**\n",
    "* **Expose your Gen AI logic to a frontend or third-party app**\n",
    "* **Deploy and monitor pipelines as services**\n",
    "\n",
    "LangServe **solves all of these**, letting you treat your LangChain pipeline like a deployable backend service.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **3. What problem does LangServe solve?** *(With Scenario)*\n",
    "\n",
    "#### 🧩 Scenario:\n",
    "\n",
    "You're building a **legal assistant AI**, and you’ve built a LangChain chain that:\n",
    "\n",
    "* Accepts a legal question.\n",
    "* Searches RAG documents.\n",
    "* Responds with a formatted, sourced answer.\n",
    "\n",
    "✅ **Problem**:\n",
    "Now your frontend React app needs to **call this LLM chain from a UI**.\n",
    "\n",
    "⛔ Without LangServe:\n",
    "\n",
    "* You must manually set up FastAPI.\n",
    "* Build `POST /predict` endpoints.\n",
    "* Handle data validation, chain calling, and logging.\n",
    "\n",
    "✅ **With LangServe**:\n",
    "\n",
    "* You register the chain via LangServe.\n",
    "* It instantly exposes endpoints like:\n",
    "\n",
    "  * `/invoke`\n",
    "  * `/stream`\n",
    "  * `/input_schema`, `/output_schema`\n",
    "\n",
    "> LangServe abstracts the **API plumbing**, letting you focus on the chain logic.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **4. What benefits does LangServe provide?**\n",
    "\n",
    "| Feature                     | Description                                                   |\n",
    "| --------------------------- | ------------------------------------------------------------- |\n",
    "| 🔧 **Zero-boilerplate API** | Automatically exposes endpoints for invoking chains.          |\n",
    "| 📜 **Schema generation**    | Generates OpenAPI/Swagger docs using Pydantic schemas.        |\n",
    "| 📡 **Streaming support**    | Built-in support for streamed LLM output (via SSE).           |\n",
    "| 🔍 **Inspectability**       | Easily view input/output logs during dev.                     |\n",
    "| 🧩 **Modularity**           | Run multiple chains in a single app with different endpoints. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **5. Why do we need FastAPI and Uvicorn to use LangServe?**\n",
    "\n",
    "* **LangServe is built on FastAPI**, which provides:\n",
    "\n",
    "  * Declarative API routing.\n",
    "  * Automatic Swagger docs.\n",
    "  * Async support.\n",
    "\n",
    "* **Uvicorn** is an ASGI server used to actually **run the FastAPI app**.\n",
    "\n",
    "> Without these, LangServe wouldn’t be able to start the HTTP service.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **6. Is LangServe just a wrapper over FastAPI?**\n",
    "\n",
    "✅ **Yes, but more than just a wrapper**.\n",
    "\n",
    "* LangServe wraps FastAPI to:\n",
    "\n",
    "  * Handle LangChain `Runnable` logic.\n",
    "  * Automatically register REST routes like `/invoke`, `/stream`.\n",
    "  * Add LLM-specific features (streaming, schema introspection).\n",
    "\n",
    "So while it **uses FastAPI under the hood**, it adds **LangChain-aware abstractions** on top.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **7. Build a Basic LangServe App (From Scratch)**\n",
    "\n",
    "#### 🔧 Step-by-step: Create an AI Paraphraser Chain and Expose it via LangServe\n",
    "\n",
    "---\n",
    "\n",
    "#### 📁 Folder structure:\n",
    "\n",
    "```\n",
    "langserve_app/\n",
    "├── app.py\n",
    "├── paraphraser_chain.py\n",
    "├── requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔌 `requirements.txt`\n",
    "\n",
    "```txt\n",
    "langchain\n",
    "langserve\n",
    "fastapi\n",
    "uvicorn\n",
    "langchain-openai\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔁 `paraphraser_chain.py`\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a paraphrasing assistant.\"),\n",
    "    (\"human\", \"Paraphrase this: {text}\")\n",
    "])\n",
    "\n",
    "# Model and Parser\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain\n",
    "paraphraser_chain = prompt | model | parser\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚀 `app.py` (LangServe registration)\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "from paraphraser_chain import paraphraser_chain\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Register the chain at /paraphrase endpoint\n",
    "add_routes(app, paraphraser_chain, path=\"/paraphrase\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ▶️ Run the App:\n",
    "\n",
    "```bash\n",
    "uvicorn app:app --reload --port 8000\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Call API (example using `curl` or Postman):\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/paraphrase/invoke \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"text\": \"I love working with AI models.\"}'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 Output:\n",
    "\n",
    "```json\n",
    "\"I enjoy engaging with artificial intelligence models.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Swagger UI:\n",
    "\n",
    "Open [http://localhost:8000/docs](http://localhost:8000/docs) for auto-generated documentation.\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Bonus Tip: Expose Multiple Chains!\n",
    "\n",
    "```python\n",
    "add_routes(app, another_chain, path=\"/summarize\")\n",
    "add_routes(app, retrieval_chain, path=\"/legal_search\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Important Questions :\n",
    "\n",
    "1. What is LangServe and how does it help in serving LLM applications?\n",
    "2. Why is FastAPI required for LangServe?\n",
    "3. What kind of problems does LangServe solve for production-grade GenAI pipelines?\n",
    "4. Explain how LangServe handles streaming.\n",
    "5. How would you deploy multiple chains with LangServe?\n",
    "6. Can LangServe be used in a serverless context like AWS Lambda? (Advanced)\n",
    "7. Is LangServe tightly coupled with LangChain’s core abstractions?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61281f13",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ **1. What is LangServe and how does it help in serving LLM applications?**\n",
    "\n",
    "**Answer**:\n",
    "LangServe is a deployment framework from the LangChain ecosystem that allows developers to **serve LangChain Runnables (like chains or agents) as production-ready REST APIs** using FastAPI. It provides a quick and standardized way to expose LLM logic over the web, allowing external applications (like frontends, CRON jobs, other APIs) to interact with AI chains.\n",
    "\n",
    "👉 It reduces boilerplate for:\n",
    "\n",
    "* Routing\n",
    "* Input/output schema generation\n",
    "* Request handling\n",
    "* Streaming responses\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **2. Why is FastAPI required for LangServe?**\n",
    "\n",
    "**Answer**:\n",
    "LangServe is built **on top of FastAPI**, leveraging its features such as:\n",
    "\n",
    "* Asynchronous request handling (important for LLM calls)\n",
    "* Auto-generation of OpenAPI (Swagger) docs\n",
    "* Pydantic-based validation\n",
    "* Clean RESTful architecture\n",
    "\n",
    "Without FastAPI, LangServe would need to re-implement all of this, so it simply extends FastAPI to work specifically with LangChain components.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **3. What kind of problems does LangServe solve for production-grade GenAI pipelines?**\n",
    "\n",
    "**Answer**:\n",
    "LangServe solves several production-level problems:\n",
    "\n",
    "### 🚫 Without LangServe:\n",
    "\n",
    "You need to:\n",
    "\n",
    "* Manually wrap chains into FastAPI routes.\n",
    "* Define request/response schemas.\n",
    "* Stream responses using Server-Sent Events (SSE) manually.\n",
    "* Handle logging, error formats, etc.\n",
    "\n",
    "### ✅ With LangServe:\n",
    "\n",
    "* You write just the chain logic (`Runnable`).\n",
    "* LangServe **auto-generates** all REST endpoints like:\n",
    "\n",
    "  * `/invoke` – for normal sync use\n",
    "  * `/stream` – for SSE streaming\n",
    "  * `/input_schema`, `/output_schema`\n",
    "* You get Swagger docs, monitoring, schema validation out-of-the-box.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **4. Explain how LangServe handles streaming.**\n",
    "\n",
    "**Answer**:\n",
    "LangServe supports **Server-Sent Events (SSE)** for real-time streaming of LLM output using the `/stream` endpoint.\n",
    "\n",
    "Example:\n",
    "\n",
    "```http\n",
    "GET /stream\n",
    "```\n",
    "\n",
    "Returns chunks of text as they are generated by the LLM. This is especially useful in chatbots or document summarization apps where **response latency matters**.\n",
    "\n",
    "Internally, LangServe uses FastAPI’s async generator pattern to yield results from the `stream()` method of a LangChain Runnable.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **5. How would you deploy multiple chains with LangServe?**\n",
    "\n",
    "**Answer**:\n",
    "LangServe allows you to **register multiple chains with different URL paths**.\n",
    "\n",
    "Example in `app.py`:\n",
    "\n",
    "```python\n",
    "from langserve import add_routes\n",
    "from fastapi import FastAPI\n",
    "\n",
    "from summarizer_chain import summarizer\n",
    "from translator_chain import translator\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "add_routes(app, summarizer, path=\"/summarize\")\n",
    "add_routes(app, translator, path=\"/translate\")\n",
    "```\n",
    "\n",
    "Each chain becomes available at a separate endpoint with its own docs and schema.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **6. Can LangServe be used in a serverless context like AWS Lambda?**\n",
    "\n",
    "**Answer**:\n",
    "LangServe is **built on FastAPI**, which runs on **ASGI** servers (like Uvicorn), and is typically designed for persistent environments (like EC2, containers, or services like Vercel, Railway, or Render).\n",
    "\n",
    "### For AWS Lambda:\n",
    "\n",
    "* It’s **not serverless-friendly by default**, but you can use **API Gateway + Lambda + Mangum** (an ASGI adapter for AWS Lambda) to make it work.\n",
    "* Alternatively, use **container-based deployment** via ECS or Fargate for better compatibility.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **7. Is LangServe tightly coupled with LangChain’s core abstractions?**\n",
    "\n",
    "**Answer**:\n",
    "Yes, LangServe is **tightly coupled with LangChain’s `Runnable` interface**, which is the unified abstraction for chains, models, and agents.\n",
    "\n",
    "It expects every object registered via `add_routes()` to implement `Runnable.invoke()` and optionally `Runnable.stream()` and `Runnable.batch()`.\n",
    "\n",
    "> This coupling ensures standard behavior and makes it easy to plug and play with any LangChain chain or agent.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Bonus Follow-up Questions\n",
    "\n",
    "| Question                                                  | Good Response Strategy                                                         |\n",
    "| --------------------------------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| How does LangServe compare to FastAPI directly?           | FastAPI is general-purpose. LangServe is AI/LLM-specific with added utilities. |\n",
    "| What’s the input/output validation strategy in LangServe? | It uses Pydantic to auto-generate schemas from chain input/output types.       |\n",
    "| Can LangServe support authentication?                     | Yes, since it’s built on FastAPI, you can plug in FastAPI auth dependencies.   |\n",
    "| How do you monitor or log LangServe usage?                | Add custom middlewares or use FastAPI’s event hooks to log each request.       |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716af446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
