{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721a62f1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ 1. What are Embeddings? (The Fundamentals)\n",
    "\n",
    "### ðŸ“Œ **Definition**:\n",
    "\n",
    "Embeddings are **numerical representations** of text (words, sentences, paragraphs, documents) in **high-dimensional vector space**. These vectors capture the **semantic meaning** of text, allowing us to compare, cluster, and retrieve them based on **meaning**, not just keywords.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· 2. Why Do We Use Embeddings?\n",
    "\n",
    "### âœ… **Purpose**:\n",
    "\n",
    "* To **convert human language into numbers** (vectors) that **models can understand** and reason over.\n",
    "* Enable:\n",
    "\n",
    "  * **Semantic search** (find similar meaning)\n",
    "  * **Clustering**\n",
    "  * **Recommendation systems**\n",
    "  * **Information retrieval in RAG pipelines**\n",
    "\n",
    "### âš–ï¸ Keyword Matching vs Embedding-based Matching:\n",
    "\n",
    "| Feature              | Keyword Matching                                             | Embedding-based Matching                                |\n",
    "| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------- |\n",
    "| Literal Match        | Yes                                                          | No                                                      |\n",
    "| Understands Synonyms | No                                                           | Yes                                                     |\n",
    "| Semantic Similarity  | No                                                           | Yes                                                     |\n",
    "| Example              | `\"How to eat mango?\"` vs `\"What are ways to consume mango?\"` | No match (keywords differ) vs High match (meaning same) |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· 3. OpenAI Embeddings Example Code\n",
    "\n",
    "Letâ€™s take a simple example using OpenAI's `text-embedding-3-small` model.\n",
    "\n",
    "### âœ… Prerequisites:\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "### âœ… Code Example:\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "response = openai.embeddings.create(\n",
    "    input=\"Machine learning is awesome!\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "embedding_vector = response.data[0].embedding\n",
    "print(embedding_vector[:5])  # Print first 5 values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· 4. Parameters of `openai.embeddings.create`\n",
    "\n",
    "| Parameter                    | Type                       | Description                                                                              |\n",
    "| ---------------------------- | -------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| `input`                      | string or list of strings  | The text(s) you want to embed.                                                           |\n",
    "| `model`                      | string                     | Which embedding model to use. e.g., `text-embedding-3-small` or `text-embedding-3-large` |\n",
    "| `encoding_format` (optional) | string (`float`, `base64`) | Whether to return raw floats or encoded values.                                          |\n",
    "| `dimensions` (optional)      | int                        | Reduce dimensions (e.g., 1536 â†’ 512) if supported.                                       |\n",
    "| `user` (optional)            | string                     | Helps OpenAI for abuse detection and analytics.                                          |\n",
    "\n",
    "> âœ… **Interview Tip:** Know the default and optional params, especially `dimensions` â€” critical for storage optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· 5. Full Workflow Using LangChain, Recursive Text Splitter, and Chroma DB\n",
    "\n",
    "Letâ€™s walk through a real-world **RAG-style embedding pipeline**, step by step.\n",
    "\n",
    "### âœ… Prerequisites:\n",
    "\n",
    "```bash\n",
    "pip install langchain openai chromadb tiktoken\n",
    "```\n",
    "\n",
    "### âœ… Step-by-step Breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **STEP 1: Load Raw Text as LangChain Documents**\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"sample.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "```\n",
    "\n",
    "ðŸ”¹ LangChain represents text as `Document` objects (text + metadata).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **STEP 2: Split into Chunks using RecursiveCharacterTextSplitter**\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = splitter.split_documents(documents)\n",
    "```\n",
    "\n",
    "ðŸ“Œ Why RecursiveCharacterTextSplitter?\n",
    "\n",
    "* It tries to split first on **newlines**, then **sentences**, then **words**, making sure context is preserved better.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **STEP 3: Generate Embeddings using OpenAI**\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **STEP 4: Store Embeddings in Chroma Vector DB**\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "vectordb.persist()\n",
    "```\n",
    "\n",
    "* Chroma stores embeddings + metadata for retrieval.\n",
    "* `persist_directory` helps save to disk for reuse.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **STEP 5: Search/Query in Chroma**\n",
    "\n",
    "```python\n",
    "query = \"What is machine learning?\"\n",
    "results = vectordb.similarity_search(query, k=3)\n",
    "\n",
    "for res in results:\n",
    "    print(res.page_content)\n",
    "```\n",
    "\n",
    "* It returns top `k` semantically similar chunks based on **cosine similarity**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Deep Concepts You Should Know\n",
    "\n",
    "### ðŸ”¸ Cosine Similarity:\n",
    "\n",
    "* Measures **angle** between two vectors.\n",
    "* Value: `-1` (opposite) to `1` (same).\n",
    "* Often used to find **semantically close documents**.\n",
    "\n",
    "### ðŸ”¸ Vector DB vs Normal DB:\n",
    "\n",
    "| Feature         | Vector DB (Chroma, Pinecone) | Traditional DB (Postgres, MySQL) |\n",
    "| --------------- | ---------------------------- | -------------------------------- |\n",
    "| Search Based On | Semantic similarity (cosine) | Exact match (SQL WHERE)          |\n",
    "| Data Structure  | Vectors + Metadata           | Tables, Rows                     |\n",
    "| Optimized For   | Nearest Neighbor Search      | Relational queries               |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Questions to Prepare\n",
    "\n",
    "### Beginner:\n",
    "\n",
    "1. What is an embedding?\n",
    "2. Why can't we use keyword search in Gen AI pipelines?\n",
    "3. What is chunking and why is it necessary before embedding?\n",
    "\n",
    "### Intermediate:\n",
    "\n",
    "4. How does RecursiveCharacterTextSplitter work?\n",
    "5. Why is cosine similarity preferred in vector search?\n",
    "\n",
    "### Advanced:\n",
    "\n",
    "6. What are the trade-offs between using `text-embedding-3-small` and `text-embedding-3-large`?\n",
    "7. What challenges would you face while using a vector DB at scale (millions of embeddings)?\n",
    "8. How would you compress or reduce the dimension of embeddings?\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Extra Topics You Should Know (To Be Fully Industry Ready)\n",
    "\n",
    "| Topic                               | Description                                                            |\n",
    "| ----------------------------------- | ---------------------------------------------------------------------- |\n",
    "| **Pinecone / Weaviate / FAISS**     | Popular vector DBs for production use.                                 |\n",
    "| **Dimension Reduction (PCA, UMAP)** | Reduce embedding size to save cost/performance.                        |\n",
    "| **Embedding Drift**                 | Semantic meaning of text can change over time, requiring re-embedding. |\n",
    "| **Hybrid Search**                   | Combine keyword + embedding search (e.g., Elastic + vector DB).        |\n",
    "| **Multi-modal Embeddings**          | Convert images/audio into vector form (CLIP, Whisper).                 |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae65cb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## âœ… **Beginner-Level Questions**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. What is an embedding?**\n",
    "\n",
    "ðŸ§  **Answer**:\n",
    "An **embedding** is a numerical vector that represents text in a high-dimensional space such that **semantically similar texts are closer together** in that space.\n",
    "\n",
    "For example:\n",
    "\n",
    "* \"Dog\" and \"Puppy\" will have vectors that are closer together than \"Dog\" and \"Car\".\n",
    "\n",
    "Embeddings capture **semantic meaning**, unlike one-hot encodings or bag-of-words.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Why can't we use keyword search in Gen AI pipelines?**\n",
    "\n",
    "ðŸ§  **Answer**:\n",
    "**Keyword search**:\n",
    "\n",
    "* Matches exact terms.\n",
    "* Fails when synonyms or paraphrasing is used.\n",
    "\n",
    "**Example**:\n",
    "\"How to eat mango?\" vs \"Ways to consume mango\"\n",
    "â†’ Keyword search = no match\n",
    "â†’ Embedding search = semantic match (both mean the same)\n",
    "\n",
    "Hence, in Gen AI pipelines, embeddings are used to retrieve **relevant context** even if the exact words differ.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What is chunking and why is it necessary before embedding?**\n",
    "\n",
    "ðŸ§  **Answer**:\n",
    "**Chunking** means splitting a large document into smaller parts (e.g., 500 words/chars per chunk).\n",
    "\n",
    "âœ… **Why necessary?**\n",
    "\n",
    "* LLMs (and embedding models) have **token limits** (e.g., OpenAI may support up to 8192 tokens).\n",
    "* Smaller chunks help maintain **coherent meaning** in each vector.\n",
    "* Enables **faster search and better context injection** during RAG.\n",
    "\n",
    "Chunking ensures **context is preserved and searchable** within token constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Intermediate-Level Questions**\n",
    "\n",
    "---\n",
    "\n",
    "### **4. How does RecursiveCharacterTextSplitter work?**\n",
    "\n",
    "ðŸ§  **Answer**:\n",
    "`RecursiveCharacterTextSplitter` is a smart splitting algorithm used in LangChain.\n",
    "\n",
    "It tries to split text **in a hierarchical order**:\n",
    "\n",
    "```\n",
    "1. Paragraphs (on \"\\n\\n\")\n",
    "2. Sentences (on \". \")\n",
    "3. Words\n",
    "4. Characters (if nothing else works)\n",
    "```\n",
    "\n",
    "It ensures:\n",
    "\n",
    "* Chunks are of **uniform length** (e.g., 500 tokens).\n",
    "* **Overlap** is maintained (e.g., 50 tokens) to preserve continuity across chunks.\n",
    "\n",
    "ðŸ“˜ **Benefit**: Balances between splitting at natural boundaries and staying within token size.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Why is cosine similarity preferred in vector search?**\n",
    "\n",
    "ðŸ§  **Answer**:\n",
    "**Cosine similarity** measures the **angle** between vectors, not their magnitude.\n",
    "\n",
    "**Why it's good**:\n",
    "\n",
    "* Captures **directional similarity** (i.e., semantic closeness).\n",
    "* Unaffected by **vector length** (which can vary due to phrasing or word count).\n",
    "\n",
    "ðŸ“Œ Example:\n",
    "\n",
    "* `\"I love AI\"` and `\"AI is great\"` â†’ similar direction (semantic), different length â†’ high cosine similarity.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Advanced-Level Questions**\n",
    "\n",
    "---\n",
    "\n",
    "### **6. What are the trade-offs between using `text-embedding-3-small` and `text-embedding-3-large`?**\n",
    "\n",
    "ðŸ§  **Answer**:\n",
    "\n",
    "| Feature           | text-embedding-3-small | text-embedding-3-large      |\n",
    "| ----------------- | ---------------------- | --------------------------- |\n",
    "| Speed             | Faster                 | Slower                      |\n",
    "| Cost              | Cheaper                | More expensive              |\n",
    "| Embedding quality | Good                   | Best (more semantic nuance) |\n",
    "| Use-case fit      | Simple search, FAQs    | Legal docs, deep context    |\n",
    "\n",
    "âœ… **Trade-off**:\n",
    "\n",
    "* Use **small** for **scale + speed**.\n",
    "* Use **large** for **depth + accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. What challenges would you face while using a vector DB at scale (millions of embeddings)?**\n",
    "\n",
    "ðŸ§  **Answer**:\n",
    "Common challenges:\n",
    "\n",
    "1. **Indexing Time**: Building ANN (approximate nearest neighbor) indexes becomes slow.\n",
    "2. **Memory Usage**: High-dimensional vectors consume lots of RAM/disk.\n",
    "3. **Latency**: Real-time similarity search needs vector indexes like HNSW, IVF.\n",
    "4. **Data Drift**: Over time, meanings shift â†’ re-embedding becomes necessary.\n",
    "5. **Updating Vectors**: Updating embeddings is not atomic in some DBs.\n",
    "\n",
    "âœ… **Best Practice**:\n",
    "\n",
    "* Use FAISS, Pinecone, or Weaviate with HNSW index.\n",
    "* Do **batch updates** to refresh embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. How would you compress or reduce the dimension of embeddings?**\n",
    "\n",
    "ðŸ§  **Answer**:\n",
    "You can reduce dimensions to:\n",
    "\n",
    "* **Save space**\n",
    "* **Improve speed** without much semantic loss\n",
    "\n",
    "### Techniques:\n",
    "\n",
    "1. **PCA (Principal Component Analysis)**:\n",
    "\n",
    "   * Projects vectors to a lower-dimensional space while preserving variance.\n",
    "   * E.g., 1536 â†’ 512\n",
    "\n",
    "2. **Autoencoders**:\n",
    "\n",
    "   * Train a neural network to learn compressed latent vectors.\n",
    "\n",
    "3. **Truncated SVD**:\n",
    "\n",
    "   * Linear algebra method for reducing matrix rank.\n",
    "\n",
    "âœ… Caution: Compression may lead to **loss in accuracy**, especially in fine-grained tasks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c627fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
