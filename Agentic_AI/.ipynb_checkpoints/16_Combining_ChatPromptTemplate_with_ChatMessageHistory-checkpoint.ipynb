{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a25b46",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 1. **What is `MessagesPlaceholder` in `ChatPromptTemplate.from_messages()`?**\n",
    "\n",
    "### ğŸ“Œ Purpose:\n",
    "\n",
    "`MessagesPlaceholder` is used in `ChatPromptTemplate` to **dynamically insert historical chat messages** (from memory) into the current prompt **at runtime**.\n",
    "\n",
    "It tells LangChain:\n",
    "ğŸ“¢ â€œAt this position in the prompt, inject all previous messages for the current session.â€\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Example:\n",
    "\n",
    "```python\n",
    "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* `\"system\"` message is the static system instruction.\n",
    "* `MessagesPlaceholder(\"messages\")` tells LangChain to **insert past messages** here.\n",
    "* The `\"human\"` message `{input}` is the **current user input**.\n",
    "\n",
    "â³ At runtime, it might look like:\n",
    "\n",
    "```plaintext\n",
    "System: You are a helpful assistant.\n",
    "Human: Hello\n",
    "AI: Hi there! How can I help?\n",
    "Human: What is LangChain?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 2. **Using Multiple Variables in ChatPromptTemplate**\n",
    "\n",
    "If you want to **add additional context** like `username`, `mood`, `language`, etc., you can simply add them as placeholders like `{username}`, `{language}`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helping user {username} who prefers {language}.\"),\n",
    "    MessagesPlaceholder(\"messages\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "### â“How to pass these to `RunnableWithMessageHistory`?\n",
    "\n",
    "You pass all prompt variables **except the one for history (e.g., `messages`)** as part of the input dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Runtime Input Example:\n",
    "\n",
    "```python\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input\": \"Can you help me?\",\n",
    "        \"username\": \"Alice\",\n",
    "        \"language\": \"English\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"session_1\"}\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "âœ… LangChain automatically fills in `messages` via `RunnableWithMessageHistory`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ 3. **Full Code: Multiple Sessions + MessagesPlaceholder + Extra Variables**\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, ChatMessageHistory\n",
    "\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "# Mock: Environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-real-key\"\n",
    "\n",
    "# Step 1: Multi-session store\n",
    "session_store: Dict[str, ChatMessageHistory] = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "# Step 2: Create prompt with messages + additional vars\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helping user {username} who speaks {language}.\"),\n",
    "    MessagesPlaceholder(\"messages\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Step 3: LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Step 4: Chain = prompt | model\n",
    "chain = prompt | llm\n",
    "\n",
    "# Step 5: Wrap with history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",         # the key for current user input\n",
    "    history_messages_key=\"messages\"     # the placeholder we defined in prompt\n",
    ")\n",
    "\n",
    "# Step 6: Use in multiple sessions\n",
    "sessions = [\"alice_001\", \"bob_007\"]\n",
    "\n",
    "for session_id in sessions:\n",
    "    print(f\"\\nğŸ” Conversation with session_id: {session_id}\")\n",
    "    \n",
    "    inputs = {\n",
    "        \"input\": \"Tell me something interesting.\",\n",
    "        \"username\": session_id.split(\"_\")[0].capitalize(),  # Alice or Bob\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    "\n",
    "    response = chain_with_history.invoke(\n",
    "        inputs,\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ¤– Response:\", response.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Important Concepts Recap\n",
    "\n",
    "| Concept                      | Purpose                                                             |\n",
    "| ---------------------------- | ------------------------------------------------------------------- |\n",
    "| `MessagesPlaceholder()`      | Injects historical messages at a specific location in the prompt    |\n",
    "| `RunnableWithMessageHistory` | Wraps your LLM chain and manages message memory per session         |\n",
    "| `input_messages_key`         | Current user input key, e.g., `\"input\"`                             |\n",
    "| `history_messages_key`       | The variable name used in `MessagesPlaceholder`, e.g., `\"messages\"` |\n",
    "| `get_session_history()`      | Creates/returns a `ChatMessageHistory` object per session           |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Important Questions and Answers\n",
    "\n",
    "### Q1: Why is `MessagesPlaceholder` needed in a prompt?\n",
    "\n",
    "> To dynamically inject previous chat messages into the current prompt so that the model retains conversational memory.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: What happens if you donâ€™t add `MessagesPlaceholder` in your prompt but still use `RunnableWithMessageHistory`?\n",
    "\n",
    "> History is still stored, but it won't be passed to the model. The model will respond without awareness of prior conversation.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: Can you use other variables besides `messages` in `ChatPromptTemplate`?\n",
    "\n",
    "> Yes, you can pass any additional variables (like `username`, `mood`, etc.) as placeholders and provide them at runtime.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: How do you manage history across different users or sessions?\n",
    "\n",
    "> By using `RunnableWithMessageHistory` with a `get_session_history()` function that returns different `ChatMessageHistory` per session ID.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b07ed9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Goal: How does the `llm` know about chat history using `session_id`?\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Step 0: The Setup Pieces You Have\n",
    "\n",
    "Before diving into the inner flow, let's list the components you already have:\n",
    "\n",
    "1. **`session_store`** â€“ a Python `dict` to store chat histories per session.\n",
    "2. **`get_session_history(session_id)`** â€“ a function that returns the right history object from `session_store`.\n",
    "3. **`RunnableWithMessageHistory`** â€“ a LangChain utility that connects a chain to a session-aware memory system.\n",
    "4. **`chain_with_history.invoke(input, config={...})`** â€“ you're invoking the chain and passing a session ID in the config.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Step 1: You Invoke the Chain with a Session ID\n",
    "\n",
    "```python\n",
    "response = chain_with_history.invoke(\n",
    "    {\n",
    "        \"input\": \"Tell me something.\",\n",
    "        \"username\": \"Alice\",\n",
    "        \"language\": \"English\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"session_id\": \"alice_001\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "#### âœ… What you're doing:\n",
    "\n",
    "* Giving current input (`\"Tell me something.\"`)\n",
    "* Passing `session_id = \"alice_001\"` through the `config` parameter\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Step 2: `RunnableWithMessageHistory` Intercepts the Call\n",
    "\n",
    "This is where magic begins.\n",
    "\n",
    "When `invoke()` is called on `RunnableWithMessageHistory`, **it intercepts the call** and performs:\n",
    "\n",
    "#### ğŸ”„ Internal Flow:\n",
    "\n",
    "1. It extracts the `session_id` from `config[\"configurable\"][\"session_id\"]`\n",
    "2. It **calls your `get_session_history(session_id)` function**\n",
    "3. This returns a `ChatMessageHistory` object (a memory object)\n",
    "4. It **injects historical messages from that object into the prompt** via the placeholder: `MessagesPlaceholder(\"messages\")`\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ Step 3: Filling the Prompt\n",
    "\n",
    "Your prompt is defined like this:\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helping user {username} who speaks {language}.\"),\n",
    "    MessagesPlaceholder(\"messages\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "Letâ€™s suppose the chat history for `\"alice_001\"` looks like this:\n",
    "\n",
    "```python\n",
    "[\n",
    "    HumanMessage(content=\"What is LangChain?\"),\n",
    "    AIMessage(content=\"LangChain helps build LLM apps.\")\n",
    "]\n",
    "```\n",
    "\n",
    "Then the final prompt sent to the LLM will be:\n",
    "\n",
    "```plaintext\n",
    "System: You are helping user Alice who speaks English.\n",
    "Human: What is LangChain?\n",
    "AI: LangChain helps build LLM apps.\n",
    "Human: Tell me something.\n",
    "```\n",
    "\n",
    "ğŸ¯ The model sees the **entire conversation context**, allowing it to reply meaningfully.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Step 4: After Model Generates Response\n",
    "\n",
    "Letâ€™s say model responds:\n",
    "\n",
    "```plaintext\n",
    "AI: Did you know LangChain also supports agents?\n",
    "```\n",
    "\n",
    "#### What Happens Next:\n",
    "\n",
    "* `RunnableWithMessageHistory` **automatically appends the new `HumanMessage` and `AIMessage`** to the history object associated with `session_id = \"alice_001\"`.\n",
    "* This way, **the next time this session talks, it will include this message in its memory.**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—ƒï¸ Step 5: How Session Store Retains It\n",
    "\n",
    "Since your `session_store` is just a Python dictionary:\n",
    "\n",
    "```python\n",
    "session_store = {\n",
    "    \"alice_001\": ChatMessageHistory([...]),\n",
    "    \"bob_007\": ChatMessageHistory([...]),\n",
    "}\n",
    "```\n",
    "\n",
    "Each user/sessionâ€™s history grows independently.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Final Understanding: Mental Model\n",
    "\n",
    "```\n",
    "User Input + Session ID  â”€â”€â”€â”€â”€â”\n",
    "                             â”‚\n",
    "               RunnableWithMessageHistory\n",
    "                             â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€ Injects history from session store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚                                                    â–¼\n",
    "PromptTemplate: [System, MessagesPlaceholder, Human input]\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                          LLM\n",
    "                             â”‚\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€ Appends response to history â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â–¼                                             \n",
    "    session_store[session_id] = Updated Message List\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ğŸ’¡ Try This Yourself\n",
    "\n",
    "To see it in action, print history after each interaction:\n",
    "\n",
    "```python\n",
    "for msg in session_store[\"alice_001\"].messages:\n",
    "    print(f\"{msg.type.title()}: {msg.content}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec548215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
