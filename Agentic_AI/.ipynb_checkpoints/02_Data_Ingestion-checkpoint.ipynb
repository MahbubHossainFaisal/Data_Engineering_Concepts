{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d308f4ab",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîπ What is Data Ingestion in Gen AI (LangChain Perspective)?\n",
    "\n",
    "**Data ingestion** refers to the process of **fetching raw data** (text, documents, webpages, PDFs, etc.) and converting it into a structured format (like LangChain's `Document` object) that can be passed to downstream components like embeddings, LLMs, vector stores, etc.\n",
    "\n",
    "In LangChain, this job is performed by **Loaders**, which take **unstructured data** and return a list of `Document` objects.\n",
    "\n",
    "---\n",
    "\n",
    "# üßæ 1. Text Loader in LangChain (from `langchain_community`)\n",
    "\n",
    "### üîç What is it?\n",
    "\n",
    "The `TextLoader` is a simple file-based loader that reads plain text files and returns them as `Document` objects.\n",
    "\n",
    "### ‚úÖ Use Case:\n",
    "\n",
    "You want to load a `.txt` file (e.g., user manuals, instructions, long paragraphs, etc.)\n",
    "\n",
    "### üìò Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"example.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc.page_content[:200])  # print first 200 characters\n",
    "```\n",
    "\n",
    "### üì• Input:\n",
    "\n",
    "A simple `.txt` file:\n",
    "\n",
    "```\n",
    "example.txt:\n",
    "---------------\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "It enables data-aware and agentic applications.\n",
    "```\n",
    "\n",
    "### üì§ Output:\n",
    "\n",
    "```python\n",
    "[Document(page_content='LangChain is a framework for developing applications powered by language models...')]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üìÑ 2. PDF Loading with `PyPDFLoader`\n",
    "\n",
    "### üîç What is PyPDFLoader?\n",
    "\n",
    "`PyPDFLoader` is a LangChain-compatible loader that uses the `PyPDF2` or similar libraries to extract text from each page of a PDF file.\n",
    "\n",
    "### ‚úÖ Use Case:\n",
    "\n",
    "Load books, scanned reports, research papers in PDF format.\n",
    "\n",
    "### üìò Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"sample.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(len(documents))  # one Document per page\n",
    "print(documents[0].page_content[:300])  # content from page 1\n",
    "```\n",
    "\n",
    "### üì• Input:\n",
    "\n",
    "A PDF file `sample.pdf` with 2 pages:\n",
    "\n",
    "* Page 1: \"Introduction to Generative AI...\"\n",
    "* Page 2: \"Training LLMs requires large amounts of text...\"\n",
    "\n",
    "### üì§ Output:\n",
    "\n",
    "```python\n",
    "[\n",
    "  Document(page_content=\"Introduction to Generative AI...\", metadata={'page': 0}),\n",
    "  Document(page_content=\"Training LLMs requires large amounts of text...\", metadata={'page': 1}),\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Must-Know Features of `PyPDFLoader`\n",
    "\n",
    "### 1. `load()` ‚Äì loads all pages into separate documents.\n",
    "\n",
    "### 2. `load_and_split()` ‚Äì if used with `CharacterTextSplitter`, can split each page into smaller chunks.\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "docs = loader.load_and_split(splitter=splitter)\n",
    "```\n",
    "\n",
    "### üîç Metadata:\n",
    "\n",
    "Each document has metadata with `page` number and file path.\n",
    "\n",
    "---\n",
    "\n",
    "# üåê 3. Web Base Loader (e.g., `WebBaseLoader`)\n",
    "\n",
    "### üîç What is it?\n",
    "\n",
    "`WebBaseLoader` is used to load content from a web page using HTTP and extract readable text (typically with BeautifulSoup under the hood).\n",
    "\n",
    "### üìò Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Natural_language_processing\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[0].page_content[:300])\n",
    "```\n",
    "\n",
    "### üì• Input:\n",
    "\n",
    "A Wikipedia URL on \"Natural Language Processing\"\n",
    "\n",
    "### üì§ Output:\n",
    "\n",
    "```python\n",
    "[Document(page_content=\"Natural language processing (NLP) is a subfield of linguistics, computer science...\")]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Must-Know Parameters/Features of `WebBaseLoader`\n",
    "\n",
    "* Can accept **list of URLs**.\n",
    "* Uses `requests` + `BeautifulSoup` for parsing.\n",
    "* Returns a single or multiple `Document` objects depending on the pages.\n",
    "\n",
    "---\n",
    "\n",
    "# üå∏ 4. Integrating BeautifulSoup4 with WebBaseLoader\n",
    "\n",
    "Internally, `WebBaseLoader` uses **BeautifulSoup4** to clean the HTML. You can override the parsing logic for **custom scraping**:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "class CustomLoader(WebBaseLoader):\n",
    "    def _scrape(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        return soup.find(\"main\").get_text()  # Extract just the main content\n",
    "\n",
    "loader = CustomLoader(\"https://example.com\")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üìú 5. Summarizing ‚ÄúAttention is All You Need‚Äù & Using ArxivLoader\n",
    "\n",
    "### ‚úÖ Summary:\n",
    "\n",
    "> \"Attention Is All You Need\" (Vaswani et al., 2017) introduced the **Transformer architecture**, which replaced recurrence with **self-attention**, enabling parallelization and better scalability. It introduced **multi-head attention**, **positional encoding**, and showed state-of-the-art results on translation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò ArxivLoader Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(query=\"Attention is all you need\", load_max_docs=1)\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[0].page_content[:300])\n",
    "```\n",
    "\n",
    "### üì§ Output:\n",
    "\n",
    "```python\n",
    "[Document(page_content=\"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms...\")]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùó Why `ArxivLoader` may need `PyMuPDFLoader`?\n",
    "\n",
    "* Arxiv PDFs are **scanned** papers.\n",
    "* Some `ArxivLoader` implementations download the PDF and require **parsing via PyMuPDFLoader** or `pdfplumber` to extract full content.\n",
    "\n",
    "You might need to combine like this:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import ArxivLoader, PyMuPDFLoader\n",
    "\n",
    "arxiv = ArxivLoader(\"2311.00000\")  # example arxiv ID\n",
    "pdf_path = arxiv.download_pdf()\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß† 6. WikipediaLoader\n",
    "\n",
    "### üîç What is it?\n",
    "\n",
    "`WikipediaLoader` fetches content from Wikipedia articles using `wikipedia` Python package.\n",
    "\n",
    "### üìò Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query=\"Large Language Models\", lang=\"en\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[0].page_content[:300])\n",
    "```\n",
    "\n",
    "### üì• Input:\n",
    "\n",
    "Query: `\"Large Language Models\"`\n",
    "\n",
    "### üì§ Output:\n",
    "\n",
    "```python\n",
    "[Document(page_content=\"A large language model (LLM) is a type of language model notable for its ability to generate human-like text...\")]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Must-Know Features of `WikipediaLoader`:\n",
    "\n",
    "* `query`: Article title or topic.\n",
    "* `lang`: Language (default: \"en\").\n",
    "* Returns single `Document`.\n",
    "* Best used with a splitter for long articles.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Bonus:  Important Questions\n",
    "\n",
    "1. **What is the purpose of Document Loaders in LangChain?**\n",
    "2. **How does PyPDFLoader differ from PyMuPDFLoader?**\n",
    "3. **How do you handle scanned PDFs where PyPDF2 fails to extract text?**\n",
    "4. **What challenges arise in web scraping for LLM pipelines?**\n",
    "5. **Can you customize a loader‚Äôs HTML parsing logic?**\n",
    "6. **Explain a real-life use case where you'd use ArxivLoader.**\n",
    "7. **Why is metadata important in LangChain documents?**\n",
    "8. **How do you split documents for chunked processing in embeddings?**\n",
    "9. **What are potential failure cases of WebBaseLoader and how to handle them?**\n",
    "10. **Compare WikipediaLoader vs ArxivLoader vs WebBaseLoader.**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Conclusion: What You Should Take Away\n",
    "\n",
    "| Loader            | Data Source     | Notes                                                |\n",
    "| ----------------- | --------------- | ---------------------------------------------------- |\n",
    "| `TextLoader`      | .txt files      | Simple, reliable                                     |\n",
    "| `PyPDFLoader`     | PDFs            | One doc per page                                     |\n",
    "| `WebBaseLoader`   | Web pages       | HTML scraping with BeautifulSoup                     |\n",
    "| `ArxivLoader`     | Research papers | May need PDF parsing integration                     |\n",
    "| `WikipediaLoader` | Wikipedia       | Ideal for general knowledge; supports multi-language |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032868f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **1. What is the purpose of Document Loaders in LangChain?**\n",
    "\n",
    "**Answer:**\n",
    "LangChain Document Loaders serve the purpose of **data ingestion**. They load **unstructured data** (text, PDF, HTML, Wikipedia pages, etc.) and convert it into a structured format ‚Äî specifically into a list of `Document` objects with:\n",
    "\n",
    "* `page_content`: the actual text,\n",
    "* `metadata`: useful context like source URL, page number, file path.\n",
    "\n",
    "These structured `Document` objects are required for:\n",
    "\n",
    "* Splitting into chunks,\n",
    "* Creating vector embeddings,\n",
    "* Using with language models in chains or retrieval pipelines.\n",
    "\n",
    "üìò Example:\n",
    "\n",
    "```python\n",
    "[Document(page_content=\"LangChain is a framework...\", metadata={'source': 'example.txt'})]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **2. How does `PyPDFLoader` differ from `PyMuPDFLoader`?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| Feature      | `PyPDFLoader`                           | `PyMuPDFLoader`                         |\n",
    "| ------------ | --------------------------------------- | --------------------------------------- |\n",
    "| Library Used | `PyPDF2` or similar                     | `PyMuPDF` (also called `fitz`)          |\n",
    "| Works on     | Mostly text-based PDFs                  | Works on both text-based & scanned PDFs |\n",
    "| Performance  | Slower, less accurate with complex PDFs | Faster, better handling of layouts      |\n",
    "| Output       | One `Document` per page                 | Can preserve better structure           |\n",
    "\n",
    "üìå Use `PyMuPDFLoader` when:\n",
    "\n",
    "* You‚Äôre dealing with **complex layouts**, **images**, or **scientific papers** from arXiv.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **3. How do you handle scanned PDFs where `PyPDF2` fails to extract text?**\n",
    "\n",
    "**Answer:**\n",
    "Scanned PDFs are essentially **images**, not text layers. `PyPDF2` will return **blank** or **garbled content**. To extract text from these:\n",
    "\n",
    "1. Use `PyMuPDFLoader`, which can sometimes extract embedded text.\n",
    "2. If that fails, use **OCR-based** loaders like `UnstructuredPDFLoader` (uses Tesseract/Unstructured.io).\n",
    "3. Alternatively, use `pdfplumber` with OCR support.\n",
    "\n",
    "üìò Example using Unstructured:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "loader = UnstructuredPDFLoader(\"scanned.pdf\")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **4. What challenges arise in web scraping for LLM pipelines?**\n",
    "\n",
    "**Answer:**\n",
    "‚úÖ Key Challenges:\n",
    "\n",
    "* **JavaScript-based websites**: `WebBaseLoader` only fetches static HTML. Dynamic content will be missing.\n",
    "* **Irrelevant text**: Ads, nav bars, comments ‚Äî can dilute useful content.\n",
    "* **Rate limiting & CAPTCHAs**: Risk of getting blocked during large-scale scraping.\n",
    "* **Ethical concerns & terms of service**: Always respect robots.txt and copyrights.\n",
    "\n",
    "‚úÖ Mitigation:\n",
    "\n",
    "* Use headless browsers like `Playwright` for dynamic pages.\n",
    "* Customize `BeautifulSoup` parsing in `WebBaseLoader`.\n",
    "* Use proxies and exponential backoff for rate-limited APIs.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **5. Can you customize a loader‚Äôs HTML parsing logic?**\n",
    "\n",
    "**Answer:**\n",
    "Yes! `WebBaseLoader` allows overriding its internal `_scrape()` method.\n",
    "\n",
    "üìò Example:\n",
    "\n",
    "```python\n",
    "class CustomLoader(WebBaseLoader):\n",
    "    def _scrape(self, html: str) -> str:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        return soup.find(\"article\").get_text()  # Extract specific tag content\n",
    "\n",
    "loader = CustomLoader(\"https://example.com/article\")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "This gives **fine-grained control** over what part of the HTML you extract ‚Äî ideal for news, blogs, product pages.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **6. Explain a real-life use case where you'd use ArxivLoader.**\n",
    "\n",
    "**Answer:**\n",
    "Let‚Äôs say you're building a **GenAI research assistant** that summarizes cutting-edge papers.\n",
    "\n",
    "Use case:\n",
    "\n",
    "* Search and load the latest papers on ‚ÄúReinforcement Learning‚Äù from arXiv.\n",
    "* Extract content.\n",
    "* Feed into an LLM for summarization or Q\\&A.\n",
    "\n",
    "üìò Code:\n",
    "\n",
    "```python\n",
    "loader = ArxivLoader(\"Reinforcement Learning\", load_max_docs=2)\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "This enables **auto-updating pipelines** based on real-time research papers.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **7. Why is metadata important in LangChain documents?**\n",
    "\n",
    "**Answer:**\n",
    "Metadata allows you to **trace back** where the data came from. It's critical for:\n",
    "\n",
    "* Showing the **source** of the answer in RAG systems.\n",
    "* Providing **context** (page number, URL, author).\n",
    "* Implementing **filters** on document retrieval.\n",
    "\n",
    "üìò Example:\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    page_content=\"Large Language Models (LLMs) are...\",\n",
    "    metadata={'source': 'wikipedia', 'title': 'LLM'}\n",
    ")\n",
    "```\n",
    "\n",
    "You can display this metadata in a chatbot as:\n",
    "\n",
    "> ‚ÄúAnswer retrieved from Wikipedia article on LLMs.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **8. How do you split documents for chunked processing in embeddings?**\n",
    "\n",
    "**Answer:**\n",
    "LLMs and vector stores have a token limit, so long documents must be split into smaller overlapping chunks using a **Text Splitter**.\n",
    "\n",
    "üìò Code:\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(documents)\n",
    "```\n",
    "\n",
    "‚úÖ Best Practices:\n",
    "\n",
    "* Use `RecursiveCharacterTextSplitter` for smart breaks (based on paragraphs, sentences).\n",
    "* Overlap chunks (e.g., 100 tokens) to preserve context across boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **9. What are potential failure cases of `WebBaseLoader` and how to handle them?**\n",
    "\n",
    "**Failures:**\n",
    "\n",
    "* Page returns 403/404 (blocked or not found).\n",
    "* Page uses JavaScript rendering (returns empty body).\n",
    "* HTML too messy ‚Äî parsing fails.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "* Retry with exponential backoff.\n",
    "* Use headless browser (e.g., `PlaywrightURLLoader`).\n",
    "* Override `_scrape()` for robust parsing.\n",
    "* Use logging + alerts for critical failures.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **10. Compare `WikipediaLoader`, `ArxivLoader`, and `WebBaseLoader`**\n",
    "\n",
    "| Loader          | Source        | Best Use Case                        | Strengths                              | Weaknesses                      |\n",
    "| --------------- | ------------- | ------------------------------------ | -------------------------------------- | ------------------------------- |\n",
    "| WikipediaLoader | Wikipedia API | General knowledge ingestion          | Easy, multilingual, structured content | Less recent, community-written  |\n",
    "| ArxivLoader     | arxiv.org     | Scientific research ingestion        | Rich technical data, scholarly papers  | May need additional PDF parsing |\n",
    "| WebBaseLoader   | Any web page  | Blogs, articles, FAQs, product pages | Flexible, works with any HTML          | Limited on JS-heavy websites    |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Summary to Remember:\n",
    "\n",
    "* **Loaders ‚Üí Documents ‚Üí Split ‚Üí Embed ‚Üí RAG**\n",
    "* Always validate PDF & HTML structure before loading\n",
    "* Metadata is **gold** in production-grade systems\n",
    "* Use OCR tools if PDFs are scanned\n",
    "* Choose the **right loader** for your use case to avoid garbage-in-garbage-out\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb2860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
