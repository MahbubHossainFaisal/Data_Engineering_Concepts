{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54959bb6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Session 1: Environment Setup & LangSmith Logging**\n",
    "\n",
    "### 🔹 `load_dotenv()` — What does it do?\n",
    "\n",
    "This is a utility function from the `python-dotenv` package.\n",
    "\n",
    "#### ✅ **What It Does:**\n",
    "\n",
    "* It **reads environment variables from a `.env` file** and adds them to `os.environ`.\n",
    "* This keeps sensitive values like API keys **out of your code**.\n",
    "\n",
    "#### 🧠 Example:\n",
    "\n",
    "```bash\n",
    "# .env\n",
    "OPENAI_API_KEY=sk-abc123\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_API_KEY=ls-xyz456\n",
    "```\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Reads and adds these variables to your Python env\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `os.environ` vs `os.getenv`\n",
    "\n",
    "#### ✅ `os.environ`:\n",
    "\n",
    "* A **dictionary-like object**.\n",
    "* Used to access or modify environment variables directly.\n",
    "\n",
    "```python\n",
    "os.environ[\"MY_VAR\"] = \"hello\"\n",
    "print(os.environ[\"MY_VAR\"])  # Output: hello\n",
    "```\n",
    "\n",
    "#### ✅ `os.getenv()`:\n",
    "\n",
    "* **Safe accessor**: If variable doesn’t exist, it returns `None` (or a default).\n",
    "\n",
    "```python\n",
    "print(os.getenv(\"MY_VAR\", \"default_value\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why set `LANGCHAIN_TRACING_V2 = true`?\n",
    "\n",
    "#### ✅ Purpose:\n",
    "\n",
    "This **enables LangSmith tracing** (LangChain's logging and debugging tool for chains).\n",
    "\n",
    "* It helps you **track inputs, outputs, and intermediate steps** in chains.\n",
    "* Useful for **debugging, versioning, and collaboration**.\n",
    "\n",
    "```python\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why set a project name using `os.environ`?\n",
    "\n",
    "LangSmith uses `project name` to:\n",
    "\n",
    "* **Group your traces** into logical units (e.g., \"dev-test\", \"summarizer-app\").\n",
    "* Helps you track experiments.\n",
    "\n",
    "#### ✅ How to set it:\n",
    "\n",
    "```python\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"MyLangchainApp\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why and when do we require `langchain-openai`?\n",
    "\n",
    "`langchain-openai` provides **LLM wrappers (like ChatOpenAI)** around the OpenAI APIs.\n",
    "\n",
    "✅ **Required when:**\n",
    "\n",
    "* You use OpenAI models like `gpt-3.5-turbo` or `gpt-4` inside LangChain.\n",
    "\n",
    "```bash\n",
    "pip install langchain-openai\n",
    "```\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "```\n",
    "\n",
    "> Without this, LangChain won't know how to talk to OpenAI models.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why do we use `ipykernel`?\n",
    "\n",
    "✅ **Purpose:**\n",
    "Used to **run code cells in Jupyter Notebooks** and **enable rich outputs** in VS Code or Colab.\n",
    "\n",
    "You need it especially when:\n",
    "\n",
    "* You run notebooks inside environments like Jupyter Lab.\n",
    "* You want **LangChain to display tool outputs or traces visually**.\n",
    "\n",
    "```bash\n",
    "pip install ipykernel\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Session 2: ChatPromptTemplate & Prompt Design**\n",
    "\n",
    "### 🔹 `ChatPromptTemplate` — What is it?\n",
    "\n",
    "LangChain’s prompt abstraction for **chat-based models** like GPT-3.5, GPT-4.\n",
    "\n",
    "It **structures prompts as a conversation**: system, human, and assistant messages.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **System Prompt vs User Prompt**\n",
    "\n",
    "| Type       | Purpose                                                       |\n",
    "| ---------- | ------------------------------------------------------------- |\n",
    "| **System** | Provides context or rules to the assistant. (One-time setup.) |\n",
    "| **User**   | The actual input/question from the user.                      |\n",
    "\n",
    "### 🧠 Example:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}.\")\n",
    "])\n",
    "```\n",
    "\n",
    "#### 👉 Functions of `ChatPromptTemplate`:\n",
    "\n",
    "* `.from_messages()` – define multi-role conversation\n",
    "* `.format_messages()` – inject actual values\n",
    "* `.invoke()` – used when wrapping inside chains\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Session 3: Chains**\n",
    "\n",
    "### 🔹 What is a Chain?\n",
    "\n",
    "A **LangChain chain is a pipeline** that connects:\n",
    "\n",
    "* Prompts\n",
    "* Models (LLMs)\n",
    "* Output parsers\n",
    "* Tools\n",
    "\n",
    "### ✅ Why use chains?\n",
    "\n",
    "To:\n",
    "\n",
    "* Combine multiple steps\n",
    "* Add memory, retrievers, tools\n",
    "* Create powerful GenAI workflows\n",
    "\n",
    "### 🔧 **How Chains Work:**\n",
    "\n",
    "Think of a chain like this:\n",
    "\n",
    "```\n",
    "User Input\n",
    "  ↓\n",
    "PromptTemplate\n",
    "  ↓\n",
    "LLM\n",
    "  ↓\n",
    "Parser (optional)\n",
    "  ↓\n",
    "Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Types of Chains\n",
    "\n",
    "* **LLMChain** – single LLM + prompt\n",
    "* **SequentialChain** – multiple steps in order\n",
    "* **RetrievalQAChain** – use a retriever with an LLM\n",
    "* **RouterChain** – dynamic routing between subchains\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Important Functions of Chains\n",
    "\n",
    "| Function              | Purpose                     |\n",
    "| --------------------- | --------------------------- |\n",
    "| `.invoke()`           | Core way to call the chain  |\n",
    "| `.run()`              | Legacy method to run chain  |\n",
    "| `.batch()`            | Run chain on list of inputs |\n",
    "| `.stream()`           | Stream outputs              |\n",
    "| `.save()` / `.load()` | Persist your chains         |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 What can be combined in chains?\n",
    "\n",
    "✅ You can combine:\n",
    "\n",
    "* Prompts\n",
    "* LLMs\n",
    "* Tools\n",
    "* Retrievers\n",
    "* Memory\n",
    "* Output Parsers\n",
    "* Agents\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Session 4: str\\_output\\_parser**\n",
    "\n",
    "### 🔹 What is `StrOutputParser`?\n",
    "\n",
    "A built-in parser to **extract clean strings** from LLM outputs.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "```\n",
    "\n",
    "### ❓ Problem It Solves:\n",
    "\n",
    "LLMs return `ChatMessage` or dicts. You often just want a **clean string**.\n",
    "\n",
    "### 🧠 Example:\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "result = chain.invoke({\"topic\": \"penguins\"})\n",
    "print(result)  # → Clean string like \"Why don't penguins fly?...\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Session 5: Put It All Together – Basic LLM App**\n",
    "\n",
    "Let’s build a complete **LLM-powered joke app**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 **Final Code Example**\n",
    "\n",
    "```python\n",
    "# ✅ Required installs\n",
    "# pip install langchain langchain-openai python-dotenv ipykernel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load secrets from .env\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"JokeGenerator\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM (OpenAI GPT-3.5)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Prompt setup\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a witty assistant who tells jokes.\"),\n",
    "    (\"user\", \"Tell me a short joke about {topic}\")\n",
    "])\n",
    "\n",
    "# Combine into a chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# User input\n",
    "user_input = input(\"Enter a topic for a joke: \")\n",
    "\n",
    "# Invoke chain\n",
    "result = chain.invoke({\"topic\": user_input})\n",
    "print(\"\\n🃏 Here's your joke:\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Important Questions**\n",
    "\n",
    "1. What is the purpose of `ChatPromptTemplate` and how is it different from `PromptTemplate`?\n",
    "2. What does `StrOutputParser` do? Why is it used in LangChain?\n",
    "3. How do chains work internally in LangChain?\n",
    "4. How would you enable tracing in LangChain and what is LangSmith?\n",
    "5. What are the differences between system, user, and assistant messages?\n",
    "6. What are the components needed to build an LLM-powered application using LangChain?\n",
    "7. When and why would you use a SequentialChain?\n",
    "8. Explain how LangChain manages state and memory across conversations.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Wrap-Up\n",
    "\n",
    "You've just learned the core LangChain components from the ground up. Here's a simple mental model:\n",
    "\n",
    "| Component            | Role                              |\n",
    "| -------------------- | --------------------------------- |\n",
    "| `.env` / os.environ  | Setup configuration               |\n",
    "| `ChatPromptTemplate` | Structure the prompt              |\n",
    "| `ChatOpenAI`         | Connect to the model              |\n",
    "| `StrOutputParser`    | Clean the output                  |\n",
    "| `Chain`              | Connect everything                |\n",
    "| `LangSmith`          | Log, trace, and debug your chains |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595582b4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ **1. What is the purpose of `ChatPromptTemplate` and how is it different from `PromptTemplate`?**\n",
    "\n",
    "### ✅ `PromptTemplate`:\n",
    "\n",
    "* Used for **text-based models**.\n",
    "* Accepts a **single template string** (non-chat).\n",
    "\n",
    "```python\n",
    "PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "```\n",
    "\n",
    "### ✅ `ChatPromptTemplate`:\n",
    "\n",
    "* Specifically for **chat-based models** (like GPT-3.5, GPT-4).\n",
    "* Allows you to define messages from **multiple roles**: system, user, assistant.\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}.\")\n",
    "])\n",
    "```\n",
    "\n",
    "### 🔄 Key Difference:\n",
    "\n",
    "| Feature      | PromptTemplate | ChatPromptTemplate          |\n",
    "| ------------ | -------------- | --------------------------- |\n",
    "| Role Support | ❌              | ✅ (system, user, assistant) |\n",
    "| Use Case     | Text models    | Chat models                 |\n",
    "| Flexibility  | Simple         | Rich conversational context |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **2. What does `StrOutputParser` do? Why is it used in LangChain?**\n",
    "\n",
    "### 🎯 Purpose:\n",
    "\n",
    "`StrOutputParser` converts the **structured LLM output (like ChatMessages)** into a **simple, clean string**.\n",
    "\n",
    "Without it:\n",
    "\n",
    "* You get complex objects like `ChatMessage(role=\"assistant\", content=\"...\")`.\n",
    "\n",
    "With it:\n",
    "\n",
    "* You get **just the text**: `\"Here's a joke...\"`\n",
    "\n",
    "### ✅ Why used:\n",
    "\n",
    "* **Simplifies chaining**\n",
    "* **Makes it easy to print or post-process results**\n",
    "\n",
    "### 🧠 Example:\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "output = chain.invoke({\"topic\": \"cats\"})\n",
    "print(output)  # Just the string, not a dict or message object\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **3. How do chains work internally in LangChain?**\n",
    "\n",
    "### 🎯 Concept:\n",
    "\n",
    "Chains are **composable pipelines** that pass inputs through a series of components:\n",
    "\n",
    "1. PromptTemplate\n",
    "2. LLM\n",
    "3. OutputParser\n",
    "\n",
    "Each component follows a **standard interface** with `.invoke()` or `.run()`.\n",
    "\n",
    "### ✅ Data Flow:\n",
    "\n",
    "```\n",
    "Input Dict → PromptTemplate → LLM → Parser → Final Output\n",
    "```\n",
    "\n",
    "LangChain automatically:\n",
    "\n",
    "* **Passes outputs as inputs** to the next step\n",
    "* **Handles logging/tracing**\n",
    "* **Supports composition** (e.g., chain of chains)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **4. How would you enable tracing in LangChain and what is LangSmith?**\n",
    "\n",
    "### 🔹 **LangSmith**:\n",
    "\n",
    "LangChain’s **observability platform** for GenAI apps. Lets you:\n",
    "\n",
    "* View inputs/outputs\n",
    "* Debug and benchmark prompts\n",
    "* Track versions of chains\n",
    "\n",
    "### ✅ How to enable tracing:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your-key\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"your-project-name\"\n",
    "```\n",
    "\n",
    "Then run chains — all runs are **automatically logged** in LangSmith.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **5. What are the differences between system, user, and assistant messages?**\n",
    "\n",
    "| Message Type  | Purpose                                           | Who sends it? |\n",
    "| ------------- | ------------------------------------------------- | ------------- |\n",
    "| **System**    | Sets the behavior or personality of the assistant | Developer     |\n",
    "| **User**      | Asks the question or gives input                  | End-user      |\n",
    "| **Assistant** | LLM-generated response                            | LLM           |\n",
    "\n",
    "### 🧠 Example:\n",
    "\n",
    "```python\n",
    "[\n",
    "  (\"system\", \"You are a helpful assistant.\"),\n",
    "  (\"user\", \"What’s the weather today?\")\n",
    "]\n",
    "```\n",
    "\n",
    "> This structure helps guide chat-based models more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **6. What are the components needed to build an LLM-powered application using LangChain?**\n",
    "\n",
    "### 🔧 Essential Components:\n",
    "\n",
    "| Component                                   | Role                    |\n",
    "| ------------------------------------------- | ----------------------- |\n",
    "| **PromptTemplate / ChatPromptTemplate**     | Structure the prompt    |\n",
    "| **LLM wrapper** (e.g., `ChatOpenAI`)        | Call the model          |\n",
    "| **Output parser** (e.g., `StrOutputParser`) | Clean/transform outputs |\n",
    "| **Chain** (e.g., `LLMChain`)                | Tie everything together |\n",
    "| **LangSmith** (optional)                    | Trace and debug         |\n",
    "| **Environment** (e.g., `.env`)              | Store secrets securely  |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **7. When and why would you use a SequentialChain?**\n",
    "\n",
    "### 🎯 Use it when:\n",
    "\n",
    "* You need **multiple LLM calls in sequence**.\n",
    "* Later steps **depend on outputs from earlier steps**.\n",
    "\n",
    "### 🧠 Example:\n",
    "\n",
    "```python\n",
    "# Step 1: Generate a blog title\n",
    "# Step 2: Generate blog content using that title\n",
    "```\n",
    "\n",
    "Use `SequentialChain` to connect them.\n",
    "\n",
    "```python\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "final_chain = SequentialChain(\n",
    "    chains=[title_chain, content_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"blog_post\"]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **8. Explain how LangChain manages state and memory across conversations.**\n",
    "\n",
    "### 🎯 State vs Memory:\n",
    "\n",
    "| Feature    | Explanation                                          |\n",
    "| ---------- | ---------------------------------------------------- |\n",
    "| **State**  | Current inputs/outputs passed in chains              |\n",
    "| **Memory** | Historical inputs and outputs (conversation history) |\n",
    "\n",
    "### ✅ How LangChain Handles Memory:\n",
    "\n",
    "You use **Memory objects** to maintain **context across turns**.\n",
    "\n",
    "#### Common memory types:\n",
    "\n",
    "* `ConversationBufferMemory`\n",
    "* `ConversationSummaryMemory`\n",
    "\n",
    "### 🧠 Example:\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chat_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n",
    "```\n",
    "\n",
    "This allows the model to **remember previous questions and answers**, enabling true conversational agents.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary Table\n",
    "\n",
    "| Question           | Key Takeaway                         |\n",
    "| ------------------ | ------------------------------------ |\n",
    "| ChatPromptTemplate | Multi-role prompt for chat models    |\n",
    "| StrOutputParser    | Clean string outputs                 |\n",
    "| Chains             | Composable pipelines                 |\n",
    "| Tracing            | Use LangSmith via env vars           |\n",
    "| Message Roles      | system/user/assistant format prompts |\n",
    "| Components         | Prompt, LLM, Parser, Chain           |\n",
    "| SequentialChain    | Multi-step workflows                 |\n",
    "| Memory             | Enable context across conversations  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c6f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
