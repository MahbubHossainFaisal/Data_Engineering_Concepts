{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eaebc9c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß† **LangChain: Working with Web Content (RAG Flow)**\n",
    "\n",
    "Let‚Äôs break the entire process **step-by-step**, following a logical order of how to build a system that can **read a web page and answer questions from it**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. What is `langchain_community`?\n",
    "\n",
    "The `langchain_community` library:\n",
    "\n",
    "* Hosts **integrations** (not in LangChain core) like loaders, embeddings, retrievers.\n",
    "* Lets LangChain **stay lean** while allowing developers to contribute and maintain community-supported integrations.\n",
    "\n",
    "### üìå Example Components:\n",
    "\n",
    "* `WebBaseLoader` (web page loader)\n",
    "* `FAISS` (vector database)\n",
    "* `HuggingFaceEmbeddings` (embedding models)\n",
    "\n",
    "> Think of it as a **plugin repo** that extends LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. Step-by-Step Process: From Web to LLM Answer\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Step 1: Scrape Webpage using `WebBaseLoader`**\n",
    "\n",
    "#### ‚ùì What is `WebBaseLoader`?\n",
    "\n",
    "`WebBaseLoader` (from `langchain_community.document_loaders`) uses **BeautifulSoup4** under the hood to extract clean text content from webpages.\n",
    "\n",
    "### ‚úÖ Purpose:\n",
    "\n",
    "* Download and parse HTML.\n",
    "* Extracts raw text for downstream processing.\n",
    "\n",
    "### ‚úÖ Code Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/LangChain\")\n",
    "documents = loader.load()\n",
    "print(documents[0].page_content[:500])  # Shows the text scraped\n",
    "```\n",
    "\n",
    "If not used:\n",
    "\n",
    "* You‚Äôd manually have to fetch HTML and parse it ‚Äî extra work and error-prone.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Step 2: Split text into chunks using `RecursiveCharacterTextSplitter`**\n",
    "\n",
    "#### ‚ùì Why Split?\n",
    "\n",
    "* LLMs like GPT-4 have **token limits** (e.g., \\~8k, \\~32k).\n",
    "* If you pass a giant article, **only part gets processed**.\n",
    "* Chunks allow for **semantic vector representation** and **effective search**.\n",
    "\n",
    "#### üß† What if we don‚Äôt?\n",
    "\n",
    "* You‚Äôd either hit context limit OR\n",
    "* Miss relevant parts during retrieval\n",
    "\n",
    "### ‚úÖ Code Example:\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = splitter.split_documents(documents)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Step 3: Convert chunks to embeddings**\n",
    "\n",
    "#### ‚ùì Why use embeddings?\n",
    "\n",
    "* Convert text into **numeric vectors** that capture **semantic meaning**.\n",
    "* Enables **similarity search** ‚Äî find related chunks to a query.\n",
    "\n",
    "#### ‚ùì What if we don‚Äôt?\n",
    "\n",
    "* You can‚Äôt **retrieve relevant content**, hence no grounding for the LLM.\n",
    "\n",
    "### üîπ Why cosine similarity?\n",
    "\n",
    "Cosine similarity checks **how close two vectors are in direction**, perfect for **semantic closeness**, even if lengths differ.\n",
    "\n",
    "### ‚úÖ Code Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectors = embedding_model.embed_documents([chunk.page_content for chunk in chunks])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Step 4: Store Embeddings in Vector DB (FAISS)**\n",
    "\n",
    "#### ‚ùì Why vector store?\n",
    "\n",
    "* You want **fast retrieval** of relevant chunks from many.\n",
    "* Vector DBs like FAISS allow efficient **similarity search**.\n",
    "\n",
    "#### ‚ùì What if we don‚Äôt?\n",
    "\n",
    "* You‚Äôll have to loop through all vectors **manually** to compute cosine similarity ‚Äî slow and impractical.\n",
    "\n",
    "### ‚úÖ Code Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Final App: Web-based LLM QA\n",
    "\n",
    "### ‚úÖ App Pipeline Summary:\n",
    "\n",
    "1. Load ‚Üí 2. Chunk ‚Üí 3. Embed ‚Üí 4. Store ‚Üí 5. Retrieve ‚Üí 6. Query with LLM\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Full Code:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load secrets (OpenAI key)\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Step 1: Load web page\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/LangChain\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 4: Vector store\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Step 5: Retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Step 6: LLM app using chain\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions based on the following context.\"),\n",
    "    (\"user\", \"{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Chain = prompt ‚Üí LLM ‚Üí output parser\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example user query\n",
    "query = \"What is LangChain used for?\"\n",
    "answer = rag_chain.invoke(query)\n",
    "print(answer)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Important Questions\n",
    "\n",
    "| Question                       | Why it‚Äôs asked                                             |\n",
    "| ------------------------------ | ---------------------------------------------------------- |\n",
    "| What is the RAG pipeline?      | To test full retrieval + generation knowledge              |\n",
    "| Why do we split documents?     | Token efficiency and semantic search                       |\n",
    "| Why vector DBs like FAISS?     | Real-time scalable similarity search                       |\n",
    "| Why embeddings?                | To convert text into machine-understandable meaning        |\n",
    "| Cosine similarity?             | Core to understanding similarity search                    |\n",
    "| What is `langchain_community`? | Check understanding of ecosystem structure                 |\n",
    "| Alternatives to FAISS?         | Chroma, Pinecone, Weaviate, Qdrant                         |\n",
    "| Limitations of this approach?  | Latency, hallucination risk if irrelevant chunks retrieved |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30798ee0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. What is the complete RAG pipeline in LangChain, and why is each step necessary?\n",
    "\n",
    "### üîπ RAG = Retrieval-Augmented Generation\n",
    "\n",
    "RAG is a technique where external knowledge (from documents, DBs, web pages) is retrieved and injected into the LLM prompt to improve response accuracy.\n",
    "\n",
    "### üß† Pipeline Steps:\n",
    "\n",
    "| Step                                    | Purpose                                                               |\n",
    "| --------------------------------------- | --------------------------------------------------------------------- |\n",
    "| 1. **Load documents**                   | Load raw text data from a web page, PDF, CSV, etc. (via loaders)      |\n",
    "| 2. **Split documents**                  | Break long documents into manageable chunks due to LLM context limits |\n",
    "| 3. **Embed chunks**                     | Convert text chunks into high-dimensional vectors                     |\n",
    "| 4. **Store in vector DB**               | Store vectors in FAISS, Pinecone, etc. to enable semantic search      |\n",
    "| 5. **Retrieve relevant docs**           | Given a query, find semantically similar chunks                       |\n",
    "| 6. **Combine retrieved chunks + query** | Format with a prompt                                                  |\n",
    "| 7. **Run through LLM**                  | Use a chain to generate a final answer                                |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. What does `WebBaseLoader` do under the hood, and when would you choose it over custom scraping?\n",
    "\n",
    "### üîπ What It Is:\n",
    "\n",
    "A loader from `langchain_community.document_loaders` that fetches and parses the textual content of a web page.\n",
    "\n",
    "### üõ†Ô∏è Internals:\n",
    "\n",
    "* Uses `requests` to fetch the HTML\n",
    "* Uses `BeautifulSoup` (via bs4) to parse the DOM\n",
    "* Extracts visible text and wraps it in a `Document` object\n",
    "\n",
    "### ‚úÖ When to use:\n",
    "\n",
    "* When you want **fast, out-of-the-box** scraping\n",
    "* When HTML content is clean and follows semantic tags\n",
    "\n",
    "### ‚ùå When not to use:\n",
    "\n",
    "* Complex websites (JS-heavy)\n",
    "* If you need metadata like `<meta>`, author, timestamps ‚Äî go with **custom BeautifulSoup** or **Selenium**\n",
    "\n",
    "### üß™ Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/LangChain\")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. Why do we use `RecursiveCharacterTextSplitter`, and what could go wrong if we don‚Äôt chunk text properly?\n",
    "\n",
    "### üîπ Why split?\n",
    "\n",
    "LLMs have **token limits** (\\~4k to 128k). So we **split long text into chunks** small enough to fit the context window.\n",
    "\n",
    "### üîπ RecursiveCharacterTextSplitter:\n",
    "\n",
    "* Tries to split at **paragraph > sentence > word > character** in that order.\n",
    "* Avoids breaking in the middle of a sentence.\n",
    "* You can also define `chunk_size` and `chunk_overlap`.\n",
    "\n",
    "### ‚ùó Consequences of skipping:\n",
    "\n",
    "* Overlong context ‚Üí prompt rejection\n",
    "* Important context may get clipped\n",
    "* Poor retrieval granularity (entire doc retrieved instead of relevant part)\n",
    "\n",
    "### üß™ Example:\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. Why do we use embeddings in LangChain? How do they enable semantic search?\n",
    "\n",
    "### üîπ What are embeddings?\n",
    "\n",
    "They are vector representations of text that **capture meaning** rather than exact words.\n",
    "E.g., ‚Äúcat‚Äù and ‚Äúfeline‚Äù will have close vectors.\n",
    "\n",
    "### üîπ Why use them?\n",
    "\n",
    "* Enable **semantic similarity search** ‚Äî even if the same words are not used\n",
    "* Power **vector databases** like FAISS\n",
    "\n",
    "### üîπ How?\n",
    "\n",
    "Embedding models like `OpenAIEmbeddings` or `HuggingFaceEmbeddings` convert each chunk into a 1536D (or more) vector.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. Why is cosine similarity used for comparing vectors in semantic retrieval?\n",
    "\n",
    "### üîπ Cosine similarity:\n",
    "\n",
    "Measures the **angle** between two vectors, not their length.\n",
    "\n",
    "### ‚úÖ Why angle matters?\n",
    "\n",
    "Two documents might have different lengths but similar **direction**, i.e., similar **meaning**.\n",
    "\n",
    "### üîπ Formula:\n",
    "\n",
    "```python\n",
    "cos_sim(A, B) = dot(A, B) / (||A|| * ||B||)\n",
    "```\n",
    "\n",
    "### ‚ùó Without cosine similarity:\n",
    "\n",
    "* You‚Äôd use Euclidean distance which is sensitive to **magnitude**\n",
    "* Bad for comparing semantic content\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 6. What role does FAISS play in LangChain, and what alternatives exist for production use cases?\n",
    "\n",
    "### üîπ What is FAISS?\n",
    "\n",
    "* Facebook AI Similarity Search\n",
    "* A local, fast, in-memory vector store\n",
    "* Optimized for dense vector search\n",
    "\n",
    "### ‚úÖ Why use it?\n",
    "\n",
    "* Fast nearest neighbor search\n",
    "* Easy to use with LangChain\n",
    "* Great for POCs or small scale use\n",
    "\n",
    "### ‚ùå Limitations:\n",
    "\n",
    "* Not persistent\n",
    "* No REST APIs\n",
    "* Doesn‚Äôt scale well\n",
    "\n",
    "### üåê Alternatives:\n",
    "\n",
    "* **Pinecone** ‚Äì fully managed, scalable\n",
    "* **Weaviate** ‚Äì self-hosted/cloud, metadata filtering\n",
    "* **Chroma** ‚Äì local, open-source\n",
    "* **Qdrant** ‚Äì great for filtering and payloads\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 7. What is a Retriever in LangChain and how does it work with a vectorstore?\n",
    "\n",
    "### üîπ Retriever:\n",
    "\n",
    "LangChain abstraction that wraps a **vector store** to define how to retrieve relevant documents.\n",
    "\n",
    "### üîπ Why use Retriever?\n",
    "\n",
    "* Allows you to switch between different backends easily\n",
    "* Encapsulates search strategy (`search_type`, `k`)\n",
    "\n",
    "### üß™ Example:\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "```\n",
    "\n",
    "You then pass this retriever to a chain like `RetrievalQA`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 8. How does the LLM chain consume retrieved documents to generate a final answer?\n",
    "\n",
    "### üîπ Process:\n",
    "\n",
    "1. User asks a question\n",
    "2. Retriever fetches relevant docs\n",
    "3. Docs + user question are **inserted into a prompt**\n",
    "4. Prompt passed to LLM\n",
    "5. LLM generates the final answer\n",
    "\n",
    "### üîπ Example prompt (via `ChatPromptTemplate`):\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"Answer this question based on the context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "```\n",
    "\n",
    "Then you run:\n",
    "\n",
    "```python\n",
    "chain = prompt | model | StrOutputParser()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 9. What are the limitations of using a RAG pipeline over simply calling an LLM?\n",
    "\n",
    "### ‚ùå Limitations of RAG:\n",
    "\n",
    "| Issue           | Description                                                         |\n",
    "| --------------- | ------------------------------------------------------------------- |\n",
    "| Retrieval Error | If relevant docs are not retrieved, LLM will hallucinate            |\n",
    "| Latency         | Multiple stages (load, embed, retrieve, LLM) increase response time |\n",
    "| Maintenance     | Need to update vector DB with fresh documents                       |\n",
    "| Complexity      | More components ‚Üí harder to debug and test                          |\n",
    "\n",
    "‚úÖ But it‚Äôs still better than fine-tuning when:\n",
    "\n",
    "* You need up-to-date info\n",
    "* You want context control\n",
    "* You‚Äôre working with private/custom data\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 10. If the web content updates frequently, how would you keep your vectorstore up-to-date?\n",
    "\n",
    "### üîÑ Strategy for freshness:\n",
    "\n",
    "1. **Schedule periodic scraping** (e.g., via Airflow or cron)\n",
    "2. **Recompute embeddings** for new/changed content\n",
    "3. **Deduplicate** based on content hash or URL\n",
    "4. **Upsert** to FAISS (or delete and re-index)\n",
    "\n",
    "### üß™ Pseudocode:\n",
    "\n",
    "```python\n",
    "new_docs = loader.load()\n",
    "new_chunks = splitter.split_documents(new_docs)\n",
    "new_vectors = embeddings.embed_documents(new_chunks)\n",
    "vectorstore.add_documents(new_chunks)  # or vectorstore.update()\n",
    "```\n",
    "\n",
    "For large setups: implement versioning + change tracking.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb450f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Goal:\n",
    "\n",
    "Given a webpage URL, scrape the content, chunk and embed it, store in FAISS, and **let user ask questions** based on the web content.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Tech Stack:\n",
    "\n",
    "* `langchain`, `langchain_community`, `langchain_openai`\n",
    "* `beautifulsoup4`, `faiss-cpu`\n",
    "* `openai` (for embeddings + LLM)\n",
    "* `dotenv` (for loading environment variables)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step-by-step Implementation:\n",
    "\n",
    "### üî∏ 1. **Install dependencies**\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-community langchain-openai openai beautifulsoup4 faiss-cpu python-dotenv\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ 2. **Create `.env` to hold OpenAI key**\n",
    "\n",
    "```env\n",
    "OPENAI_API_KEY=your_openai_key\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ 3. **Code: `web_rag_app.py`**\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Step 1: Load web content\n",
    "url = \"https://en.wikipedia.org/wiki/LangChain\"\n",
    "loader = WebBaseLoader(url)\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Embed documents\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Step 4: Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Step 5: Prompt + LLM chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert assistant. Use the context to answer the question.\"),\n",
    "    (\"user\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", api_key=openai_api_key)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 6: RetrievalQA wrapper (optional)\n",
    "qa_chain = RetrievalQA(retriever=retriever, combine_documents_chain=chain)\n",
    "\n",
    "# Step 7: Ask a question\n",
    "question = input(\"Ask a question about the webpage: \")\n",
    "answer = qa_chain.run(question)\n",
    "print(\"\\nüîç Answer:\\n\", answer)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Explanation Recap:\n",
    "\n",
    "| Stage             | Component                           | LangChain Concept        |\n",
    "| ----------------- | ----------------------------------- | ------------------------ |\n",
    "| Web scraping      | `WebBaseLoader`                     | Loader                   |\n",
    "| Text splitting    | `RecursiveCharacterTextSplitter`    | Text Splitter            |\n",
    "| Embedding         | `OpenAIEmbeddings`                  | Vector Representation    |\n",
    "| Storing vectors   | `FAISS`                             | Vector Store             |\n",
    "| Retrieval         | `as_retriever()`                    | Retriever                |\n",
    "| LLM invocation    | `ChatOpenAI` + `ChatPromptTemplate` | Chain                    |\n",
    "| Result generation | `RetrievalQA`                       | Combined Retrieval + LLM |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example Run:\n",
    "\n",
    "```bash\n",
    "$ python web_rag_app.py\n",
    "Ask a question about the webpage: What is LangChain used for?\n",
    "\n",
    "üîç Answer:\n",
    "LangChain is a framework for building applications powered by large language models. It is used for tasks such as question answering, document summarization, and more by integrating LLMs with external data sources.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Bonus Tips:\n",
    "\n",
    "* Use **LangSmith** to trace and debug chains (`LANGCHAIN_TRACING_V2 = true`)\n",
    "* You can persist FAISS using `vectorstore.save_local(\"db\")` and load using `FAISS.load_local(...)`\n",
    "* Add **metadata filtering** with Chroma or Weaviate if documents are from multiple sources\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1aabd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
