{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0783f2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîÑ Why Do We Need to Split Documents into Chunks?\n",
    "\n",
    "**Large Language Models (LLMs)** like GPT-4 and Claude have a **context window limit** (e.g., 4K, 8K, or 32K tokens). When working with long documents like PDFs, research papers, or website content, feeding the entire document to the model is **not feasible**.\n",
    "\n",
    "Hence, we:\n",
    "\n",
    "* **Split documents** into smaller **overlapping chunks**,\n",
    "* **Embed** each chunk individually into a vector store,\n",
    "* During retrieval, fetch only **relevant chunks**.\n",
    "\n",
    "üìå **Analogy**: Think of reading a book chapter-wise. If you‚Äôre only interested in one topic, you search within relevant chapters ‚Äî not the entire book.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ `langchain-text-splitters`: What Is It?\n",
    "\n",
    "LangChain provides a separate package:\n",
    "**`langchain-text-splitters`**\n",
    "\n",
    "> It contains classes and utilities to split raw text or documents into manageable chunks.\n",
    "\n",
    "üí° It supports splitting by:\n",
    "\n",
    "* Characters\n",
    "* Tokens (e.g., using tiktoken for OpenAI)\n",
    "* Sentences\n",
    "* Markdown headers\n",
    "* Code blocks (e.g., Python-specific)\n",
    "\n",
    "---\n",
    "\n",
    "## üìò RecursiveCharacterTextSplitter ‚Äì The Most Commonly Used Splitter\n",
    "\n",
    "This is the **most intelligent and flexible** text splitter. It tries to split using:\n",
    "\n",
    "1. Paragraphs (`\\n\\n`)\n",
    "2. Sentences (`.`)\n",
    "3. Words (` `)\n",
    "4. Characters (as fallback)\n",
    "\n",
    "### ‚úÖ Purpose:\n",
    "\n",
    "* Preserve **semantic meaning** as much as possible.\n",
    "* Avoid breaking in the middle of a sentence or word.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Important Parameters of `RecursiveCharacterTextSplitter`\n",
    "\n",
    "```python\n",
    "RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    length_function=len\n",
    ")\n",
    "```\n",
    "\n",
    "| Parameter         | Description                                                                   |\n",
    "| ----------------- | ----------------------------------------------------------------------------- |\n",
    "| `chunk_size`      | Maximum size of each chunk (in characters or tokens depending on splitter)    |\n",
    "| `chunk_overlap`   | Number of characters that **overlap** between chunks for context preservation |\n",
    "| `separators`      | List of strings to split on (in order of priority)                            |\n",
    "| `length_function` | How the splitter measures the chunk (default: `len`)                          |\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Example: Splitting Using `RecursiveCharacterTextSplitter`\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"LangChain is a framework for developing LLM-powered applications. \n",
    "It enables memory, chains, agents, and retrieval using vector stores.\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "```\n",
    "\n",
    "### üßæ Output:\n",
    "\n",
    "```\n",
    "Chunk 1: LangChain is a framework for developing LLM-\n",
    "Chunk 2: or developing LLM-powered applications. It en\n",
    "Chunk 3: ications. It enables memory, chains, agents, \n",
    "Chunk 4: , agents, and retrieval using vector stores.\n",
    "```\n",
    "\n",
    "> üîÑ Overlap helps preserve the meaning across chunk boundaries!\n",
    "\n",
    "---\n",
    "\n",
    "## üìÑ `create_documents()` ‚Äì What Does It Do?\n",
    "\n",
    "LangChain also provides a utility method called `create_documents()` that:\n",
    "\n",
    "* Accepts a **list of raw strings** (not LangChain `Document` objects)\n",
    "* Wraps them with metadata into `Document` format\n",
    "* Splits them into chunks\n",
    "\n",
    "### üß™ Example:\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, create_documents\n",
    "\n",
    "texts = [\"LangChain helps build LLM apps.\", \"It supports memory, chains, and more.\"]\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=20, chunk_overlap=5)\n",
    "\n",
    "docs = create_documents(texts, text_splitter=splitter)\n",
    "for doc in docs:\n",
    "    print(doc.page_content, doc.metadata)\n",
    "```\n",
    "\n",
    "### üîç Output:\n",
    "\n",
    "```python\n",
    "LangChain helps buil {'source': '0'}\n",
    "ps build LLM apps. {'source': '0'}\n",
    "It supports memory, {'source': '1'}\n",
    "emory, chains, and {'source': '1'}\n",
    "d more. {'source': '1'}\n",
    "```\n",
    "\n",
    "‚úÖ **Key benefit**: auto-generates metadata like `source`, useful for document tracing.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÑ `split_documents()` ‚Äì When to Use?\n",
    "\n",
    "If you already have a list of LangChain `Document` objects (e.g., from PDF or Web loaders), you can‚Äôt use `create_documents()` anymore. You must use:\n",
    "\n",
    "```python\n",
    "split_documents(documents)\n",
    "```\n",
    "\n",
    "This method preserves the **original metadata** and splits the `page_content` of each document.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùó Why `create_documents()` Can't Work on LangChain `Document` Types?\n",
    "\n",
    "Because:\n",
    "\n",
    "* `create_documents()` expects **raw strings** and generates new `Document` objects from scratch.\n",
    "* But `Document` objects already include `page_content` + `metadata`, so the right way is to call:\n",
    "\n",
    "```python\n",
    "splitter.split_documents(documents)\n",
    "```\n",
    "\n",
    "‚úÖ This keeps the source file name, page number, or URL intact!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öîÔ∏è Difference Between `create_documents()` and `split_documents()`\n",
    "\n",
    "| Feature           | `create_documents()`                         | `split_documents()`                     |\n",
    "| ----------------- | -------------------------------------------- | --------------------------------------- |\n",
    "| Input             | List of strings                              | List of `Document` objects              |\n",
    "| Output            | List of `Document` chunks                    | List of `Document` chunks               |\n",
    "| Metadata Handling | Generates new metadata (e.g., `source: '0'`) | Preserves original metadata             |\n",
    "| Use Case          | From raw text / notes                        | After document loading (e.g., PDF, Web) |\n",
    "| When to Use       | First ingestion from text                    | After loading docs with loaders         |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Final Example: Both in Action\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, create_documents\n",
    "\n",
    "# From raw strings\n",
    "raw_texts = [\"LangChain is amazing.\", \"It allows memory and agents.\"]\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=15, chunk_overlap=5)\n",
    "docs_created = create_documents(raw_texts, text_splitter=splitter)\n",
    "\n",
    "# From loaded documents\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"example.txt\")\n",
    "loaded_docs = loader.load()\n",
    "docs_split = splitter.split_documents(loaded_docs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Important Questions You Can Now Answer:\n",
    "\n",
    "1. Why do we split documents into smaller parts in GenAI pipelines?\n",
    "2. What is the difference between `create_documents()` and `split_documents()`?\n",
    "3. What happens if we don‚Äôt add chunk overlap while splitting?\n",
    "4. Can you explain what `RecursiveCharacterTextSplitter` does internally?\n",
    "5. When would you NOT use `create_documents()`?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b9370",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ‚úÖ **1. Why do we split documents into smaller parts in GenAI pipelines?**\n",
    "\n",
    "**Answer**:\n",
    "LLMs like GPT-4 or Claude have a **context window limit** (e.g., 8K or 32K tokens). If a document exceeds this limit, it can't be processed in one go.\n",
    "So we split the document into **smaller chunks** (e.g., 500 tokens), optionally with **overlapping content** (e.g., 50 tokens), to:\n",
    "\n",
    "* Stay within token limits,\n",
    "* Preserve semantic meaning,\n",
    "* Enable chunk-level embedding and retrieval.\n",
    "\n",
    "**Example**:\n",
    "A 10-page PDF is split into 30 overlapping chunks. Only the top 3 relevant chunks are passed to the LLM when answering a question, reducing noise and cost.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **2. What is the difference between `create_documents()` and `split_documents()`?**\n",
    "\n",
    "| Feature               | `create_documents()`                           | `split_documents()`                                     |\n",
    "| --------------------- | ---------------------------------------------- | ------------------------------------------------------- |\n",
    "| **Input**             | List of raw strings                            | List of `Document` objects                              |\n",
    "| **Output**            | List of `Document` chunks                      | List of `Document` chunks                               |\n",
    "| **Metadata Handling** | Auto-generates metadata (`source: '0'`, etc.)  | Preserves original metadata from input                  |\n",
    "| **Use Case**          | For first-time processing of raw text          | For splitting already-loaded documents (e.g. PDFs, web) |\n",
    "| **Example Usage**     | When working with scraped or manual text input | When using loaders like `PyPDFLoader`, `WebBaseLoader`  |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **3. What happens if we don‚Äôt add `chunk_overlap` while splitting?**\n",
    "\n",
    "**Answer**:\n",
    "If you don‚Äôt use **chunk overlap**, the end of one chunk and the start of the next will have **no shared context**, which may:\n",
    "\n",
    "* Break sentence flow,\n",
    "* Reduce accuracy in retrieval,\n",
    "* Cause LLMs to lose understanding of context transitions.\n",
    "\n",
    "**Example**:\n",
    "A chunk ends with ‚ÄúBarack Obama was born in‚Äù and the next chunk starts with ‚ÄúHawaii.‚Äù Without overlap, this fact is split awkwardly.\n",
    "\n",
    "Overlap (e.g., 50 tokens) ensures such facts are preserved in multiple chunks.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **4. Can you explain what `RecursiveCharacterTextSplitter` does internally?**\n",
    "\n",
    "**Answer**:\n",
    "`RecursiveCharacterTextSplitter` is a **multi-level smart text splitter**. It works like this:\n",
    "\n",
    "1. It tries to split using **higher-level separators** first:\n",
    "   `[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]`\n",
    "2. If the resulting chunk is too long, it **recursively** applies the next separator level.\n",
    "3. If it reaches the character level and the chunk is still too long, it hard-splits it.\n",
    "\n",
    "üí° The idea is to keep chunks **coherent**, preferring paragraph/sentence boundaries.\n",
    "\n",
    "**Example**:\n",
    "A 1,000-character paragraph is too long:\n",
    "\n",
    "* First try splitting by `\\n\\n` ‚Üí if not enough,\n",
    "* Then by `.` ‚Üí if still too long,\n",
    "* Then by spaces or characters.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **5. When would you NOT use `create_documents()`?**\n",
    "\n",
    "**Answer**:\n",
    "You **should not use `create_documents()`** when you already have **loaded documents** using LangChain loaders (e.g., PDF, web, Notion, Arxiv).\n",
    "\n",
    "Why?\n",
    "\n",
    "* These loaders return `Document` objects with **metadata** like page number, file path, URL, etc.\n",
    "* `create_documents()` would override or discard this metadata.\n",
    "\n",
    "üëâ Instead, use `split_documents()` to **retain all original metadata** and just split the content.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Bonus: What are some key parameters of `RecursiveCharacterTextSplitter`?**\n",
    "\n",
    "| Parameter         | Description                                         |\n",
    "| ----------------- | --------------------------------------------------- |\n",
    "| `chunk_size`      | Max size of each chunk (in characters/tokens)       |\n",
    "| `chunk_overlap`   | Characters/tokens shared between adjacent chunks    |\n",
    "| `separators`      | List of split symbols (priority order)              |\n",
    "| `length_function` | How chunk length is measured (`len` or token count) |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763994c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **What is `length_function` in `RecursiveCharacterTextSplitter`?**\n",
    "\n",
    "### üîπ Definition:\n",
    "\n",
    "`length_function` is a **custom function** that tells the splitter **how to measure the length** of each chunk.\n",
    "By default, LangChain uses Python‚Äôs `len()` to count **characters**.\n",
    "\n",
    "```python\n",
    "length_function = len\n",
    "```\n",
    "\n",
    "But in **LLMs**, we usually care about **tokens**, not characters.\n",
    "So we can pass a custom function like `tiktoken_len()` to **count tokens instead** of characters.\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Analogy:\n",
    "\n",
    "> Imagine you're packing suitcases (chunks).\n",
    "> You need to make sure **each suitcase doesn't exceed airline limits**.\n",
    "\n",
    "* If you use a **ruler**, you're measuring **inches** (characters).\n",
    "* If you use a **weighing scale**, you're measuring **weight in kg** (tokens).\n",
    "\n",
    "üß≥ In GenAI, tokens are like \"kg\" ‚Äî they **directly affect cost and limits** of LLMs.\n",
    "So `length_function` = how you want to **measure your suitcase size**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example 1 ‚Äì Default Behavior (Character Count)\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog. \" * 10\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks[:2])\n",
    "```\n",
    "\n",
    "### üîç What happens?\n",
    "\n",
    "* It splits every 100 **characters**, with 10 characters overlapping.\n",
    "* It doesn‚Äôt care how many **tokens** that makes ‚Äî could be more or fewer than LLM limits.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example 2 ‚Äì Token-Based Splitting using `tiktoken`\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Use tiktoken tokenizer (e.g., for GPT-3.5)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tiktoken_len = lambda text: len(encoding.encode(text))\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=tiktoken_len\n",
    ")\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog. \" * 10\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks[:2])\n",
    "```\n",
    "\n",
    "### üîç What happens here?\n",
    "\n",
    "* It splits based on **token count**, not characters.\n",
    "* Useful when:\n",
    "\n",
    "  * You want fine control over **token budgets**,\n",
    "  * You're going to embed or send text to LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why is `length_function` important?\n",
    "\n",
    "| Scenario                    | Should You Use Token-Based?    |\n",
    "| --------------------------- | ------------------------------ |\n",
    "| Embedding chunks            | ‚úÖ Yes ‚Äî costs depend on tokens |\n",
    "| Prompt engineering          | ‚úÖ Yes ‚Äî LLMs use token limits  |\n",
    "| Simple character processing | ‚ùå No ‚Äî `len()` is enough       |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849f651",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üîπ Step 1: Sample Text\n",
    "\n",
    "```python\n",
    "text = \"LangChain helps developers build applications powered by language models. \" * 5\n",
    "```\n",
    "\n",
    "üëâ This gives us a **repetitive** paragraph useful to illustrate chunk boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 2: Setup\n",
    "\n",
    "#### üîß Imports + Tokenizer\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "```\n",
    "\n",
    "#### üîß Token-based function\n",
    "\n",
    "```python\n",
    "# For OpenAI models like gpt-3.5, gpt-4\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "token_count = lambda text: len(encoding.encode(text))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 3: Split using Character Length\n",
    "\n",
    "```python\n",
    "splitter_char = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=120,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len  # character count\n",
    ")\n",
    "\n",
    "chunks_char = splitter_char.split_text(text)\n",
    "```\n",
    "\n",
    "#### ‚úÖ Output (first two chunks):\n",
    "\n",
    "```python\n",
    "[\n",
    "  \"LangChain helps developers build applications powered by language models. LangChain helps developers build applications powered by\",\n",
    "  \"build applications powered by language models. LangChain helps developers build applications powered by language models. LangChain h\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 4: Split using Token Count\n",
    "\n",
    "```python\n",
    "splitter_token = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=40,\n",
    "    chunk_overlap=10,\n",
    "    length_function=token_count\n",
    ")\n",
    "\n",
    "chunks_token = splitter_token.split_text(text)\n",
    "```\n",
    "\n",
    "#### ‚úÖ Output (first two chunks):\n",
    "\n",
    "```python\n",
    "[\n",
    "  \"LangChain helps developers build applications powered by language models. LangChain helps developers build applications\",\n",
    "  \"powered by language models. LangChain helps developers build applications powered by language models. LangChain helps\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Key Observations:\n",
    "\n",
    "| Feature           | Character-Based                     | Token-Based                               |\n",
    "| ----------------- | ----------------------------------- | ----------------------------------------- |\n",
    "| Measures          | Length in characters (letters)      | Token count (used by LLMs)                |\n",
    "| Chunk Consistency | Less precise for model input length | Matches LLM token budgets more accurately |\n",
    "| Overlap Meaning   | Overlap = characters                | Overlap = tokens                          |\n",
    "| Ideal For         | Simple local splits, text preview   | LLMs, Embeddings, Prompt contexts         |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed883e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **1. What is a token in an LLM?**\n",
    "\n",
    "A **token** is the smallest unit of text that a language model understands. Tokens can represent words, subwords, punctuation marks, spaces, or even parts of words. For example, the word `unbelievable` might be split into multiple tokens like `un`, `believ`, `able`.\n",
    "\n",
    "Models like GPT or Claude don‚Äôt process entire sentences directly ‚Äî they process **sequences of tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **2. How do tokens differ from words and characters?**\n",
    "\n",
    "* **Words** are language units separated by spaces.\n",
    "* **Characters** are individual letters or symbols.\n",
    "* **Tokens** are **model-specific subword units**, created by a **tokenizer**.\n",
    "\n",
    "Example:\n",
    "\n",
    "```text\n",
    "Text: \"Don't stop\"\n",
    "Words: [\"Don't\", \"stop\"]\n",
    "Characters: ['D', 'o', 'n', \"'\", 't', ..., 'p']\n",
    "Tokens (tiktoken): [\"Don\", \"'\", \"t\", \" stop\"]\n",
    "```\n",
    "\n",
    "So, tokens can be **shorter or longer** than words, and may include parts of words + punctuation.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **3. Why is tokenization important in LLM pipelines?**\n",
    "\n",
    "Tokenization is critical because:\n",
    "\n",
    "* LLMs **only operate on tokens**.\n",
    "* The **input/output token limits** of the model depend on token count, not word count.\n",
    "* Tokenization defines how text is **segmented** and influences cost, latency, and performance.\n",
    "* Embedding and chunking logic relies on token counts to prevent cutoff errors or data loss.\n",
    "\n",
    "So, poor understanding of tokenization can lead to:\n",
    "\n",
    "* Truncated prompts\n",
    "* Higher cost\n",
    "* Lower-quality generation\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **4. How do you count tokens before sending a prompt to GPT-4?**\n",
    "\n",
    "I use OpenAI‚Äôs [`tiktoken`](https://github.com/openai/tiktoken) library to simulate GPT-4‚Äôs tokenizer:\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4/3.5 encoding\n",
    "text = \"LangChain helps developers build LLM-powered apps.\"\n",
    "tokens = encoding.encode(text)\n",
    "print(len(tokens))  # Gives token count\n",
    "```\n",
    "\n",
    "This helps me ensure that my prompt stays within the model‚Äôs token limit.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **5. What are the token limits of popular models?**\n",
    "\n",
    "| Model             | Max Tokens |\n",
    "| ----------------- | ---------- |\n",
    "| GPT-3.5           | 4,096      |\n",
    "| GPT-4 Turbo       | 128,000    |\n",
    "| Claude 2          | 100,000    |\n",
    "| Gemini 1.5        | \\~1M       |\n",
    "| Cohere Command R+ | \\~128K     |\n",
    "\n",
    "Note: These limits include both **prompt** and **response** tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **6. What happens if you exceed token limits?**\n",
    "\n",
    "If you exceed token limits:\n",
    "\n",
    "* **OpenAI/GPT models** will return an error like `context length exceeded`.\n",
    "* **Claude or Gemini** may silently truncate or raise errors.\n",
    "* If you split documents improperly, your chunk might get **cut off mid-sentence**, leading to:\n",
    "\n",
    "  * Poor completions\n",
    "  * Loss of important context\n",
    "  * Failed memory or retrieval steps in RAG pipelines\n",
    "\n",
    "So always chunk text using **token-aware splitters** like `RecursiveCharacterTextSplitter` with a token-based `length_function`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **7. How does chunking relate to tokens? Why not use character-based chunks?**\n",
    "\n",
    "Chunking is used to split large documents into smaller parts before feeding them into an LLM. It must be **token-aware** because:\n",
    "\n",
    "* LLMs operate in token space.\n",
    "* Two chunks with the same number of characters might have **very different token lengths**.\n",
    "* Character-based splitting might **exceed token limits** or **cut tokens improperly**.\n",
    "\n",
    "That‚Äôs why in LangChain and other libraries, we use:\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=token_count\n",
    ")\n",
    "```\n",
    "\n",
    "This ensures **chunks are token-safe** and avoid model errors.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **8. How do emojis and Unicode characters affect token count?**\n",
    "\n",
    "Emojis and non-English scripts (like Chinese, Arabic, Devanagari) often take **more than one token**.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* \"üí°\" = 1‚Äì4 tokens depending on tokenizer\n",
    "* \"‰Ω†Â•Ω\" (Chinese) = 1 token per character\n",
    "* \"‚ù§Ô∏è\" = 2 tokens\n",
    "\n",
    "So if you're processing **multilingual or emoji-rich content**, you should **always count tokens** instead of assuming based on character length.\n",
    "\n",
    "---\n",
    "\n",
    "## üí¨ Bonus Tip: Answer With Tools\n",
    "\n",
    "In real interviews, **mentioning tools or best practices** shows you're practical.\n",
    "\n",
    "Example:\n",
    "\n",
    "> ‚ÄúTo ensure my prompts stay under token limits, I always simulate the tokenizer using `tiktoken`. For visual inspection, I also use the OpenAI Tokenizer web tool.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Summary Table\n",
    "\n",
    "| Concept        | Description                                                |\n",
    "| -------------- | ---------------------------------------------------------- |\n",
    "| Token          | Smallest LLM-readable unit of text                         |\n",
    "| Tokenizer      | Tool to break text into tokens                             |\n",
    "| Why Important? | Cost, chunking, context size, performance                  |\n",
    "| Count Tokens   | Use `tiktoken` for OpenAI, online tools for others         |\n",
    "| Interview Prep | Focus on LLM limits, tokenizer behaviors, chunking impacts |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e16b95",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **1. Purpose of Text Splitters in LangChain**\n",
    "\n",
    "Text splitters are used to divide large documents into smaller chunks that:\n",
    "\n",
    "* **Fit within the token limits** of LLMs (e.g., GPT-4),\n",
    "* Preserve **semantic meaning**, and\n",
    "* Improve performance in **RAG (Retrieval-Augmented Generation)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **2. CharacterTextSplitter ‚Äì Basic Version**\n",
    "\n",
    "### üîπ How It Works:\n",
    "\n",
    "* Splits text **purely based on character count**, without caring about sentence/paragraph boundaries.\n",
    "* Simple logic: it takes N characters (default 1000) and moves ahead with a given overlap.\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "```\n",
    "\n",
    "### ‚úÖ **When to Use:**\n",
    "\n",
    "* You want **very simple, fast** splitting.\n",
    "* Your text has **clear separators** (e.g., paragraphs separated by newlines).\n",
    "* You are building a **proof of concept** or minimal demo.\n",
    "\n",
    "### ‚ö†Ô∏è Limitation:\n",
    "\n",
    "* Can **break sentences** or thoughts mid-way.\n",
    "* Doesn't try to be intelligent about preserving context.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **3. RecursiveCharacterTextSplitter ‚Äì Smart Version**\n",
    "\n",
    "### üîπ How It Works:\n",
    "\n",
    "* Tries to split text by **increasingly smaller logical units**:\n",
    "\n",
    "  * First by paragraphs (`\\n\\n`)\n",
    "  * Then by sentences (`.`)\n",
    "  * Then by words (` `)\n",
    "  * Finally by characters\n",
    "* Uses a **recursive strategy** to preserve as much **semantic coherence** as possible.\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "```\n",
    "\n",
    "### ‚úÖ **When to Use:**\n",
    "\n",
    "* You want **intelligent chunking** that avoids breaking meaning.\n",
    "* You're preparing input for **RAG**, **summarization**, or **Q\\&A** tasks.\n",
    "* Your text is **unstructured**, like PDFs, scraped HTML, research papers.\n",
    "* You care about **sentence integrity** and **semantic boundaries**.\n",
    "\n",
    "### üîç Example:\n",
    "\n",
    "Imagine splitting this text:\n",
    "\n",
    "```\n",
    "LangChain is an open-source framework that helps developers build applications with LLMs.\n",
    "It provides tools for retrieval, chaining, memory, and more.\n",
    "```\n",
    "\n",
    "**CharacterTextSplitter** may cut after 100 characters **in the middle of a sentence**.\n",
    "**RecursiveCharacterTextSplitter** would prefer to **split between sentences or paragraphs**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ ** Summary:**\n",
    "\n",
    "| Feature                    | CharacterTextSplitter             | RecursiveCharacterTextSplitter            |\n",
    "| -------------------------- | --------------------------------- | ----------------------------------------- |\n",
    "| Strategy                   | Fixed-size, naive character split | Hierarchical: paragraph ‚Üí sentence ‚Üí word |\n",
    "| Preserves Semantic Meaning | ‚ùå Often breaks sentences          | ‚úÖ Tries to preserve logical boundaries    |\n",
    "| Performance                | ‚úÖ Faster                          | ‚ö†Ô∏è Slightly slower but smarter            |\n",
    "| Use Case                   | Quick demos, structured text      | RAG, summarization, QA, unstructured docs |\n",
    "| When to Use                | Minimal control needed            | Need semantic-aware chunks                |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Important Q: When would RecursiveCharacterTextSplitter fail?\n",
    "\n",
    "**Answer**:\n",
    "RecursiveCharacterTextSplitter relies on finding good separators (like `\\n`, `.`). If the text has no clear structure (e.g., **binary data**, **code without line breaks**, or poorly OCR-ed documents), it may default to token count-based splitting at the lowest level (characters). In that case, **custom logic or regex-based splitting** might be required.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b04d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
