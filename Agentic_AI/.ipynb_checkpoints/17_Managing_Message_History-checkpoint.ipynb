{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5f0d83",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß† 1. Why Do We Need to Manage Conversation History?\n",
    "\n",
    "### üéØ Problem:\n",
    "\n",
    "If message history is left **unbounded**, the list of chat messages grows with each interaction. Over time, this causes:\n",
    "\n",
    "* **Context Overflow**: LLMs like GPT-4 have a maximum context window (e.g., 128k tokens for GPT-4-turbo).\n",
    "* **Performance Degradation**: Sending long context increases token usage and cost.\n",
    "* **Errors**: LLM may throw \"context too long\" errors.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-World Scenario:\n",
    "\n",
    "Suppose you're building a chatbot that stores every message. After 100 messages, you hit:\n",
    "\n",
    "* A total of 20,000 tokens used in history alone.\n",
    "* Then the next user message + expected response might overflow the model's 32,000 token limit.\n",
    "* LLM may fail with `context_length_exceeded` error.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÇÔ∏è 2. `trim_messages` ‚Äì Your Savior!\n",
    "\n",
    "### What Is It?\n",
    "\n",
    "`trim_messages` is a LangChain utility used to **prune/trim chat history** so the total token count fits within a safe limit before being passed to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ 3. Key Parameters of `trim_messages`\n",
    "\n",
    "Let‚Äôs go over each one with examples:\n",
    "\n",
    "| Parameter        | Description                                                                                                 | Example                                               |\n",
    "| ---------------- | ----------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |\n",
    "| `max_tokens`     | Max tokens allowed for chat history. It ensures total tokens (from messages) don‚Äôt exceed this number.      | `max_tokens=1000`                                     |\n",
    "| `strategy`       | Strategy to trim messages. Common: `\"tokens\"`, `\"recency\"`, `\"length\"`                                      | `\"tokens\"` trims older messages based on token count. |\n",
    "| `token_counter`  | Custom function to count tokens (needed for accurate trimming). Use `get_tokenizer()` for OpenAI tokenizer. | `token_counter=get_tokenizer(\"gpt-4\")`                |\n",
    "| `include_system` | Whether to include the system message when trimming.                                                        | `include_system=False` trims only human/AI messages.  |\n",
    "| `allow_partials` | Allow cutting a message partially if full message crosses token limit.                                      | `allow_partials=True`                                 |\n",
    "| `start_on`       | Whether to start trimming from start or end.                                                                | `start_on=\"start\"` trims old messages first.          |\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 4. `itemgetter` and `RunnablePassthrough`\n",
    "\n",
    "### `itemgetter`:\n",
    "\n",
    "* A utility to extract certain fields from input dictionary.\n",
    "* Example: `itemgetter(\"input\")` will fetch only the `\"input\"` key from:\n",
    "\n",
    "```python\n",
    "{\"input\": \"Hello\", \"username\": \"Alice\"} ‚Üí returns \"Hello\"\n",
    "```\n",
    "\n",
    "### `RunnablePassthrough`:\n",
    "\n",
    "* Acts as a no-op (identity function).\n",
    "* Used in chains when you want to pass a component‚Äôs input straight through without modification.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó 5. How to Use Trimmer with `RunnableWithMessageHistory`?\n",
    "\n",
    "Here‚Äôs how you pass a trimmer inside `RunnableWithMessageHistory`:\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,  # your chat chain\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"messages\",\n",
    "    history_factory_config={\n",
    "        \"input_trimmer\": input_trimmer  # <-- this is where trimming happens\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßë‚Äçüíª 6. ‚úÖ Complete Code Example (Multiple Sessions + ChatPromptTemplate + Trimming)\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, ChatMessageHistory\n",
    "from langchain_core.messages import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import RunnableMap\n",
    "from langchain_core.runnables.utils import itemgetter\n",
    "from langchain_core.runnables.history import trim_messages\n",
    "from langchain_core.utils.token import get_tokenizer\n",
    "\n",
    "# üóÉÔ∏è Session store\n",
    "session_store = {}\n",
    "\n",
    "# üß† Function to get session-specific message history\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "# ‚úÇÔ∏è Token counter using GPT-4 tokenizer\n",
    "token_counter = get_tokenizer(\"gpt-4\")\n",
    "\n",
    "# üéØ Input trimmer\n",
    "input_trimmer = trim_messages(\n",
    "    max_tokens=1000,\n",
    "    strategy=\"tokens\",\n",
    "    token_counter=token_counter,\n",
    "    include_system=False,\n",
    "    allow_partials=True,\n",
    "    start_on=\"start\"\n",
    ")\n",
    "\n",
    "# üß± ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant for user {username}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ü§ñ LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# üîó Final chain\n",
    "chain = (\n",
    "    {\n",
    "        \"input\": itemgetter(\"input\"),\n",
    "        \"username\": itemgetter(\"username\"),\n",
    "        \"messages\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# üì¶ Wrap with RunnableWithMessageHistory\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"messages\",\n",
    "    history_factory_config={\n",
    "        \"input_trimmer\": input_trimmer\n",
    "    }\n",
    ")\n",
    "\n",
    "# ‚úÖ Simulating a conversation\n",
    "session_id = \"user_123\"\n",
    "\n",
    "# First message\n",
    "print(chain_with_history.invoke(\n",
    "    {\"input\": \"Hi there!\", \"username\": \"Alice\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    "))\n",
    "\n",
    "# Next message\n",
    "print(chain_with_history.invoke(\n",
    "    {\"input\": \"Tell me about LangChain.\", \"username\": \"Alice\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    "))\n",
    "\n",
    "# ‚úÖ Print trimmed session history\n",
    "print(\"Current Chat History:\")\n",
    "for msg in session_store[session_id].messages:\n",
    "    print(f\"{msg.type.title()}: {msg.content}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "| Concept                      | Role                                |\n",
    "| ---------------------------- | ----------------------------------- |\n",
    "| `ChatMessageHistory`         | Stores per-session messages         |\n",
    "| `RunnableWithMessageHistory` | Ties session memory to LLM chain    |\n",
    "| `MessagesPlaceholder`        | Injects messages into prompt        |\n",
    "| `trim_messages()`            | Limits memory footprint by trimming |\n",
    "| `itemgetter`                 | Extracts input fields               |\n",
    "| `RunnablePassthrough`        | Passes messages directly to prompt  |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Questions You Must Be Able to Answer\n",
    "\n",
    "1. Why is trimming chat history important in LLMs?\n",
    "2. How does `trim_messages` prevent context overflows?\n",
    "3. What happens if `max_tokens` is too small in trimming config?\n",
    "4. How does `MessagesPlaceholder` integrate with memory?\n",
    "5. How would you build persistent memory instead of in-memory dict?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b0160",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. **Why is trimming chat history important in LLMs?**\n",
    "\n",
    "### üß† Context Window Limitation\n",
    "\n",
    "LLMs like GPT-4-turbo have **fixed context windows** (e.g., 128k tokens for GPT-4-turbo). This means:\n",
    "\n",
    "* The LLM can only ‚Äúsee‚Äù and process a limited number of tokens at once.\n",
    "* If chat history grows without control, it can exceed this limit and cause:\n",
    "\n",
    "  * **Truncation**: Earlier messages get ignored.\n",
    "  * **Errors**: `context_length_exceeded`\n",
    "  * **Increased cost**: More tokens = more \\$\\$.\n",
    "\n",
    "### üéØ Why it‚Äôs important?\n",
    "\n",
    "Because Gen AI applications are **chat-based and iterative**, history grows naturally. If you don‚Äôt trim or manage history:\n",
    "\n",
    "* Performance suffers.\n",
    "* Cost skyrockets.\n",
    "* Model may respond with hallucinations due to missing context.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. **How does `trim_messages` prevent context overflows?**\n",
    "\n",
    "### üîß `trim_messages()` Function\n",
    "\n",
    "LangChain‚Äôs `trim_messages()` helps **prune** message history **before** passing it to the LLM, by removing or shortening older messages to stay within `max_tokens`.\n",
    "\n",
    "### üîÅ How it Works:\n",
    "\n",
    "1. Counts total tokens in the message history (based on `token_counter`).\n",
    "2. If token count > `max_tokens`, it starts trimming based on:\n",
    "\n",
    "   * **Strategy** (`tokens`, `recency`, etc.)\n",
    "   * **Trimming direction** (`start_on=‚Äústart‚Äù`)\n",
    "   * **Partial removal allowed or not** (`allow_partials=True`)\n",
    "3. Returns the **pruned message list** to keep things within bounds.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. **What happens if `max_tokens` is too small in trimming config?**\n",
    "\n",
    "If `max_tokens` is too small (e.g., 10 or 50 tokens):\n",
    "\n",
    "* It might trim all meaningful messages out.\n",
    "* LLM could receive **incomplete or no context**, leading to:\n",
    "\n",
    "  * Generic replies\n",
    "  * Misunderstandings (no memory of past interactions)\n",
    "* `allow_partials=False` will make it fail to include anything if one message is too long.\n",
    "\n",
    "üõ†Ô∏è **Solution:**\n",
    "Always estimate:\n",
    "\n",
    "* Token cost of input\n",
    "* Expected token cost of output\n",
    "* Keep `max_tokens` of history reasonable (e.g., 1,000‚Äì3,000)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. **How does `MessagesPlaceholder` integrate with memory?**\n",
    "\n",
    "### ‚ú® `MessagesPlaceholder(variable_name=\"messages\")`\n",
    "\n",
    "In `ChatPromptTemplate`, this placeholder is used to **dynamically inject message history** into the prompt.\n",
    "\n",
    "It connects to the `ChatMessageHistory` that is returned from `RunnableWithMessageHistory`.\n",
    "\n",
    "### üß± Example:\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a bot\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* `messages` is the name of the variable to be replaced by message history.\n",
    "* The chain must be configured to pass `messages` to this template (done via `RunnableWithMessageHistory`).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. **How would you build persistent memory instead of in-memory dict?**\n",
    "\n",
    "### üß† In-memory:\n",
    "\n",
    "```python\n",
    "session_store = {}\n",
    "```\n",
    "\n",
    "Good for demos, but:\n",
    "\n",
    "* Doesn‚Äôt persist across restarts\n",
    "* Can‚Äôt scale across servers\n",
    "\n",
    "### üíæ Persistent Alternatives:\n",
    "\n",
    "| Option              | Description                                                                     |\n",
    "| ------------------- | ------------------------------------------------------------------------------- |\n",
    "| **Redis**           | Fast, scalable in-memory store with persistence. Use `RedisChatMessageHistory`. |\n",
    "| **SQL/NoSQL DB**    | Store JSON/chat logs in MongoDB/PostgreSQL                                      |\n",
    "| **S3/Blob Storage** | Serialize chat history and upload                                               |\n",
    "\n",
    "### ‚úÖ Redis Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    return RedisChatMessageHistory(session_id=session_id, url=\"redis://localhost:6379\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Question                            | Summary Answer                                                       |\n",
    "| ----------------------------------- | -------------------------------------------------------------------- |\n",
    "| Why trim chat history?              | To avoid LLM context overflow, high costs, and errors.               |\n",
    "| How does `trim_messages` help?      | It removes/prunes history based on token limits using strategies.    |\n",
    "| What if `max_tokens` is too small?  | Chat history becomes meaningless, causing poor responses.            |\n",
    "| What does `MessagesPlaceholder` do? | Injects history into prompts dynamically using memory.               |\n",
    "| How to make memory persistent?      | Use Redis, a database, or storage systems instead of in-memory dict. |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ  Tips\n",
    "\n",
    "1. **What if your model supports 128k tokens, should you still trim?**\n",
    "\n",
    "   * Yes. Memory should always be scoped, even in large windows, for cost and relevance.\n",
    "\n",
    "2. **How do you customize trimming for specific users (e.g., VIP users)?**\n",
    "\n",
    "   * Use user-level `max_tokens` or different trimming strategies dynamically.\n",
    "\n",
    "3. **How would you handle trimming in a multilingual bot?**\n",
    "\n",
    "   * Tokenization is language-specific; choose token counters that are accurate across language types.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575015af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
