{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f511fea6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üìå What is the **Purpose** of `RecursiveJsonTextSplitter`?\n",
    "\n",
    "Structured data like **JSON** (often from APIs, logs, telemetry, config files, etc.) is hierarchical and nested. Standard text splitters don‚Äôt handle structure well ‚Äî they treat everything as plain text.\n",
    "\n",
    "> ‚úÖ **Goal:** To **preserve the structure and context** of JSON data while splitting it into manageable chunks for LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What Does It Do?\n",
    "\n",
    "* Parses a JSON object.\n",
    "* Traverses it **recursively** (depth-first) ‚Äî preserving context and keys.\n",
    "* Converts **nested JSON paths** into meaningful, chunked text documents with **metadata**.\n",
    "* Ensures the LLM can **understand key-value relationships** and **hierarchy** in JSON.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è How Does It Work?\n",
    "\n",
    "### Mechanism:\n",
    "\n",
    "* You feed a JSON dict or string.\n",
    "* It recursively traverses keys and values:\n",
    "\n",
    "  * If a value is primitive (string, int), it records the key path.\n",
    "  * If a value is another dict or list, it dives in recursively.\n",
    "* It produces `Document` objects with:\n",
    "\n",
    "  * `page_content`: the JSON chunk as text\n",
    "  * `metadata`: path to the content in the hierarchy\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Example with Real Use Case\n",
    "\n",
    "### üß© Input: JSON from an E-commerce Order System\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"order_id\": \"123\",\n",
    "  \"customer\": {\n",
    "    \"name\": \"Alice\",\n",
    "    \"email\": \"alice@example.com\"\n",
    "  },\n",
    "  \"items\": [\n",
    "    {\n",
    "      \"product\": \"Laptop\",\n",
    "      \"price\": 999,\n",
    "      \"quantity\": 1\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"Mouse\",\n",
    "      \"price\": 25,\n",
    "      \"quantity\": 2\n",
    "    }\n",
    "  ],\n",
    "  \"notes\": \"Deliver between 10am-5pm\"\n",
    "}\n",
    "```\n",
    "\n",
    "### üìú Code:\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveJsonTextSplitter\n",
    "\n",
    "json_data = {\n",
    "  \"order_id\": \"123\",\n",
    "  \"customer\": {\n",
    "    \"name\": \"Alice\",\n",
    "    \"email\": \"alice@example.com\"\n",
    "  },\n",
    "  \"items\": [\n",
    "    {\"product\": \"Laptop\", \"price\": 999, \"quantity\": 1},\n",
    "    {\"product\": \"Mouse\", \"price\": 25, \"quantity\": 2}\n",
    "  ],\n",
    "  \"notes\": \"Deliver between 10am-5pm\"\n",
    "}\n",
    "\n",
    "splitter = RecursiveJsonTextSplitter(max_chunk_size=200)\n",
    "docs = splitter.split_json(json_data)\n",
    "```\n",
    "\n",
    "### üßæ Output:\n",
    "\n",
    "Each chunk becomes a `Document` like:\n",
    "\n",
    "```python\n",
    "Document(\n",
    "  page_content='order_id: 123',\n",
    "  metadata={'path': 'order_id'}\n",
    ")\n",
    "\n",
    "Document(\n",
    "  page_content='customer.name: Alice',\n",
    "  metadata={'path': 'customer.name'}\n",
    ")\n",
    "\n",
    "Document(\n",
    "  page_content='items[0].product: Laptop',\n",
    "  metadata={'path': 'items[0].product'}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Features\n",
    "\n",
    "* ‚úÖ Preserves **semantic structure** of nested JSON\n",
    "* ‚úÖ Adds **metadata paths** to help identify the data‚Äôs location\n",
    "* ‚úÖ Good for LLMs to reason over **structured records**\n",
    "* ‚úÖ Avoids flattening the JSON ‚Äî no loss of information\n",
    "* ‚úÖ Integrates with RAG workflows (metadata helps retrieval)\n",
    "* ‚úÖ Works well with vector stores for deep retrieval\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Important Parameters\n",
    "\n",
    "| Parameter        | Purpose                                                                 |\n",
    "| ---------------- | ----------------------------------------------------------------------- |\n",
    "| `max_chunk_size` | Max characters per chunk. Helps prevent breaking in middle of structure |\n",
    "| `keep_separator` | Keeps `:` and `[]` etc. when splitting (defaults to `True`)             |\n",
    "| `add_metadata`   | Includes metadata path in result                                        |\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Limitations\n",
    "\n",
    "* ‚ùå Doesn't tokenize by model's tokenizer ‚Äî may need further splitting with `RecursiveCharacterTextSplitter`.\n",
    "* ‚ùå Works best on well-formed, **clean JSON** (not arbitrary blobs or invalid JSON).\n",
    "* ‚ùå Metadata paths might get long/deep ‚Äî need trimming if used in UI.\n",
    "* ‚ùå Splits are based on character limits, not semantic grouping ‚Äî so large subtrees may get split mid-way.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ When to Use It?\n",
    "\n",
    "| Use Case                            | Why RecursiveJsonTextSplitter?                              |\n",
    "| ----------------------------------- | ----------------------------------------------------------- |\n",
    "| Logs, API responses, telemetry data | Preserve nested key-value structure                         |\n",
    "| JSON config files                   | Keep path context for each setting                          |\n",
    "| Structured documents in RAG         | Enables metadata-based retrieval and chunking               |\n",
    "| Chain-of-thought or reasoning tasks | Helps LLMs see full structure and relationships in the data |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Important Questions\n",
    "\n",
    "1. **Why would you use RecursiveJsonTextSplitter over a generic TextSplitter?**\n",
    "2. **How does it preserve the hierarchy of nested structures in the output?**\n",
    "3. **What kind of metadata is generated, and how can it be useful in retrieval tasks?**\n",
    "4. **What challenges do you face when chunking deeply nested JSON?**\n",
    "5. **How would you preprocess a raw JSON log file for LLM ingestion using LangChain?**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8f99b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ‚úÖ **1. Why would you use `RecursiveJsonTextSplitter` over a generic `TextSplitter`?**\n",
    "\n",
    "**Answer:**\n",
    "Generic `TextSplitter` (like `CharacterTextSplitter`) treats input as flat, unstructured text. It doesn't understand nested data structures like JSON.\n",
    "\n",
    "In contrast, `RecursiveJsonTextSplitter`:\n",
    "\n",
    "* Preserves the **hierarchical structure** of JSON.\n",
    "* Retains **contextual paths** (e.g., `items[0].product`) using metadata.\n",
    "* Helps LLMs **understand** relationships between keys and values.\n",
    "\n",
    "üîë *Use it when you want to chunk structured JSON data without losing its meaning.*\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **2. How does it preserve the hierarchy of nested structures in the output?**\n",
    "\n",
    "**Answer:**\n",
    "It recursively traverses the JSON:\n",
    "\n",
    "* For every primitive value, it records the **full path** to that value (like `customer.name`, `items[1].quantity`).\n",
    "* Each value becomes a separate `Document`, and the path is stored in the `metadata`.\n",
    "\n",
    "This approach ensures that:\n",
    "\n",
    "* The LLM sees **contextual relationships**.\n",
    "* You can trace back where each chunk came from.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **3. What kind of metadata is generated, and how can it be useful in retrieval tasks?**\n",
    "\n",
    "**Answer:**\n",
    "Each chunk is wrapped in a `Document` object with:\n",
    "\n",
    "```python\n",
    "Document(\n",
    "  page_content='items[0].product: Laptop',\n",
    "  metadata={'path': 'items[0].product'}\n",
    ")\n",
    "```\n",
    "\n",
    "üìå **Usefulness in retrieval:**\n",
    "\n",
    "* The metadata (like `items[1].product`) acts as a **unique identifier** or **context tag**.\n",
    "* It enables **metadata filtering** in vector databases.\n",
    "* It can improve the relevance of **search and retrieval** in RAG pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **4. What challenges do you face when chunking deeply nested JSON?**\n",
    "\n",
    "**Answer:**\n",
    "Some key challenges include:\n",
    "\n",
    "* **Chunk size overflow:** Deeply nested data can generate large strings that exceed `max_chunk_size`.\n",
    "* **Loss of semantic grouping:** Related items across siblings (like `items[0]` and `items[1]`) may be separated into different chunks.\n",
    "* **Too deep metadata paths:** Long paths like `root.level1.level2.level3.key` may need truncation in downstream UI or indexing.\n",
    "* **Non-primitive values** (like nested objects/lists) require careful traversal and formatting.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **5. How would you preprocess a raw JSON log file for LLM ingestion using LangChain?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Here‚Äôs a clean step-by-step:\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveJsonTextSplitter\n",
    "import json\n",
    "\n",
    "# Step 1: Load raw JSON logs\n",
    "with open(\"log.json\") as f:\n",
    "    raw_json = json.load(f)\n",
    "\n",
    "# Step 2: Create the splitter\n",
    "splitter = RecursiveJsonTextSplitter(max_chunk_size=300)\n",
    "\n",
    "# Step 3: Split JSON into chunks\n",
    "docs = splitter.split_json(raw_json)\n",
    "\n",
    "# Step 4 (optional): Store in vector DB or pass to LLM\n",
    "```\n",
    "\n",
    "üîç *This preserves structure, enables smart chunking, and prepares logs for semantic search or summarization.*\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ\n",
    "\n",
    "If asked to explain in a real-world analogy:\n",
    "\n",
    "> ‚ÄúRecursiveJsonTextSplitter is like reading a tree-structured folder and extracting each file along with the full path, so the AI knows not just the content but also **where** it belongs.‚Äù\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94809f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
