{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f885d6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# 1) High-level overview — what Snowpipe is and why it exists\n",
    "\n",
    "**What is Snowpipe?**\n",
    "Snowpipe is Snowflake’s *serverless continuous file ingestion* service. It watches a stage (internal or external like AWS S3) and loads new files into Snowflake automatically (micro-batches), following the `COPY INTO` logic defined in a named **PIPE**. Use it when you want new files available for analytics within minutes, without scheduling big batch COPY jobs. ([Snowflake Documentation][1])\n",
    "\n",
    "**Purpose / problem Snowpipe solves**\n",
    "\n",
    "* Removes manual/scheduled ETL runs and long latency between file arrival and availability in the datawarehouse.\n",
    "* Automates ingest so data consumers (dashboards, ML features) see fresh data quickly.\n",
    "* Offloads load compute to Snowflake’s serverless ingestion service — you don’t manage a warehouse for the ingestion itself. ([Snowflake Documentation][1])\n",
    "\n",
    "**What problem it *doesn’t* solve** (common wrong assumption)\n",
    "\n",
    "* It’s not a full streaming message platform like Kafka — it ingests files (micro-batches) or supports Snowpipe Streaming for near-real-time client streaming. For record-level, real-time event processing you might prefer a streaming solution + Snowpipe Streaming or Snowflake Streams + Tasks. ([Snowflake Documentation][2])\n",
    "\n",
    "---\n",
    "\n",
    "# 2) The story / real scenario (storytelling approach)\n",
    "\n",
    "Imagine: Acme Payments collects daily transaction files from POS terminals. Each store uploads a compressed CSV every few minutes to an S3 bucket `acme-raw/transactions/`. Analysts want the transactions table updated within ~2–5 minutes of file arrival for fraud detection dashboards.\n",
    "\n",
    "We’ll use Snowpipe so each new file in S3 is picked up and loaded into `ACME_DB.PUBLIC.TRANSACTIONS` as soon as it’s created. We’ll configure:\n",
    "\n",
    "1. External stage that points to the S3 bucket.\n",
    "2. File format for the CSV.\n",
    "3. Table to hold ingested rows.\n",
    "4. A PIPE object whose `COPY INTO` defines how to load.\n",
    "5. Auto-ingest via S3 event notifications (SNS → SQS) using the notification channel Snowflake provides.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Snowpipe workflow (step by step, simplified)\n",
    "\n",
    "1. **File appears** in S3 bucket (user SDK, app, or another service uploads file).\n",
    "2. **S3 emits an event** (object created), which is sent to SNS/SQS (depending on your AWS architecture). Snowflake must be subscribed to the SQS queue or otherwise receive those events. ([Snowflake Documentation][3])\n",
    "3. Snowflake receives the notification and **queues the file for ingestion** into the pipe.\n",
    "4. Snowpipe (Snowflake’s serverless ingestion compute) executes the `COPY INTO` defined in the pipe for those queued files and loads rows into the target table. (Snowflake prevents duplicate loads by tracking file load metadata.) ([Snowflake Documentation][1])\n",
    "5. You can monitor load history (PIPE_USAGE_HISTORY, COPY_HISTORY, UI) and respond to failures (file format issues, permission problems). ([Snowflake Documentation][4])\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Step-by-step demo (SQL + AWS items). Put these into your real account with adjustments.\n",
    "\n",
    "> Note: below SQL is runnable in Snowflake (change names to your account objects). For AWS, you’ll use the AWS console/CLI to create policy, SNS/SQS and S3 event notifications.\n",
    "\n",
    "## Snowflake side — core SQL\n",
    "\n",
    "1. **Create target table**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE acme_db.public.transactions (\n",
    "  txn_id STRING,\n",
    "  store_id STRING,\n",
    "  txn_ts TIMESTAMP_NTZ,\n",
    "  amount NUMBER(12,2),\n",
    "  raw_json VARIANT\n",
    ");\n",
    "```\n",
    "\n",
    "2. **Create file format** (CSV example)\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT acme_db.public.ff_csv\n",
    "  TYPE = 'CSV'\n",
    "  FIELD_DELIMITER = ','\n",
    "  SKIP_HEADER = 1\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  TRIM_SPACE = TRUE\n",
    "  NULL_IF = ('NULL','');\n",
    "```\n",
    "\n",
    "3. **Create external stage pointing to S3**\n",
    "   (If using S3, you can use an integration; here’s the simple stage example with credentials — in prod use STORAGE INTEGRATION + IAM roles.)\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE STAGE acme_db.public.s3_stage\n",
    "  URL='s3://acme-raw/transactions'\n",
    "  FILE_FORMAT = ff_csv\n",
    "  -- best practice: use storage integration instead of credentials\n",
    "  CREDENTIALS=(AWS_KEY_ID='XXXX' AWS_SECRET_KEY='YYYY');\n",
    "```\n",
    "\n",
    "Or use a Storage Integration (recommended) — Snowflake docs show steps to create and then configure IAM role/policy. ([Snowflake Documentation][3])\n",
    "\n",
    "4. **Create the pipe** (this is the Snowpipe object)\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE PIPE acme_db.public.pipe_transactions\n",
    "  AUTO_INGEST = TRUE\n",
    "  AS\n",
    "  COPY INTO acme_db.public.transactions\n",
    "  FROM @acme_db.public.s3_stage\n",
    "  FILE_FORMAT = (FORMAT_NAME = acme_db.public.ff_csv)\n",
    "  ON_ERROR = 'CONTINUE';  -- choose behaviour you want\n",
    "```\n",
    "\n",
    "Notes:\n",
    "\n",
    "* `AUTO_INGEST = TRUE` enables automatic notifications to trigger ingestion (you must configure S3 notifications/SQS properly). ([Snowflake Documentation][5])\n",
    "\n",
    "5. **Get the pipe’s notification channel value** (you’ll need this to configure SQS)\n",
    "\n",
    "```sql\n",
    "DESC PIPE acme_db.public.pipe_transactions;\n",
    "-- In the output look for NOTIFICATION_CHANNEL (it provides an AWS ARN or a generated channel value)\n",
    "```\n",
    "\n",
    "You’ll copy the `notification_channel` value and use it when creating the SQS subscription or in the S3 notification config (details below). ([InterWorks][6])\n",
    "\n",
    "## AWS side — key steps (high level)\n",
    "\n",
    "1. **Create an IAM role / policy** that allows Snowflake to read the S3 bucket (if using Storage Integration) — official Snowflake doc has the exact JSON for the IAM policy. Grant `s3:GetObject`, `s3:ListBucket` on bucket. ([Snowflake Documentation][3])\n",
    "\n",
    "2. **Create SNS topic or SQS queue** — common pattern:\n",
    "\n",
    "   * Create SNS topic.\n",
    "   * Create SQS queue and subscribe it to the SNS topic.\n",
    "   * Configure the S3 bucket to send `s3:ObjectCreated:*` notifications to the SNS topic (or directly to SQS).\n",
    "   * Add a policy to the SQS queue to allow the SNS topic publish and to allow Snowflake’s `notification_channel` ARN to send messages (the notification_channel is used to validate).\n",
    "\n",
    "3. **Set S3 event notifications** for relevant events (`s3:ObjectCreated:Put`, and **CompleteMultipartUpload** for large files) — if you miss `CompleteMultipartUpload`, multipart uploads may not trigger ingestion. This is a very common gotcha. ([Snowflake Documentation][7])\n",
    "\n",
    "4. **Provide/verify the notification channel**: when `AUTO_INGEST = TRUE`, Snowflake assigns a notification channel (SQS ARN or channel token). `DESC PIPE` shows it. Add that as a trusted publisher/subscriber in the SQS/SNS configuration so Snowflake will receive events. Many guides show copying `notification_channel` into the SQS configuration. ([InterWorks][6])\n",
    "\n",
    "## Alternative to auto notifications: REST API / insertFiles\n",
    "\n",
    "If you cannot rely on S3 events, you can programmatically call Snowpipe REST API `insertFiles` (via Snowflake-provided client SDKs) to tell Snowflake which files to load. That’s useful when your system knows the file names and wants to notify Snowflake directly. ([Snowflake Documentation][8])\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Important operational commands & day-to-day queries (cheat sheet)\n",
    "\n",
    "These are the queries you’ll use every day to manage/monitor Snowpipe:\n",
    "\n",
    "* **Show pipes**\n",
    "\n",
    "```sql\n",
    "SHOW PIPES IN DATABASE acme_db;\n",
    "```\n",
    "\n",
    "* **Describe pipe** (copy statement, notification channel)\n",
    "\n",
    "```sql\n",
    "DESC PIPE acme_db.public.pipe_transactions;\n",
    "```\n",
    "\n",
    "> Look for `NOTIFICATION_CHANNEL` in the output. ([Snowflake Documentation][5])\n",
    "\n",
    "* **Get pipe status** (JSON)\n",
    "\n",
    "```sql\n",
    "SELECT SYSTEM$PIPE_STATUS('acme_db.public.pipe_transactions');\n",
    "```\n",
    "\n",
    "> Useful to see `executionState` like `PAUSED`, `FAILING_OVER`, etc. ([Snowflake Documentation][9])\n",
    "\n",
    "* **Pause/resume a pipe**\n",
    "\n",
    "```sql\n",
    "ALTER PIPE acme_db.public.pipe_transactions SET PIPE_EXECUTION_PAUSED = TRUE;\n",
    "ALTER PIPE acme_db.public.pipe_transactions SET PIPE_EXECUTION_PAUSED = FALSE;\n",
    "```\n",
    "\n",
    "* **Check file load history (short window)**\n",
    "  *Table function* for recent loads (15 days window, increments vary)\n",
    "\n",
    "```sql\n",
    "SELECT * \n",
    "FROM TABLE(INFORMATION_SCHEMA.PIPE_USAGE_HISTORY(\n",
    "   DATE_RANGE_START => DATEADD('day', -1, CURRENT_TIMESTAMP()),\n",
    "   DATE_RANGE_END => CURRENT_TIMESTAMP()\n",
    "));\n",
    "```\n",
    "\n",
    "or account-level:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.PIPE_USAGE_HISTORY\n",
    "WHERE PIPE_NAME = 'ACME_DB.PUBLIC.PIPE_TRANSACTIONS'\n",
    "  AND USAGE_DATE >= '2025-10-01';\n",
    "```\n",
    "\n",
    "(Use the account-usage view for up to 365 days.) ([Snowflake Documentation][4])\n",
    "\n",
    "* **COPY_HISTORY / LOAD_HISTORY** — to inspect details of COPY statements and file loads (helpful after manual COPY or troubleshooting).\n",
    "\n",
    "* **Show failed loads**: check `LOAD_HISTORY` / `COPY_HISTORY` / pipe’s error messages (Snowsight also shows this visually). ([Snowflake Documentation][10])\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Costs and compute behaviour (short, essential)\n",
    "\n",
    "* **Serverless ingestion**: Snowpipe uses Snowflake-managed serverless compute for ingestion — you don’t need to configure a virtual warehouse to run Snowpipe (unlike manual `COPY INTO` where a warehouse is used). There is a separate billing model for Snowpipe: Snowpipe is billed per GB (newer simpler model: fixed credits per GB ingested) and other serverless attributes (older models included credits per file). Monitor `PIPE_USAGE_HISTORY` and billing lines to estimate cost. Always plan for cost monitoring because continuous micro-batches can add up. ([Snowflake Documentation][11])\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Common pitfalls & troubleshooting (with solutions)\n",
    "\n",
    "1. **No ingestion after upload**\n",
    "\n",
    "   * Missing S3 event type: large multipart uploads need `S3:ObjectCreated:CompleteMultipartUpload`. Add it. ([Snowflake Documentation][7])\n",
    "   * Wrong SQS policy or you didn’t copy the `notification_channel` correctly. Re-run `DESC PIPE` and reconfigure the SQS policy. ([InterWorks][6])\n",
    "\n",
    "2. **Duplicate loads**\n",
    "\n",
    "   * Snowpipe tracks files by name + metadata. If you overwrite files with same name in S3, Snowpipe may or may not reingest depending on metadata — best practice: write files with unique names (e.g., include UUID or timestamp) and use dedupe in downstream processing.\n",
    "\n",
    "3. **Permission issues**\n",
    "\n",
    "   * Ensure Snowflake has permission to read the S3 bucket (if using credentials or storage integration IAM role). Use the exact IAM policy from Snowflake docs. ([Snowflake Documentation][3])\n",
    "\n",
    "4. **High bill / unexpected costs**\n",
    "\n",
    "   * Check `PIPE_USAGE_HISTORY` and `ACCOUNT_USAGE.PIPE_USAGE_HISTORY` for billed bytes and credits. External processes (external tables auto metadata refreshes, etc.) can also show as “NULL” pipe entries — investigate if you see unexpected costs. ([Snowflake Documentation][12])\n",
    "\n",
    "5. **File format errors**\n",
    "\n",
    "   * If `COPY INTO` errors, set `ON_ERROR` to `CONTINUE` or route failing files to an error table; inspect `COPY_HISTORY` for error messages.\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Snowpipe advanced topics (brief pointers)\n",
    "\n",
    "* **Snowpipe Streaming** — separate capability for very low latency client streaming ingestion; billed differently and supports near-real-time streaming clients. Use when you need continuous record-by-record ingestion rather than file micro-batches. ([Snowflake Documentation][2])\n",
    "\n",
    "* **REST API insertFiles** — useful when you want to control which files to ingest (push notification from your app). Works with Snowflake SDKs (Java/Python). ([Snowflake Documentation][8])\n",
    "\n",
    "* **Snowsight UI** — visual management & lineage for pipelines/pipes; shows when pipes stall or fail and gives you tracing capability. Helpful for operations. ([Snowflake Documentation][10])\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Must-know SQL snippets (copy-paste friendly)\n",
    "\n",
    "Create a minimal pipe (example):\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE PIPE mydb.public.my_pipe AUTO_INGEST = TRUE AS\n",
    "COPY INTO mydb.public.target_table\n",
    "FROM @mydb.public.s3_stage\n",
    "FILE_FORMAT = (FORMAT_NAME = mydb.public.ff_csv)\n",
    "ON_ERROR = 'CONTINUE';\n",
    "```\n",
    "\n",
    "Pause a pipe:\n",
    "\n",
    "```sql\n",
    "ALTER PIPE mydb.public.my_pipe SET PIPE_EXECUTION_PAUSED = TRUE;\n",
    "```\n",
    "\n",
    "Get status:\n",
    "\n",
    "```sql\n",
    "SELECT SYSTEM$PIPE_STATUS('mydb.public.my_pipe');\n",
    "```\n",
    "\n",
    "See recent usage (10 minutes default):\n",
    "\n",
    "```sql\n",
    "SELECT * FROM TABLE(INFORMATION_SCHEMA.PIPE_USAGE_HISTORY());\n",
    "```\n",
    "\n",
    "Account usage (1 year):\n",
    "\n",
    "```sql\n",
    "SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.PIPE_USAGE_HISTORY WHERE PIPE_NAME = 'MYDB.PUBLIC.MY_PIPE' ORDER BY USAGE_DATE DESC LIMIT 100;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Self-test questions (you should be able to answer these)\n",
    "\n",
    "(Short answers expected when you’re ready.)\n",
    "\n",
    "1. What is the role of a `PIPE` in Snowpipe?\n",
    "2. How does Snowpipe differ from a scheduled `COPY INTO` job (compute, latency, management)?\n",
    "3. How do you configure Snowpipe auto-ingest for AWS S3 — what AWS objects are needed? (Be specific.)\n",
    "4. Where do you find the Snowpipe `notification_channel`? How is it used?\n",
    "5. What S3 event types must you include to ensure large multipart uploads are ingested?\n",
    "6. How do you pause a pipe, and why might you do that?\n",
    "7. How does Snowpipe billing work in the latest model? Where do you check billed bytes?\n",
    "8. When would you use the Snowpipe REST API `insertFiles` instead of auto-ingest?\n",
    "9. How do you diagnose a file that failed to load (what views and logs)?\n",
    "10. What best practices prevent duplicate/partial loads and unexpected costs?\n",
    "\n",
    "(If you can answer these out loud — short, correct, and concise — you’re solid.)\n",
    "\n",
    "---\n",
    "\n",
    "# 11) Quick best-practice checklist (operational)\n",
    "\n",
    "* Use **Storage Integrations + IAM role** instead of embedding keys in stages. ([Snowflake Documentation][3])\n",
    "* Name files uniquely (timestamp + uuid) to avoid re-ingestion ambiguity.\n",
    "* Include `CompleteMultipartUpload` in S3 event config. ([Snowflake Documentation][7])\n",
    "* Use `PIPE_USAGE_HISTORY` and the Account Usage views to track costs. ([Snowflake Documentation][12])\n",
    "* Monitor `SYSTEM$PIPE_STATUS` and set alerts when pipes become `PAUSED` or `FAILING_OVER`. ([Snowflake Documentation][9])\n",
    "\n",
    "---\n",
    "\n",
    "# 12) Short troubleshooting playbook (when Snowpipe doesn’t load)\n",
    "\n",
    "1. `SHOW PIPES; DESC PIPE <pipe>;` — confirm pipe exists and `AUTO_INGEST` true and copy statement correct. ([Snowflake Documentation][5])\n",
    "2. Check S3 bucket event configuration — includes `Put` and `CompleteMultipartUpload`. ([Snowflake Documentation][7])\n",
    "3. Check SQS/SNS policies and that `notification_channel` is set in SQS as expected. ([InterWorks][6])\n",
    "4. Inspect `PIPE_USAGE_HISTORY` & `COPY_HISTORY` for errors, failed files and billed bytes. ([Snowflake Documentation][4])\n",
    "5. If using REST API, check your `insertFiles` implementation and credentials. ([Snowflake Documentation][8])\n",
    "\n",
    "---\n",
    "\n",
    "# 13) Where to read / references (official docs to bookmark)\n",
    "\n",
    "* Snowpipe introduction & concepts. ([Snowflake Documentation][1])\n",
    "* Automating Snowpipe for Amazon S3 (IAM, bucket config). ([Snowflake Documentation][3])\n",
    "* `CREATE PIPE` syntax & details. ([Snowflake Documentation][5])\n",
    "* Snowpipe billing (fixed credits per GB). ([Snowflake Documentation][11])\n",
    "* Snowpipe troubleshooting (multipart uploads, notifications). ([Snowflake Documentation][7])\n",
    "\n",
    "---\n",
    "\n",
    "# 14) Final guidance — how I’d teach this in a hands-on lab\n",
    "\n",
    "1. **Lab 1 (30 mins)**: Create a simple S3 bucket + sample CSVs and a stage + pipe with `AUTO_INGEST=FALSE`. Manually `COPY INTO` to understand COPY behavior and errors.\n",
    "2. **Lab 2 (45 mins)**: Enable `AUTO_INGEST=TRUE`, set up SQS+SNS and S3 notifications, use `DESC PIPE` to copy notification channel, upload files and watch `PIPE_USAGE_HISTORY`. Fix a broken file to practice troubleshooting.\n",
    "3. **Lab 3 (30 mins)**: Use the `insertFiles` REST API to programmatically queue a file and compare latency and cost to auto-ingest.\n",
    "4. **Lab 4 (optional)**: Try Snowpipe Streaming for a sample streaming client and compare architecture & cost.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "[1]: https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro?utm_source=chatgpt.com \"Snowpipe\"\n",
    "[2]: https://docs.snowflake.com/en/user-guide/snowpipe-streaming/data-load-snowpipe-streaming-overview?utm_source=chatgpt.com \"Snowpipe Streaming\"\n",
    "[3]: https://docs.snowflake.com/en/user-guide/data-load-snowpipe-auto-s3?utm_source=chatgpt.com \"Automating Snowpipe for Amazon S3\"\n",
    "[4]: https://docs.snowflake.com/en/sql-reference/functions/pipe_usage_history?utm_source=chatgpt.com \"PIPE_USAGE_HISTORY\"\n",
    "[5]: https://docs.snowflake.com/en/sql-reference/sql/create-pipe?utm_source=chatgpt.com \"CREATE PIPE - Snowpipe\"\n",
    "[6]: https://interworks.com/blog/2023/02/21/automated-ingestion-from-aws-s3-into-snowflake-via-snowpipe/?utm_source=chatgpt.com \"Automated Ingestion from AWS S3 into Snowflake via ...\"\n",
    "[7]: https://docs.snowflake.com/en/user-guide/data-load-snowpipe-ts?utm_source=chatgpt.com \"Troubleshooting Snowpipe\"\n",
    "[8]: https://docs.snowflake.com/en/user-guide/data-load-snowpipe-rest-apis?utm_source=chatgpt.com \"Snowpipe REST API\"\n",
    "[9]: https://docs.snowflake.com/en/sql-reference/functions/system_pipe_status?utm_source=chatgpt.com \"SYSTEM$PIPE_STATUS\"\n",
    "[10]: https://docs.snowflake.com/en/user-guide/data-load-snowpipe-snowsight?utm_source=chatgpt.com \"Manage Snowpipe in Snowsight\"\n",
    "[11]: https://docs.snowflake.com/en/user-guide/data-load-snowpipe-billing?utm_source=chatgpt.com \"Snowpipe costs\"\n",
    "[12]: https://docs.snowflake.com/en/sql-reference/account-usage/pipe_usage_history?utm_source=chatgpt.com \"PIPE_USAGE_HISTORY view\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf0a36",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ What is the role of a `PIPE` in Snowpipe?\n",
    "\n",
    "A **PIPE** in Snowflake is an object that defines **how files will be loaded automatically** into a target table.\n",
    "\n",
    "It acts like a *bridge* between:\n",
    "\n",
    "* The **stage** (where your files are stored, e.g., S3), and\n",
    "* The **table** (where the data should finally land).\n",
    "\n",
    "Inside the pipe, Snowflake stores:\n",
    "\n",
    "* The `COPY INTO` command (which defines the file format, target table, and error handling).\n",
    "* The *state* of which files have already been loaded (so duplicates aren’t reloaded).\n",
    "* The *connection* to a notification channel (for auto-ingest).\n",
    "\n",
    "So you can think of it as:\n",
    "🧩 **PIPE = Automation + Metadata + COPY logic**\n",
    "\n",
    "When Snowflake receives a notification (that a new file arrived), the **PIPE** automatically executes that `COPY INTO` internally, using Snowflake’s *serverless compute*.\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ How does Snowpipe differ from a scheduled `COPY INTO` job (compute, latency, management)?\n",
    "\n",
    "| Feature        | **Snowpipe**                                                                 | **Scheduled COPY INTO**                                  |\n",
    "| -------------- | ---------------------------------------------------------------------------- | -------------------------------------------------------- |\n",
    "| **Compute**    | Uses **Snowflake-managed serverless compute** (you don’t manage a warehouse) | Requires your **own warehouse** to be up/running         |\n",
    "| **Trigger**    | Event-driven (new file upload triggers ingestion)                            | Time-based (runs on a fixed schedule, e.g., hourly)      |\n",
    "| **Latency**    | Low latency (data available within minutes)                                  | Higher latency (depends on schedule interval)            |\n",
    "| **Management** | Fully automated                                                              | You must orchestrate scheduling, retries, and monitoring |\n",
    "| **Cost model** | Charged per GB (serverless ingestion credits)                                | Warehouse credits consumed while COPY runs               |\n",
    "| **Use case**   | Near real-time ingestion                                                     | Periodic or bulk loading                                 |\n",
    "\n",
    "In short:\n",
    "➡️ **Snowpipe = Continuous, automated, serverless ingestion**\n",
    "➡️ **Scheduled COPY = Manual, batch, warehouse-based ingestion**\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ How do you configure Snowpipe auto-ingest for AWS S3 — what AWS objects are needed? (Be specific.)\n",
    "\n",
    "For **auto-ingest** to work, you need AWS services to send *notifications* to Snowflake when a new file lands in S3.\n",
    "\n",
    "Here’s the required setup (step-by-step logic):\n",
    "\n",
    "1. **S3 Bucket** – where your raw files are uploaded.\n",
    "   Example: `s3://acme-raw/transactions/`\n",
    "\n",
    "2. **SNS Topic** – receives “ObjectCreated” events from the S3 bucket.\n",
    "\n",
    "3. **SQS Queue** – subscribed to the SNS topic.\n",
    "   (Snowflake listens to this queue to know which file arrived.)\n",
    "\n",
    "4. **Snowpipe Notification Channel** – Snowflake provides this ARN when you create the pipe (`DESC PIPE` shows it).\n",
    "   You must **authorize this notification channel ARN** to publish messages to your SQS queue.\n",
    "\n",
    "5. **S3 Event Notification Configuration**\n",
    "\n",
    "   * You configure the bucket to publish the following events to the SNS topic:\n",
    "\n",
    "     * `s3:ObjectCreated:Put`\n",
    "     * `s3:ObjectCreated:CompleteMultipartUpload` (very important for large files)\n",
    "\n",
    "6. **IAM Role or Storage Integration**\n",
    "\n",
    "   * Grants Snowflake read access to your S3 bucket.\n",
    "\n",
    "So the flow is:\n",
    "\n",
    "```\n",
    "S3 → SNS → SQS → Snowflake (Pipe) → COPY INTO Table\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Where do you find the Snowpipe `notification_channel`? How is it used?\n",
    "\n",
    "Once your pipe is created, run:\n",
    "\n",
    "```sql\n",
    "DESC PIPE <database>.<schema>.<pipe_name>;\n",
    "```\n",
    "\n",
    "You’ll see a column named **NOTIFICATION_CHANNEL**.\n",
    "Example output:\n",
    "\n",
    "```\n",
    "+----------------------+------------------------------------------------------------+\n",
    "| property             | value                                                      |\n",
    "+----------------------+------------------------------------------------------------+\n",
    "| NOTIFICATION_CHANNEL | arn:aws:sqs:us-east-1:123456789012:snowflake_pipe_channel  |\n",
    "+----------------------+------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "✅ **How it’s used:**\n",
    "\n",
    "* You copy this ARN value into your AWS SQS policy.\n",
    "* It tells AWS which Snowflake account is allowed to receive S3 event notifications.\n",
    "* Without linking this ARN, Snowflake won’t be able to receive messages about new files.\n",
    "\n",
    "In other words, this channel is the **secure handshake** between your AWS bucket events and your Snowflake pipe.\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ What S3 event types must you include to ensure large multipart uploads are ingested?\n",
    "\n",
    "You must configure the following **S3 event types**:\n",
    "\n",
    "1. `s3:ObjectCreated:Put`\n",
    "   → triggers when a standard upload completes.\n",
    "\n",
    "2. `s3:ObjectCreated:CompleteMultipartUpload`\n",
    "   → triggers when large files (uploaded in multiple parts) are completed.\n",
    "\n",
    "⚠️ If you miss `CompleteMultipartUpload`, Snowpipe won’t detect large file uploads that use multipart upload (common with SDKs or Spark).\n",
    "This is one of the most frequent misconfigurations causing “Snowpipe not ingesting my files” problems.\n",
    "\n",
    "---\n",
    "\n",
    "## 6️⃣ How do you pause a pipe, and why might you do that?\n",
    "\n",
    "To **pause**:\n",
    "\n",
    "```sql\n",
    "ALTER PIPE <pipe_name> SET PIPE_EXECUTION_PAUSED = TRUE;\n",
    "```\n",
    "\n",
    "To **resume**:\n",
    "\n",
    "```sql\n",
    "ALTER PIPE <pipe_name> SET PIPE_EXECUTION_PAUSED = FALSE;\n",
    "```\n",
    "\n",
    "✅ **Why pause a pipe:**\n",
    "\n",
    "* You are troubleshooting an ingestion issue and don’t want more files to load.\n",
    "* You need to make schema or format changes (e.g., table structure change).\n",
    "* You want to stop ingestion temporarily for maintenance or cost control.\n",
    "\n",
    "When paused, Snowflake won’t process new notifications until you resume it.\n",
    "\n",
    "---\n",
    "\n",
    "## 7️⃣ How does Snowpipe billing work in the latest model? Where do you check billed bytes?\n",
    "\n",
    "### 🔹 Billing model\n",
    "\n",
    "Snowpipe uses **serverless compute**, so you’re billed:\n",
    "\n",
    "* Per **GB ingested** (fixed rate per GB, regardless of number of files)\n",
    "* Or for **serverless compute seconds** (older model, depending on your account edition)\n",
    "\n",
    "You **don’t pay for a warehouse**, since Snowflake manages ingestion internally.\n",
    "\n",
    "### 🔹 Where to check billed usage\n",
    "\n",
    "You can monitor Snowpipe usage and cost through:\n",
    "\n",
    "1. **ACCOUNT_USAGE.PIPE_USAGE_HISTORY**\n",
    "\n",
    "   ```sql\n",
    "   SELECT *\n",
    "   FROM SNOWFLAKE.ACCOUNT_USAGE.PIPE_USAGE_HISTORY\n",
    "   WHERE PIPE_NAME = '<pipe_name>'\n",
    "   ORDER BY USAGE_DATE DESC;\n",
    "   ```\n",
    "\n",
    "   → Shows bytes loaded, credits billed, and execution time.\n",
    "\n",
    "2. **Snowsight / Classic UI → Usage → Serverless Features → Snowpipe**\n",
    "   → Shows detailed billing by pipe.\n",
    "\n",
    "---\n",
    "\n",
    "## 8️⃣ When would you use the Snowpipe REST API `insertFiles` instead of auto-ingest?\n",
    "\n",
    "You use the **REST API (insertFiles)** when:\n",
    "\n",
    "* You **can’t configure AWS S3 event notifications**, e.g., due to organizational restrictions.\n",
    "* You want your **own application or Airflow DAG** to control ingestion (manually trigger it when a file is ready).\n",
    "* You have **custom naming or timing logic** (for example, only ingest files after validation).\n",
    "* You’re ingesting from **non-AWS sources** (like Azure Blob or GCP Storage without event notifications).\n",
    "\n",
    "With `insertFiles`, your app directly calls:\n",
    "\n",
    "```\n",
    "POST /v1/data/pipes/{pipeName}/insertFiles\n",
    "```\n",
    "\n",
    "and passes a list of file names.\n",
    "Snowflake then queues those files for ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "## 9️⃣ How do you diagnose a file that failed to load (what views and logs)?\n",
    "\n",
    "To troubleshoot ingestion issues:\n",
    "\n",
    "1. **PIPE_USAGE_HISTORY**\n",
    "\n",
    "   ```sql\n",
    "   SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.PIPE_USAGE_HISTORY\n",
    "   WHERE PIPE_NAME = '<pipe_name>' ORDER BY USAGE_DATE DESC;\n",
    "   ```\n",
    "\n",
    "   → Shows high-level pipe activity, success/failure counts.\n",
    "\n",
    "2. **COPY_HISTORY**\n",
    "\n",
    "   ```sql\n",
    "   SELECT * FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(\n",
    "       TABLE_NAME => '<table_name>',\n",
    "       START_TIME => DATEADD('hour', -1, CURRENT_TIMESTAMP())\n",
    "   ));\n",
    "   ```\n",
    "\n",
    "   → Shows details of every file loaded, including errors.\n",
    "\n",
    "3. **LOAD_HISTORY / PIPE_EXECUTION_HISTORY**\n",
    "   → Useful for identifying partially loaded or skipped files.\n",
    "\n",
    "4. **Snowsight UI → Databases → Pipes → [your pipe] → History tab**\n",
    "   → Graphical view of successes and failures with error details.\n",
    "\n",
    "Common errors you’ll see:\n",
    "\n",
    "* File format mismatch\n",
    "* Missing columns\n",
    "* Truncated values\n",
    "* Access denied (S3 permission issue)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔟 What best practices prevent duplicate/partial loads and unexpected costs?\n",
    "\n",
    "Here’s the golden checklist:\n",
    "\n",
    "| Category               | Best Practice                                                                                               |\n",
    "| ---------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| **File naming**        | Always use **unique filenames** (include timestamp or UUID). Never overwrite files.                         |\n",
    "| **Error handling**     | Use `ON_ERROR = 'CONTINUE'` or route bad rows to an error table.                                            |\n",
    "| **Event config**       | Include both `Put` and `CompleteMultipartUpload` events in S3 config.                                       |\n",
    "| **Permissions**        | Use **Storage Integrations** and IAM roles (avoid hard-coded keys).                                         |\n",
    "| **Monitoring**         | Query `PIPE_USAGE_HISTORY` weekly for cost trends.                                                          |\n",
    "| **Schema changes**     | Pause pipes before altering table schema.                                                                   |\n",
    "| **Batch optimization** | Compress small files (e.g., gzip or merge small batches) — too many tiny files increase ingestion overhead. |\n",
    "| **Retention**          | Use `COPY_HISTORY` to verify no duplicate ingestion occurred.                                               |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary Table (Quick Recall)**\n",
    "\n",
    "| Question | Key Concept                                                                 |\n",
    "| -------- | --------------------------------------------------------------------------- |\n",
    "| 1        | PIPE = object that automates COPY INTO & tracks files                       |\n",
    "| 2        | Snowpipe = serverless, event-driven; COPY = scheduled, warehouse-driven     |\n",
    "| 3        | Needs S3, SNS, SQS, notification_channel, IAM role                          |\n",
    "| 4        | `DESC PIPE` shows notification_channel → used in SQS policy                 |\n",
    "| 5        | Add `Put` + `CompleteMultipartUpload` events                                |\n",
    "| 6        | Pause via `ALTER PIPE SET PIPE_EXECUTION_PAUSED`                            |\n",
    "| 7        | Billed per GB serverless ingestion → check PIPE_USAGE_HISTORY               |\n",
    "| 8        | REST API `insertFiles` used when you can’t use auto-ingest                  |\n",
    "| 9        | Diagnose via COPY_HISTORY / PIPE_USAGE_HISTORY                              |\n",
    "| 10       | Unique filenames, correct events, monitor costs, pause before schema change |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3b5a3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1) How to check for any error in Snowpipe? (strategy / SQL)\n",
    "\n",
    "Snowpipe does **not** push an automatic Snowflake-level error notification to you when a file fails — you must *query Snowflake objects that record load/validation info* or build an alerting wrapper. Key built-in tools:\n",
    "\n",
    "**A. `VALIDATE_PIPE_LOAD` (table function)** — shows errors encountered by Snowpipe for a pipe in the last 14 days.\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM TABLE(VALIDATE_PIPE_LOAD(\n",
    "  PIPE_NAME => 'MYDB.MYSCHEMA.MY_PIPE',\n",
    "  START_TIME => DATEADD('day', -1, CURRENT_TIMESTAMP())\n",
    "));\n",
    "```\n",
    "\n",
    "Use this first — it returns file-level validation errors and useful metadata. ([Snowflake Documentation][1])\n",
    "\n",
    "**B. `INFORMATION_SCHEMA.COPY_HISTORY` (table function)** — shows COPY statements (including those executed by Snowpipe) and the first error per file (useful if a file produced an error during COPY).\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(\n",
    "  TABLE_NAME => 'TRANSACTIONS',\n",
    "  START_TIME => DATEADD('hour', -6, CURRENT_TIMESTAMP())\n",
    "))\n",
    "ORDER BY LAST_LOAD_TIME DESC;\n",
    "```\n",
    "\n",
    "This is the canonical place to read COPY error messages per file. ([Snowflake Documentation][2])\n",
    "\n",
    "**C. `PIPE_USAGE_HISTORY` / `SNOWFLAKE.ACCOUNT_USAGE.PIPE_USAGE_HISTORY`** — pipe activity and billed bytes (quick way to spot spikes/failures). The table function `PIPE_USAGE_HISTORY()` returns recent pipe activity (about 14 days) and the account usage view returns up to 365 days for billing/ops analysis. ([Snowflake Documentation][3])\n",
    "\n",
    "**D. `SYSTEM$PIPE_STATUS`** — returns a JSON with pipe health info: `executionState`, `lastIngestedTimestamp`, `numOutstandingMessagesOnChannel`, etc. Good to check whether the pipe is `RUNNING` / `PAUSED` or if messages are piling up in the channel.\n",
    "\n",
    "```sql\n",
    "SELECT SYSTEM$PIPE_STATUS('MYDB.MYSCHEMA.MY_PIPE');\n",
    "```\n",
    "\n",
    "Use this when investigating ingestion lag or pending messages. ([Snowflake Documentation][4])\n",
    "\n",
    "**E. Snowsight UI** — visual history of pipe loads and copy errors (quick for human triage). ([Stack Overflow][5])\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Strategy because “Snowpipe doesn't throw any error or notification”\n",
    "\n",
    "Correct — Snowpipe itself won’t email or push alerts to you automatically. **Operational strategy**:\n",
    "\n",
    "1. **Poll & validate automatically**\n",
    "\n",
    "   * Schedule a short Task (or external orchestration like Airflow) to run every 1–5 minutes:\n",
    "\n",
    "     * `SELECT SYSTEM$PIPE_STATUS(...)` — fail if `executionState != 'RUNNING'` or `numOutstandingMessagesOnChannel > threshold`.\n",
    "     * `SELECT * FROM TABLE(VALIDATE_PIPE_LOAD(...)) WHERE <error detected>` — if non-empty, raise alert.\n",
    "     * `SELECT * FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(...)) WHERE error is not null` — find earliest failing file(s).\n",
    "   * If any check fails, **send a notification** (Teams/Slack/email) using `SYSTEM$SEND_SNOWFLAKE_NOTIFICATION` or call an external webhook. (You can create an Alert object or a Task that calls `SYSTEM$SEND_SNOWFLAKE_NOTIFICATION`.) ([Medium][6])\n",
    "\n",
    "2. **Leverage SQS/SNS metadata**\n",
    "\n",
    "   * Put S3 event messages in an SQS queue with a **DLQ (dead-letter queue)**. Configure an external monitor that alerts when messages land in the DLQ (indicates delivery problem between AWS and Snowflake).\n",
    "\n",
    "3. **Instrument COPY behavior**\n",
    "\n",
    "   * In the `COPY INTO` inside the pipe, use `ON_ERROR = 'CONTINUE'` or `ON_ERROR = 'SKIP_FILE'` depending on your tolerance — but in any case, log failed filenames to an `error_table` via `COPY` options or use downstream `VALIDATE_PIPE_LOAD` to find them.\n",
    "\n",
    "4. **Store failure rows / auditing**\n",
    "\n",
    "   * Use a staging table where Snowpipe first loads raw rows, then run a `VALIDATION` pipeline (Stream+Task) that verifies data and writes bad rows to `staging_errors` with full context. This is the best pattern for production.\n",
    "\n",
    "> TL;DR: **don’t rely on implicit Snowpipe alerts** — poll VALIDATE_PIPE_LOAD, COPY_HISTORY, and SYSTEM$PIPE_STATUS and wire those checks into your alerting system.\n",
    "\n",
    "References: VALIDATE_PIPE_LOAD docs and Troubleshooting guide. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "## 3) “You can't alter the COPY command under same pipe object! You have to recreate the pipe object” — correctness & safe recipe\n",
    "\n",
    "**Fact:** You **cannot** change the `COPY INTO <table>` statement of a pipe using `ALTER PIPE`. The Snowflake docs explicitly list that the `COPY INTO` clause cannot be modified via `ALTER PIPE`. To change the COPY definition you must `CREATE OR REPLACE PIPE` (or `DROP` and `CREATE`). ([Snowflake Documentation][7])\n",
    "\n",
    "**Safe procedure to change COPY logic with minimal risk** (recommended runbook):\n",
    "\n",
    "1. `ALTER PIPE mypipe SET PIPE_EXECUTION_PAUSED = TRUE;`\n",
    "2. Confirm pending queue is empty:\n",
    "\n",
    "   ```sql\n",
    "   SELECT parse_json(SYSTEM$PIPE_STATUS('MYDB.MYSCHEMA.MY_PIPE')):numOutstandingMessagesOnChannel;\n",
    "   ```\n",
    "\n",
    "   Wait until pending = 0 (or use a small wait + check loop). ([Snowflake Documentation][4])\n",
    "3. `CREATE OR REPLACE PIPE mypipe AS COPY INTO ...`  ← new definition (this internally recreates the pipe).\n",
    "4. Optionally run `ALTER PIPE mypipe REFRESH;` to push recent staged files into the pipe ingest queue (see next section for REFRESH semantics).\n",
    "5. `ALTER PIPE mypipe SET PIPE_EXECUTION_PAUSED = FALSE;`\n",
    "6. Validate with `SYSTEM$PIPE_STATUS` and `PIPE_USAGE_HISTORY` / `VALIDATE_PIPE_LOAD`.\n",
    "\n",
    "**Why you must pause first?** Because otherwise in-flight notifications can be processed against the old/new definitions leading to unpredictable behavior and possible duplicates.\n",
    "\n",
    "References/notes: docs explain `COPY` cannot be altered with `ALTER PIPE` and suggest pausing/checking status before recreating. ([Snowflake Documentation][7])\n",
    "\n",
    "---\n",
    "\n",
    "## 4) If you CREATE OR REPLACE a pipe, do you need to refresh pipe metadata? How to refresh?\n",
    "\n",
    "**Yes — two relevant facts:**\n",
    "\n",
    "* Snowpipe tracks a *pipe-level internal file-load metadata* that prevents reloading the same files (this metadata is associated with the pipe object). That metadata remains tied to the specific pipe object. If you recreate a pipe, the new pipe gets its own metadata/history. You may need to explicitly `REFRESH` the pipe to have Snowpipe (re)consider files that are already staged. ([Snowflake Documentation][8])\n",
    "\n",
    "* **`ALTER PIPE ... REFRESH`** exists and copies files staged **within the previous 7 days** into the Snowpipe ingest queue for loading (this is the supported refresh mechanism). Use `ALTER PIPE mypipe REFRESH;` to make Snowpipe attempt to load recent staged files (subject to the pipe’s file tracking semantics and age limits). The REFRESH function is intended for short-term recovery or to pull in recent historical files — it is not for regular loads of very old archives. ([Snowflake Documentation][7])\n",
    "\n",
    "Example:\n",
    "\n",
    "```sql\n",
    "-- after recreating pipe\n",
    "ALTER PIPE mydb.myschema.mypipe REFRESH;\n",
    "SELECT SYSTEM$PIPE_STATUS('MYDB.MYSCHEMA.MY_PIPE');\n",
    "```\n",
    "\n",
    "**Important caveats:**\n",
    "\n",
    "* The REFRESH command copies staged files from **last 7 days** by default (older files need manual `COPY INTO` or REST API to re-ingest). ([Stack Overflow][9])\n",
    "* Recreating a pipe generates a *new* internal load-history for the pipe. If you need to preserve the prior pipe’s history or avoid duplicates, capture timestamps or file lists before dropping the pipe and design a safe refresh window. Many teams record the last-processed file timestamp before replacement, and then `REFRESH` only files after that timestamp. Community threads discuss this pattern. ([Stack Overflow][10])\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Is it always good practice to have 1 pipe object for one S3 bucket? (notification channel behavior)\n",
    "\n",
    "**Reality & recommendation:**\n",
    "\n",
    "* **Notification channel behavior:** if you create multiple pipes that target the same external bucket (or the same account) Snowflake often shows the **same `notification_channel`** (SQS) value for those pipes — because the cloud messaging channel (SNS/SQS integration) is tied to the **account/region** and bucket mapping. So yes, multiple pipes can end up sharing the same channel ARN. That’s expected. ([Cloudyard][11])\n",
    "\n",
    "* **But** sharing a notification channel **does not mean** pipes won’t compete or accidentally load the same files. If multiple pipes point to overlapping paths (or use the same stage/prefix), you can accidentally have the same file eligible to be loaded by more than one pipe (and lead to duplicate rows). The basic rule: **file load metadata is tracked per-pipe**, so two different pipes do not share load history — they can both load the same file unless you design separation.\n",
    "\n",
    "**Best practice (recommended):**\n",
    "\n",
    "1. **One ingest pipeline per logical target**: create one pipe per logical source→table line (e.g., `bucket/prefix/payments/` → `payments_table`). This minimizes coordination and dedup complexity.\n",
    "2. **Use distinct stages or prefix path** for different pipes: `@stage/payments/`, `@stage/customers/` — this prevents overlap.\n",
    "3. If you must have multiple pipes for the same bucket, **explicitly use `PATTERN` or `PREFIX`** in the pipe definition to guarantee disjoint file sets.\n",
    "4. **Avoid relying on shared notification channel semantics** to isolate loads — treat notification channel as transport only and separate at the pipe definition level. ([Cloudyard][11])\n",
    "\n",
    "Short summary: **notification channel may be the same**, but best practice is *one pipe per logical prefix/table* (or use patterns) to avoid accidental duplicate ingestion and to simplify operational ownership.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Truncate scenario — exact behavior\n",
    "\n",
    "> Scenario: You uploaded `file1.csv` in S3; Snowpipe ingested it into `table T`. Then you `TRUNCATE TABLE T`. Next you upload `file2.csv`. Will Snowpipe reload `file1.csv` (again) or only `file2.csv`?\n",
    "\n",
    "**Answer (explicit):** Snowpipe **decides whether to load files based on the pipe’s internal file-load metadata**, not the current contents of the target table. Because `file1.csv` was already processed by that pipe, Snowpipe will **not** attempt to re-ingest `file1.csv` simply because the table was truncated. Only `file2.csv` (new file) will be loaded automatically.\n",
    "\n",
    "**If you need to reload `file1.csv` after truncating**, your options are:\n",
    "\n",
    "* Run a manual `COPY INTO <table> ... FILES = ('file1.csv') FORCE = TRUE;` — this will explicitly re-load duplicates. (`FORCE = TRUE` forces reloading the files even if previously loaded). ([Snowflake Documentation][2])\n",
    "* Or delete the pipe and recreate it (new pipe has no load history) and then `ALTER PIPE ... REFRESH` (or rely on notifications) — but recreate means you must be careful not to accidentally duplicate *other* files; this approach is more disruptive and not recommended solely to reprocess files. ([Stack Overflow][10])\n",
    "* Or upload the file with a **new unique filename** (recommended operational pattern) — Snowpipe sees it as new and will ingest it. (Use timestamp/UUID in file name.)\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "* Truncating the table does **not** reset the pipe’s loaded-file tracking.\n",
    "* To re-ingest an already-loaded file you must force the COPY or change file identity (filename) or recreate the pipe (with side-effects). ([Snowflake Documentation][8])\n",
    "\n",
    "---\n",
    "\n",
    "## 7) “Snowpipe copies data to the table based on the name of the files but only copy command do that based on the hash value of the files” — explanation & scenarios\n",
    "\n",
    "There’s a subtle but important distinction here — let’s break it down precisely.\n",
    "\n",
    "**How Snowpipe prevents duplicates (what it *tracks*)**\n",
    "\n",
    "* **Snowpipe’s internal metadata** (associated **with the pipe**) stores the *path + filename* of files that have been loaded by that pipe, and prevents re-loading files with the **same name**. The docs explicitly state Snowpipe prevents loading files with the same **name** even if their eTag/metadata changed. In other words: **file identity for Snowpipe = file path + name** (not checksum). That’s why changing file contents but keeping the same filename typically will *not* trigger re-ingestion by Snowpipe. ([Snowflake Documentation][8])\n",
    "\n",
    "**How `COPY INTO` (manual bulk load) handles duplicates**\n",
    "\n",
    "* `COPY INTO <table>` (manual/bulk copy) uses a different set of metadata: bulk loads store load history with the **target table** for some retention period (docs mention bulk load history stored in table metadata for 64 days). Bulk `COPY` supports `FORCE = TRUE` to explicitly force reloading even if file checksums match prior loads. COPY can use checksum/hash behavior or internal tracking to determine if file contents were already loaded by the same `COPY` job. ([Snowflake Documentation][8])\n",
    "\n",
    "**Scenarios — what happens and why**\n",
    "\n",
    "1. **Scenario A — Snowpipe + same filename overwritten in S3**\n",
    "\n",
    "   * Upload `data.csv` → Snowpipe loads it. Later overwrite `data.csv` in S3 with a different payload but same filename. Snowpipe will **not** re-load it (because load history for that pipe records the filename). If you need re-load, you must use a manual `COPY` with `FORCE=TRUE`, change the filename, or recreate the pipe (not recommended). ([Snowflake Documentation][8])\n",
    "\n",
    "2. **Scenario B — Bulk COPY run twice with same files and `FORCE=FALSE`**\n",
    "\n",
    "   * The second `COPY` may skip files if table metadata indicates those files were already loaded; `FORCE=TRUE` will re-load them. (COPY has options to override.) ([Snowflake Documentation][2])\n",
    "\n",
    "3. **Scenario C — Multiple pipes point to same bucket and same file**\n",
    "\n",
    "   * Because load metadata is **per-pipe**, each pipe might load the same file independently (duplicates). Avoid this by segregating prefixes, using PATTERNs, or designing only one pipe to handle a prefix. ([Cloudyard][11])\n",
    "\n",
    "4. **Scenario D — Reprocess historic files**\n",
    "\n",
    "   * Use `ALTER PIPE ... REFRESH` (last 7 days) or `COPY INTO` manually (with `FORCE=TRUE`) or REST API `insertFiles` to explicitly trigger ingestion of older files. ([Snowflake Documentation][7])\n",
    "\n",
    "**Rule of thumb:**\n",
    "\n",
    "* **Snowpipe = filename-based, per-pipe tracking (short retention, ~14 days)**.\n",
    "* **Bulk COPY = table-level tracking + COPY options (FORCE, PURGE, VALIDATION_MODE)** and can be used to force re-ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical runbook — common tasks & SQL snippets\n",
    "\n",
    "**Check pipe state + pending messages**\n",
    "\n",
    "```sql\n",
    "SELECT parse_json(SYSTEM$PIPE_STATUS('MYDB.MYSCHEMA.MY_PIPE')) AS pipe_status;\n",
    "```\n",
    "\n",
    "**Find validation errors (last 24 hours)**\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM TABLE(VALIDATE_PIPE_LOAD(\n",
    "  PIPE_NAME=>'MYDB.MYSCHEMA.MY_PIPE',\n",
    "  START_TIME=>DATEADD('day', -1, CURRENT_TIMESTAMP())\n",
    "));\n",
    "```\n",
    "\n",
    "**Inspect COPY history for table in last 6 hours**\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(\n",
    "  TABLE_NAME=>'TRANSACTIONS',\n",
    "  START_TIME=>DATEADD('hour', -6, CURRENT_TIMESTAMP())\n",
    "));\n",
    "```\n",
    "\n",
    "**Pause pipe safely**\n",
    "\n",
    "```sql\n",
    "ALTER PIPE MYDB.MYSCHEMA.MY_PIPE SET PIPE_EXECUTION_PAUSED = TRUE;\n",
    "-- wait until SYSTEM$PIPE_STATUS shows pending messages = 0\n",
    "```\n",
    "\n",
    "**Recreate pipe safely (change COPY)**\n",
    "\n",
    "```sql\n",
    "-- 1) pause\n",
    "ALTER PIPE mypipe SET PIPE_EXECUTION_PAUSED = TRUE;\n",
    "\n",
    "-- 2) verify pending = 0\n",
    "SELECT parse_json(SYSTEM$PIPE_STATUS('MYDB.MYSCHEMA.MY_PIPE')):numOutstandingMessagesOnChannel;\n",
    "\n",
    "-- 3) create or replace (new COPY)\n",
    "CREATE OR REPLACE PIPE mypipe AUTO_INGEST = TRUE AS\n",
    "  COPY INTO ... ;\n",
    "\n",
    "-- 4) optionally refresh\n",
    "ALTER PIPE mypipe REFRESH;\n",
    "\n",
    "-- 5) resume\n",
    "ALTER PIPE mypipe SET PIPE_EXECUTION_PAUSED = FALSE;\n",
    "```\n",
    "\n",
    "**Force re-load a file explicitly**\n",
    "\n",
    "```sql\n",
    "COPY INTO MYDB.MYSCHEMA.TRANSACTIONS\n",
    "  FROM @my_stage/path/\n",
    "  FILES = ('file1.csv')\n",
    "  FORCE = TRUE;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Short list of gotchas and final recommendations\n",
    "\n",
    "* **Retention windows differ**: Snowpipe file metadata ≈ **14 days**; bulk load history stored in table metadata is longer (docs mention 64 days). Plan reprocessing strategies accordingly. ([Snowflake Documentation][8])\n",
    "* **Avoid overwriting files** with the same name when you expect Snowpipe to re-ingest the changed content. Use unique names (timestamp/UUID). ([Stack Overflow][12])\n",
    "* **Pause→Check pending→Recreate→Refresh→Resume** is the safe way to change pipe definitions. ([Stack Overflow][5])\n",
    "* **Use VALIDATE_PIPE_LOAD and COPY_HISTORY** in automation to surface file-level errors; build alerts off these. ([Snowflake Documentation][1])\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "[1]: https://docs.snowflake.com/en/sql-reference/functions/validate_pipe_load?utm_source=chatgpt.com \"VALIDATE_PIPE_LOAD\"\n",
    "[2]: https://docs.snowflake.com/en/sql-reference/sql/copy-into-table?utm_source=chatgpt.com \"COPY INTO <table>\"\n",
    "[3]: https://docs.snowflake.com/en/sql-reference/functions/pipe_usage_history?utm_source=chatgpt.com \"PIPE_USAGE_HISTORY\"\n",
    "[4]: https://docs.snowflake.com/en/sql-reference/functions/system_pipe_status?utm_source=chatgpt.com \"SYSTEM$PIPE_STATUS\"\n",
    "[5]: https://stackoverflow.com/questions/60728379/how-to-move-or-alter-a-pipe-without-missing-or-duplicating-any-records?utm_source=chatgpt.com \"How to Move or Alter a Pipe without missing or duplicating ...\"\n",
    "[6]: https://medium.com/snowflake/introduction-to-snowflakes-data-pipeline-alerts-notifications-9beac8d127cc?utm_source=chatgpt.com \"Introduction to Snowflake's data pipeline alerts & notifications\"\n",
    "[7]: https://docs.snowflake.com/en/sql-reference/sql/alter-pipe?utm_source=chatgpt.com \"ALTER PIPE\"\n",
    "[8]: https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro?utm_source=chatgpt.com \"Snowpipe\"\n",
    "[9]: https://stackoverflow.com/questions/72675419/why-snow-pipe-is-not-loading-all-the-data?utm_source=chatgpt.com \"Why snow pipe is not loading all the data\"\n",
    "[10]: https://stackoverflow.com/questions/69166544/preserving-the-load-history-when-re-creating-pipes-in-snowflake?utm_source=chatgpt.com \"Preserving the load history when re-creating pipes in ...\"\n",
    "[11]: https://cloudyard.in/2022/06/snowpipe-load-multiple-tables-with-same-bucket/?utm_source=chatgpt.com \"Snowpipe: Load multiple tables with same bucket\"\n",
    "[12]: https://stackoverflow.com/questions/76660028/how-much-time-snowpipe-keeps-track-of-files-that-has-being-already-loaded?utm_source=chatgpt.com \"How much time SnowPipe keeps track of files that has ...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80feec91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
