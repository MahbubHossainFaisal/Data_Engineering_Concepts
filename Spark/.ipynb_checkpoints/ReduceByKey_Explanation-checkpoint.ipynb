{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7862a",
   "metadata": {},
   "source": [
    "### An example of reducebykey step by step to understand how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9bfc1",
   "metadata": {},
   "source": [
    "### **Scenario:**\n",
    "You have sales data for different regions, and you want to calculate the **total sales for each region**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Input Data**\n",
    "Here’s the input dataset (key-value pairs):\n",
    "```\n",
    "data = [\n",
    "    (\"North\", 100),\n",
    "    (\"South\", 200),\n",
    "    (\"North\", 150),\n",
    "    (\"East\", 50),\n",
    "    (\"South\", 300),\n",
    "    (\"East\", 100),\n",
    "    (\"North\", 50)\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Execution of `reduceByKey`**\n",
    "\n",
    "#### 1. **Create an RDD**\n",
    "First, we create an RDD from the input data:\n",
    "```python\n",
    "rdd = sc.parallelize(data)\n",
    "```\n",
    "\n",
    "At this stage, the RDD contains:\n",
    "```\n",
    "[(\"North\", 100), (\"South\", 200), (\"North\", 150), (\"East\", 50), (\"South\", 300), (\"East\", 100), (\"North\", 50)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Group Data by Key**\n",
    "When you call `reduceByKey`, Spark groups all the values with the same key together **across partitions**.\n",
    "\n",
    "```python\n",
    "grouped_data = {\n",
    "    \"North\": [100, 150, 50],\n",
    "    \"South\": [200, 300],\n",
    "    \"East\": [50, 100]\n",
    "}\n",
    "```\n",
    "\n",
    "This grouping happens in a distributed manner across Spark partitions.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Apply the Reduction Function**\n",
    "Next, the **reduction function** is applied to each group of values. In this example, the function is:\n",
    "```python\n",
    "lambda x, y: x + y\n",
    "```\n",
    "\n",
    "This function takes two values at a time and combines them. Here’s how it works for each key:\n",
    "\n",
    "- **For \"North\"**:\n",
    "  ```\n",
    "  Step 1: 100 + 150 = 250\n",
    "  Step 2: 250 + 50 = 300\n",
    "  Result: (\"North\", 300)\n",
    "  ```\n",
    "\n",
    "- **For \"South\"**:\n",
    "  ```\n",
    "  Step 1: 200 + 300 = 500\n",
    "  Result: (\"South\", 500)\n",
    "  ```\n",
    "\n",
    "- **For \"East\"**:\n",
    "  ```\n",
    "  Step 1: 50 + 100 = 150\n",
    "  Result: (\"East\", 150)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Create a New RDD**\n",
    "After applying the reduction function, a new RDD is created with the results:\n",
    "```\n",
    "[(\"North\", 300), (\"South\", 500), (\"East\", 150)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Complete Code Example**\n",
    "Here’s the complete Spark code to compute total sales by region:\n",
    "```python\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Spark Configuration\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ReduceByKeyExample\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    (\"North\", 100),\n",
    "    (\"South\", 200),\n",
    "    (\"North\", 150),\n",
    "    (\"East\", 50),\n",
    "    (\"South\", 300),\n",
    "    (\"East\", 100),\n",
    "    (\"North\", 50)\n",
    "]\n",
    "\n",
    "# Step 1: Create an RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Step 2: Apply reduceByKey to calculate total sales per region\n",
    "result_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Step 3: Collect the results to view them\n",
    "results = result_rdd.collect()\n",
    "\n",
    "# Step 4: Print the results\n",
    "for region, total_sales in results:\n",
    "    print(f\"{region}: {total_sales}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "```\n",
    "North: 300\n",
    "South: 500\n",
    "East: 150\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualization of Steps**\n",
    "\n",
    "1. **Input Data**:\n",
    "   ```\n",
    "   (\"North\", 100), (\"South\", 200), (\"North\", 150), (\"East\", 50), (\"South\", 300), (\"East\", 100), (\"North\", 50)\n",
    "   ```\n",
    "\n",
    "2. **Grouped by Key**:\n",
    "   ```\n",
    "   \"North\": [100, 150, 50]\n",
    "   \"South\": [200, 300]\n",
    "   \"East\": [50, 100]\n",
    "   ```\n",
    "\n",
    "3. **Reduced by Key**:\n",
    "   ```\n",
    "   \"North\": 300\n",
    "   \"South\": 500\n",
    "   \"East\": 150\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "1. **Grouping Happens First**: All values for the same key are grouped.\n",
    "2. **Reduction Happens in Steps**: The function is applied iteratively to combine values.\n",
    "3. **Distributed Processing**: The grouping and reduction are distributed across Spark partitions for scalability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a889352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
