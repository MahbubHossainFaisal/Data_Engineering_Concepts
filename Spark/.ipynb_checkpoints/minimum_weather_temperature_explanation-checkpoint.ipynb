{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Setting up Spark configuration and context\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MinWeatherTemperature\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Function to parse each line of the dataset\n",
    "def parseLine(line):\n",
    "    fields = line.split(\",\")  # Assuming the file is CSV\n",
    "    station_id = fields[0]\n",
    "    entry_type = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9 / 5) + 32  # Convert to Fahrenheit\n",
    "    return (station_id, entry_type, temperature)\n",
    "\n",
    "# Reading the file\n",
    "lines = sc.textFile(\"file:///spark/weather1800.csv\")\n",
    "\n",
    "# Parsing lines into structured tuples\n",
    "parsed_lines = lines.map(parseLine)\n",
    "\n",
    "# Filtering for minimum temperature entries (TMIN)\n",
    "tmin_records = parsed_lines.filter(lambda x: x[1] == \"TMIN\")\n",
    "\n",
    "# Mapping station ID to temperature\n",
    "station_temps = tmin_records.map(lambda x: (x[0], x[2]))\n",
    "\n",
    "# Reducing to find the minimum temperature for each station\n",
    "min_temps = station_temps.reduceByKey(lambda x, y: min(x, y))\n",
    "\n",
    "# Collecting and printing the results\n",
    "results = min_temps.collect()\n",
    "\n",
    "for result in results:\n",
    "    print(f'{result[0]} -> {result[1]:.2f} Fahrenheit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f89dc79",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Sample Dataset (`weather1800.csv`)\n",
    "```\n",
    "ITE00100554,18000101,TMAX,-75\n",
    "ITE00100554,18000101,TMIN,-148\n",
    "ITE00100554,18000102,TMAX,-70\n",
    "ITE00100554,18000102,TMIN,-125\n",
    "ITE00100554,18000103,TMIN,-160\n",
    "EZE00100082,18000101,TMAX,-44\n",
    "EZE00100082,18000101,TMIN,-78\n",
    "EZE00100082,18000102,TMAX,-33\n",
    "EZE00100082,18000102,TMIN,-64\n",
    "EZE00100082,18000103,TMIN,-90\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "#### **Step 1: Parsing the Data**\n",
    "\n",
    "Using the `parseLine` function, each line is converted into a tuple:\n",
    "```python\n",
    "def parseLine(line):\n",
    "    fields = line.split(\",\")\n",
    "    station_id = fields[0]\n",
    "    entry_type = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9 / 5) + 32  # Celsius to Fahrenheit\n",
    "    return (station_id, entry_type, temperature)\n",
    "```\n",
    "\n",
    "The parsed data looks like this:\n",
    "```\n",
    "[\n",
    " ('ITE00100554', 'TMAX', 5.0),\n",
    " ('ITE00100554', 'TMIN', -9.4),\n",
    " ('ITE00100554', 'TMAX', 5.0),\n",
    " ('ITE00100554', 'TMIN', -13.0),\n",
    " ('ITE00100554', 'TMIN', -20.8),\n",
    " ('EZE00100082', 'TMAX', 24.8),\n",
    " ('EZE00100082', 'TMIN', -10.4),\n",
    " ('EZE00100082', 'TMAX', 27.4),\n",
    " ('EZE00100082', 'TMIN', -11.2),\n",
    " ('EZE00100082', 'TMIN', -22.0)\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Filtering for `TMIN` Records**\n",
    "\n",
    "The `filter` operation keeps only records where the `entry_type` is `\"TMIN\"`:\n",
    "```python\n",
    "tmin_records = parsed_lines.filter(lambda x: x[1] == \"TMIN\")\n",
    "```\n",
    "\n",
    "Filtered data:\n",
    "```\n",
    "[\n",
    " ('ITE00100554', 'TMIN', -9.4),\n",
    " ('ITE00100554', 'TMIN', -13.0),\n",
    " ('ITE00100554', 'TMIN', -20.8),\n",
    " ('EZE00100082', 'TMIN', -10.4),\n",
    " ('EZE00100082', 'TMIN', -11.2),\n",
    " ('EZE00100082', 'TMIN', -22.0)\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Mapping Station ID to Temperature**\n",
    "\n",
    "The `map` operation transforms the data into key-value pairs of `(station_id, temperature)`:\n",
    "```python\n",
    "station_temps = tmin_records.map(lambda x: (x[0], x[2]))\n",
    "```\n",
    "\n",
    "Mapped data:\n",
    "```\n",
    "[\n",
    " ('ITE00100554', -9.4),\n",
    " ('ITE00100554', -13.0),\n",
    " ('ITE00100554', -20.8),\n",
    " ('EZE00100082', -10.4),\n",
    " ('EZE00100082', -11.2),\n",
    " ('EZE00100082', -22.0)\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Finding the Minimum Temperature**\n",
    "\n",
    "The `reduceByKey` operation groups records by `station_id` and reduces the values for each key using the `min` function:\n",
    "```python\n",
    "min_temps = station_temps.reduceByKey(lambda x, y: min(x, y))\n",
    "```\n",
    "\n",
    "**How `reduceByKey` works:**\n",
    "\n",
    "1. **Grouping by Key**:\n",
    "   - Records are grouped by `station_id`:\n",
    "     ```\n",
    "     ITE00100554: [-9.4, -13.0, -20.8]\n",
    "     EZE00100082: [-10.4, -11.2, -22.0]\n",
    "     ```\n",
    "\n",
    "2. **Applying the `min` Function**:\n",
    "   - For `ITE00100554`:\n",
    "     ```\n",
    "     min(-9.4, -13.0) = -13.0\n",
    "     min(-13.0, -20.8) = -20.8\n",
    "     Final: -20.8\n",
    "     ```\n",
    "   - For `EZE00100082`:\n",
    "     ```\n",
    "     min(-10.4, -11.2) = -11.2\n",
    "     min(-11.2, -22.0) = -22.0\n",
    "     Final: -22.0\n",
    "     ```\n",
    "\n",
    "Reduced data:\n",
    "```\n",
    "[\n",
    " ('ITE00100554', -20.8),\n",
    " ('EZE00100082', -22.0)\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 5: Collecting and Printing Results**\n",
    "\n",
    "The `collect` method retrieves the final results:\n",
    "```python\n",
    "results = min_temps.collect()\n",
    "```\n",
    "\n",
    "Results:\n",
    "```\n",
    "[\n",
    " ('ITE00100554', -20.8),\n",
    " ('EZE00100082', -22.0)\n",
    "]\n",
    "```\n",
    "\n",
    "Each result is printed:\n",
    "```\n",
    "ITE00100554 -> -20.80 Fahrenheit\n",
    "EZE00100082 -> -22.00 Fahrenheit\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Transformations\n",
    "\n",
    "| Step                | Operation           | Input Data                          | Output Data                                 |\n",
    "|---------------------|---------------------|-------------------------------------|--------------------------------------------|\n",
    "| Parse Lines         | `map(parseLine)`   | Raw CSV lines                       | Structured tuples                          |\n",
    "| Filter `TMIN`       | `filter`           | All temperature records             | Only `TMIN` records                        |\n",
    "| Map Station to Temp | `map`              | `TMIN` tuples                       | Key-value pairs `(station_id, temperature)`|\n",
    "| Reduce by Key       | `reduceByKey(min)` | Key-value pairs                     | Minimum temperature per station            |\n",
    "| Collect Results     | `collect`          | Reduced data                        | Final results in driver program            |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts Explained\n",
    "\n",
    "1. **`map`**:\n",
    "   - Transforms each element of the RDD. For example, parsing a line into a structured tuple.\n",
    "\n",
    "2. **`filter`**:\n",
    "   - Retains only elements that satisfy a condition. For example, filtering for `TMIN` records.\n",
    "\n",
    "3. **`reduceByKey`**:\n",
    "   - Groups data by key and reduces the values using a specified function. For example, finding the minimum temperature for each station.\n",
    "\n",
    "4. **`collect`**:\n",
    "   - Retrieves the final results to the driver program.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d9094",
   "metadata": {},
   "source": [
    "### If we don't use min_temp.collect(). what would happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d2af9",
   "metadata": {},
   "source": [
    "If we do not use `min_temp.collect()`, the results of the `reduceByKey` operation will remain as an **RDD** (Resilient Distributed Dataset) in the Spark cluster and will not be retrieved to the driver program for further use or display. Here's what would happen:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Data Stays in the Cluster**\n",
    "   - Without `collect()`, the data is not transferred from the Spark executors to the driver program.\n",
    "   - The RDD `min_temp` will exist only as a distributed dataset in the cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **No Output on the Driver Console**\n",
    "   - The `for` loop that tries to iterate over `results` will fail because `results` won't exist (since `collect()` wasn't called).\n",
    "   - This will result in an **error**, as the driver program cannot access the distributed data directly.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **How Spark Works**\n",
    "   - Spark transformations like `reduceByKey` are **lazy**, meaning they don't execute until an **action** is called.\n",
    "   - `collect()` is an action that triggers the execution of all preceding transformations and brings the data back to the driver program.\n",
    "   - Without `collect()`, the transformations remain unexecuted, and the program cannot proceed with further steps (like printing results).\n",
    "\n",
    "---\n",
    "\n",
    "### Example Without `collect()`\n",
    "\n",
    "```python\n",
    "min_temp = weather_temp.reduceByKey(lambda x, y: min(x, y))\n",
    "for result in min_temp:\n",
    "    print(result)  # This will fail\n",
    "```\n",
    "\n",
    "#### Error Message:\n",
    "```\n",
    "TypeError: 'PipelinedRDD' object is not iterable\n",
    "```\n",
    "\n",
    "This happens because `min_temp` is an RDD, and RDDs cannot be directly iterated over in the driver program.\n",
    "\n",
    "---\n",
    "\n",
    "### Correct Way: Use `collect()`\n",
    "\n",
    "To bring the RDD data to the driver program, use:\n",
    "```python\n",
    "results = min_temp.collect()\n",
    "for result in results:\n",
    "    print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Alternative Actions to `collect()`\n",
    "If you don't want to use `collect()`, you can use other Spark actions like:\n",
    "1. **`take(n)`**:\n",
    "   - Retrieves the first `n` elements from the RDD.\n",
    "   - Example:\n",
    "     ```python\n",
    "     results = min_temp.take(5)\n",
    "     print(results)\n",
    "     ```\n",
    "\n",
    "2. **`saveAsTextFile(path)`**:\n",
    "   - Saves the RDD data to a file instead of bringing it to the driver.\n",
    "   - Example:\n",
    "     ```python\n",
    "     min_temp.saveAsTextFile(\"output/min_temp_results\")\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "Without `collect()` or another action, the RDD remains in the cluster, and no actual computation or data retrieval will occur on the driver side. To use the data locally (e.g., for printing), you must trigger an action like `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ed80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
