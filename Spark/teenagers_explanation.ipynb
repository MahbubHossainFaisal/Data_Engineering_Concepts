{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import collections\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"teenagers\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "def mapper(line):\n",
    "    data = line.split(',')\n",
    "    id = int(data[0])\n",
    "    name = str(data[1].encode('utf-8'))\n",
    "    age = int(data[2])\n",
    "    numFriends = int(data[3])\n",
    "\n",
    "    return Row(id,name,age,numFriends)\n",
    "    \n",
    "\n",
    "lines = spark.SparkContext.textFile(\"fakeFriends.csv\")\n",
    "\n",
    "people = lines.map(mapper)\n",
    "\n",
    "peopleSchema = people.createDataframe('people').cache()\n",
    "peopleSchema.createOrReplaceTempView()\n",
    "\n",
    "teenagers = spark.sql(\"select * from people where age> 10 and age < 21\");\n",
    "\n",
    "for teen in teenagers.collect():\n",
    "    print(teen)\n",
    "\n",
    "\n",
    "peopleSchema.groupBy(\"age\").count().orderBy(\"age\").show()\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fdca4a",
   "metadata": {},
   "source": [
    "\n",
    "### **Step-by-Step Explanation**\n",
    "\n",
    "#### 1. **Importing Required Libraries**\n",
    "```python\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import collections\n",
    "```\n",
    "- **`SparkSession`**: Entry point to using Spark SQL. It allows creating DataFrames and executing SQL queries.\n",
    "- **`Row`**: Used to create a row-like structure for DataFrame creation.\n",
    "- **`collections`**: Standard Python library (not used in this code).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Creating a Spark Session**\n",
    "```python\n",
    "spark = SparkSession.builder.appName(\"teenagers\").getOrCreate()\n",
    "```\n",
    "- A Spark session is created with the application name `\"teenagers\"`. This initializes the Spark engine.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Defining the Mapper Function**\n",
    "```python\n",
    "def mapper(line):\n",
    "    data = line.split(',')\n",
    "    id = int(data[0])\n",
    "    name = str(data[1].encode('utf-8'))\n",
    "    age = int(data[2])\n",
    "    numFriends = int(data[3])\n",
    "\n",
    "    return Row(id, name, age, numFriends)\n",
    "```\n",
    "- **Purpose**: Converts a line from the CSV file into a structured `Row` object.\n",
    "- **How it works**:\n",
    "  1. Splits the line using `,` to separate columns.\n",
    "  2. Extracts the `id`, `name`, `age`, and `numFriends` fields, converting them to appropriate types.\n",
    "  3. Returns a `Row` object with these fields.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Loading the CSV File**\n",
    "```python\n",
    "lines = spark.SparkContext.textFile(\"fakeFriends.csv\")\n",
    "```\n",
    "- **`textFile`**: Loads the `fakeFriends.csv` file as an RDD (Resilient Distributed Dataset), where each line is a string.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Mapping Lines to Rows**\n",
    "```python\n",
    "people = lines.map(mapper)\n",
    "```\n",
    "- Applies the `mapper` function to each line of the RDD, converting it into an RDD of `Row` objects.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Creating a DataFrame**\n",
    "```python\n",
    "peopleSchema = people.createDataframe('people').cache()\n",
    "```\n",
    "- **`createDataFrame`**: Converts the RDD of `Row` objects into a DataFrame named `peopleSchema`.\n",
    "- **`cache`**: Caches the DataFrame in memory for faster access during subsequent operations.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Creating a Temporary SQL View**\n",
    "```python\n",
    "peopleSchema.createOrReplaceTempView()\n",
    "```\n",
    "- Registers the DataFrame as a temporary SQL table (`people`) to run SQL queries on it.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **Finding Teenagers**\n",
    "```python\n",
    "teenagers = spark.sql(\"select * from people where age> 10 and age < 21\")\n",
    "```\n",
    "- Executes an SQL query to find all rows where `age` is between 11 and 20 (teenagers).\n",
    "\n",
    "---\n",
    "\n",
    "#### 9. **Printing Teenagers**\n",
    "```python\n",
    "for teen in teenagers.collect():\n",
    "    print(teen)\n",
    "```\n",
    "- **`collect`**: Brings the results from the cluster to the driver program as a Python list.\n",
    "- **`print`**: Prints each teenagerâ€™s details.\n",
    "\n",
    "---\n",
    "\n",
    "#### 10. **Grouping by Age**\n",
    "```python\n",
    "peopleSchema.groupBy(\"age\").count().orderBy(\"age\").show()\n",
    "```\n",
    "- Groups the DataFrame by `age` and counts the number of people in each age group.\n",
    "- Orders the results by age and displays them.\n",
    "\n",
    "---\n",
    "\n",
    "#### 11. **Stopping the Spark Session**\n",
    "```python\n",
    "spark.stop()\n",
    "```\n",
    "- Stops the Spark session and releases resources.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sample Input File (`fakeFriends.csv`)**\n",
    "```\n",
    "1,John,18,200\n",
    "2,Jane,22,150\n",
    "3,Mike,15,300\n",
    "4,Sara,19,250\n",
    "5,Tom,10,100\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Execution with Example**\n",
    "\n",
    "#### **1. Mapper Function**\n",
    "Input line: `\"1,John,18,200\"`\n",
    "\n",
    "Output Row:\n",
    "```python\n",
    "Row(id=1, name='John', age=18, numFriends=200)\n",
    "```\n",
    "\n",
    "#### **2. DataFrame Creation**\n",
    "DataFrame:\n",
    "```\n",
    "+---+----+---+----------+\n",
    "| id|name|age|numFriends|\n",
    "+---+----+---+----------+\n",
    "|  1|John| 18|       200|\n",
    "|  2|Jane| 22|       150|\n",
    "|  3|Mike| 15|       300|\n",
    "|  4|Sara| 19|       250|\n",
    "|  5|Tom | 10|       100|\n",
    "+---+----+---+----------+\n",
    "```\n",
    "\n",
    "#### **3. SQL Query for Teenagers**\n",
    "SQL Query:\n",
    "```sql\n",
    "SELECT * FROM people WHERE age > 10 AND age < 21;\n",
    "```\n",
    "\n",
    "Result:\n",
    "```\n",
    "+---+----+---+----------+\n",
    "| id|name|age|numFriends|\n",
    "+---+----+---+----------+\n",
    "|  1|John| 18|       200|\n",
    "|  3|Mike| 15|       300|\n",
    "|  4|Sara| 19|       250|\n",
    "+---+----+---+----------+\n",
    "```\n",
    "\n",
    "#### **4. Grouping by Age**\n",
    "Output:\n",
    "```\n",
    "+---+-----+\n",
    "|age|count|\n",
    "+---+-----+\n",
    "| 10|    1|\n",
    "| 15|    1|\n",
    "| 18|    1|\n",
    "| 19|    1|\n",
    "| 22|    1|\n",
    "+---+-----+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts Illustrated**\n",
    "1. **RDD to DataFrame Conversion**: Shows how to transform unstructured text data into structured DataFrames.\n",
    "2. **SQL Queries**: Demonstrates SQL-like operations on DataFrames.\n",
    "3. **Grouping and Aggregation**: Groups and counts data efficiently.\n",
    "\n",
    "Let me know if you'd like further clarification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e6249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
