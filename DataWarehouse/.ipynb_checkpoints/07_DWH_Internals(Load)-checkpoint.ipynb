{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6dd71ea",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üî∑ **1. Initial Load**\n",
    "\n",
    "### üîπ **What is Initial Load?**\n",
    "\n",
    "The **initial load** is the very first-time load of data from source systems into the data warehouse. It brings historical data into the DWH to make it ready for production use.\n",
    "\n",
    "### ‚úÖ Characteristics:\n",
    "\n",
    "* Done **only once** (before the system goes live).\n",
    "* Brings **full snapshot** of all historical data.\n",
    "* Used for **backfilling** dimensions and facts.\n",
    "* Often happens in **batch** mode with bulk insert operations.\n",
    "* Can take hours/days depending on volume.\n",
    "\n",
    "### üìå Real Scenario:\n",
    "\n",
    "For a retail company launching a new warehouse, we brought:\n",
    "\n",
    "* 5 years of customer transactions\n",
    "* 10 years of product catalog history\n",
    "* Complete store hierarchy\n",
    "\n",
    "The volume was 2 TB, and the initial load was done over a weekend.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ **2. Incremental Load**\n",
    "\n",
    "### üîπ **What is Incremental Load?**\n",
    "\n",
    "Once the DWH goes live, **new or changed data** needs to be loaded periodically (hourly, daily, etc.). This is handled using **incremental loading** techniques.\n",
    "\n",
    "### ‚úÖ Categories of data changes:\n",
    "\n",
    "1. **New Records**: e.g., new customer signups, new orders.\n",
    "2. **Modified Records**: e.g., user updated their email or address.\n",
    "3. **Deleted Records**: e.g., GDPR requests for deletion, deactivated products.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ **3. Types of Incremental Load**\n",
    "\n",
    "Let‚Äôs break them down with examples:\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ **Append (Insert Only)**\n",
    "\n",
    "* Adds only new records into the warehouse.\n",
    "* No updates or deletes performed.\n",
    "\n",
    "üìå Example:\n",
    "\n",
    "```sql\n",
    "INSERT INTO sales_fact (order_id, customer_id, order_date, amount)\n",
    "SELECT * FROM staging_sales\n",
    "WHERE order_date = CURRENT_DATE - 1;\n",
    "```\n",
    "\n",
    "üìç Use When:\n",
    "\n",
    "* Data is immutable (like logs, transactions).\n",
    "* Easy to identify new records (like using `created_at` timestamp).\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ **In-Place Update**\n",
    "\n",
    "* Performs `INSERT` for new data.\n",
    "* Performs `UPDATE` for modified records (usually matched by primary key or natural key).\n",
    "* May include `DELETE` for removed data if needed.\n",
    "\n",
    "üìå Example (Upsert in Snowflake):\n",
    "\n",
    "```sql\n",
    "MERGE INTO customer_dim AS target\n",
    "USING staging_customer AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET ...\n",
    "WHEN NOT MATCHED THEN INSERT ...\n",
    "```\n",
    "\n",
    "üìç Use When:\n",
    "\n",
    "* Data is mutable (e.g., customer address changes).\n",
    "* You need to keep warehouse always in sync with source.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ **Complete Replacement (Truncate and Reload)**\n",
    "\n",
    "* Delete all data in target table.\n",
    "* Load the latest full snapshot from source.\n",
    "\n",
    "üìå Example:\n",
    "\n",
    "```sql\n",
    "TRUNCATE TABLE product_dim;\n",
    "\n",
    "INSERT INTO product_dim\n",
    "SELECT * FROM staging_product;\n",
    "```\n",
    "\n",
    "üìç Use When:\n",
    "\n",
    "* Table is small.\n",
    "* No reliable incremental identifiers (like `last_updated` column).\n",
    "* Data volatility is high (e.g., reference datasets).\n",
    "\n",
    "‚ùóCaution: Can cause **report downtime** and **integrity issues** if not managed with care.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ **Rolling Append**\n",
    "\n",
    "* A variation of append load.\n",
    "* Only a **rolling window of recent data** is loaded (e.g., last 30 days).\n",
    "* Older data remains untouched.\n",
    "\n",
    "üìå Example:\n",
    "\n",
    "```sql\n",
    "DELETE FROM order_fact\n",
    "WHERE order_date >= CURRENT_DATE - 30;\n",
    "\n",
    "INSERT INTO order_fact\n",
    "SELECT * FROM staging_orders\n",
    "WHERE order_date >= CURRENT_DATE - 30;\n",
    "```\n",
    "\n",
    "üìç Use When:\n",
    "\n",
    "* High volume of recent data updates.\n",
    "* System performance is critical.\n",
    "* Ideal for clickstream or IoT data.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ **What if Loading Is Not Done Properly?**\n",
    "\n",
    "| Failure                  | Impact                                                     |\n",
    "| ------------------------ | ---------------------------------------------------------- |\n",
    "| Skipped Incremental Load | Reports show incomplete or outdated data                   |\n",
    "| Duplicate Loading        | Double counting in reports, misleading KPIs                |\n",
    "| Missing Delete Handling  | Obsolete data retained, e.g., deleted users still reported |\n",
    "| Truncate & Reload Misuse | Downtime, broken joins, snapshot inconsistencies           |\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Real-Life Example (Scenario Based)\n",
    "\n",
    "**Company**: E-commerce platform with millions of daily orders.\n",
    "\n",
    "**Data Sources**: Order DB (PostgreSQL), Customer DB (MongoDB), Shipping API (JSON dumps)\n",
    "\n",
    "### Loading Strategy:\n",
    "\n",
    "| Layer         | Strategy                               |\n",
    "| ------------- | -------------------------------------- |\n",
    "| Orders Fact   | Append Load (new orders daily)         |\n",
    "| Customer Dim  | In-Place Update (email, phone updates) |\n",
    "| Product Dim   | Complete Replacement (weekly snapshot) |\n",
    "| Shipping Logs | Rolling Append (last 7 days window)    |\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* Orders are immutable ‚Üí append is safe.\n",
    "* Customers update frequently ‚Üí in-place update ensures consistency.\n",
    "* Products table changes structure and description ‚Üí safer to reload completely.\n",
    "* Shipping logs are heavy and recent-data focused ‚Üí rolling load is efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Important Questions\n",
    "\n",
    "1. **How do you handle incremental loads in your warehouse?**\n",
    "2. **What strategy would you choose for mutable vs. immutable data?**\n",
    "3. **Can you explain the difference between append and in-place update loads?**\n",
    "4. **Have you ever dealt with failed loads? How did you recover data integrity?**\n",
    "5. **What are common risks of truncate-reload strategy and how do you mitigate them?**\n",
    "6. **How do you track data change timestamps in source systems?**\n",
    "7. **Which type of loading would you prefer for a product catalog that changes weekly? Why?**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20862d10",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 1. **How do you handle incremental loads in your warehouse?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "We use **metadata-driven ELT/ETL pipelines** to handle incremental loads. Each table has **CDC markers** like `last_updated_timestamp` or surrogate keys to detect changes. Based on the data nature:\n",
    "\n",
    "* **Transactional fact tables**: we use **append-only** strategy.\n",
    "* **Slowly changing dimensions (SCDs)**: we use **in-place upserts** (Type 1 or Type 2).\n",
    "* For some high-volume logs, we use a **rolling window** approach (e.g., keep last 30 days).\n",
    "\n",
    "These loads are orchestrated through **Apache Airflow** with proper **auditing, error handling**, and **idempotency** to ensure safe re-runs.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. **What strategy would you choose for mutable vs. immutable data?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* For **immutable data** (e.g., logs, events), I choose **append** since records never change after creation.\n",
    "* For **mutable data** (e.g., customer profiles, product info), I use **in-place update** or **SCD handling** because values can be corrected or enriched over time.\n",
    "\n",
    "üîÅ If a source doesn‚Äôt provide CDC flags, I rely on **hash-diff comparison** between staging and target.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 3. **Can you explain the difference between append and in-place update loads?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Append**: Adds only **new records**. Existing data remains untouched. Ideal for transaction logs, clickstream data.\n",
    "* **In-place update**: Updates existing rows and inserts new ones. Used for data that can **mutate**, like customer contact info.\n",
    "\n",
    "üìå Example:\n",
    "For `customer_dim`, if email changes:\n",
    "\n",
    "* Append would ignore it.\n",
    "* In-place update would reflect the latest email.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 4. **Have you ever dealt with failed loads? How did you recover data integrity?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Yes ‚Äî multiple times. For example, once a staging load was corrupted due to timezone misalignment in timestamp parsing.\n",
    "\n",
    "To recover:\n",
    "\n",
    "1. We immediately **quarantined** the corrupted batch.\n",
    "2. Rolled back downstream tables using **time-partitioned backups**.\n",
    "3. Re-ran the job with corrected logic.\n",
    "4. Updated our **data validation rules** to include timezone consistency checks.\n",
    "\n",
    "We also maintain a **load audit table** that tracks load status (start\\_time, end\\_time, row count, checksum) to identify anomalies early.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 5. **What are common risks of truncate-reload strategy and how do you mitigate them?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Risks:**\n",
    "\n",
    "* **Downtime**: If truncate takes long, reporting gets affected.\n",
    "* **Foreign key breaks**: If dimension-fact relationships are broken temporarily.\n",
    "* **Accidental Data Loss**: If the source has bugs, you'll overwrite good data with bad.\n",
    "\n",
    "**Mitigation:**\n",
    "\n",
    "* Use **staging table + swap strategy**:\n",
    "\n",
    "  * Load into a temp table.\n",
    "  * Validate row counts, checksums.\n",
    "  * Then atomically swap.\n",
    "\n",
    "* Perform **row-level diff checks** before truncating.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 6. **How do you track data change timestamps in source systems?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Best practice is to request source systems to include **audit fields**:\n",
    "\n",
    "* `created_at`, `updated_at`, `deleted_at`\n",
    "* Or an **incremental surrogate key** or **CDC log position**\n",
    "\n",
    "If not available:\n",
    "\n",
    "* I implement a **hashing strategy** (e.g., MD5 across important columns) to detect changes between previous and current records.\n",
    "* For APIs, I use `since` parameter if supported, else snapshot + diff.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 7. **Which type of loading would you prefer for a product catalog that changes weekly? Why?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "I‚Äôd go with **complete replacement (truncate-reload)** strategy.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* Product catalogs are **relatively small** (few thousand to million rows).\n",
    "* Changes are frequent and not always flagged.\n",
    "* Easier and faster to load the entire clean snapshot weekly.\n",
    "\n",
    "If performance is a concern, I‚Äôd explore **MERGE-based upserts** using `product_id` + `hashdiff`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33175d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
