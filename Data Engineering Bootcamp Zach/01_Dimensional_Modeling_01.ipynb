{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4036887b",
   "metadata": {},
   "source": [
    "### Dimensional Data Modeling\n",
    "\n",
    "Dimensional Data Modeling (DDM) is a design technique in data warehousing that organizes data for better querying and reporting. It uses two types of tables:\n",
    "- **Fact Tables**: Store measurable, numeric data (e.g., sales amount, order count).\n",
    "- **Dimension Tables**: Provide descriptive context to the facts (e.g., product details, customer information).\n",
    "\n",
    "This design improves query performance and simplifies data analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Complex Data Types**\n",
    "In some cases, dimensions may use complex data types like **Struct** and **Array** to manage semi-structured data efficiently.\n",
    "\n",
    "1. **Struct**:\n",
    "   - A nested data type that groups related fields into a single column.\n",
    "   - Example:\n",
    "     ```json\n",
    "     {\n",
    "       \"address\": {\n",
    "         \"street\": \"123 Elm St\",\n",
    "         \"city\": \"Springfield\",\n",
    "         \"zip\": \"62704\"\n",
    "       }\n",
    "     }\n",
    "     ```\n",
    "   - Use case: When fields are logically grouped, e.g., a full address.\n",
    "\n",
    "2. **Array**:\n",
    "   - A data type that holds multiple values of the same type in a single column.\n",
    "   - Example:\n",
    "     ```json\n",
    "     {\n",
    "       \"tags\": [\"Electronics\", \"Sale\", \"Featured\"]\n",
    "     }\n",
    "     ```\n",
    "   - Use case: For attributes with variable cardinality, like tags or keywords.\n",
    "\n",
    "---\n",
    "\n",
    "#### **What is a Dimension?**\n",
    "Dimensions provide context to the data stored in fact tables. They answer \"who,\" \"what,\" \"where,\" and \"when\" questions about facts. \n",
    "\n",
    "- **Dimensions Identifying an Entity**:\n",
    "  These uniquely identify an object (e.g., Customer ID, Product ID).\n",
    "  - Example: `Customer_Dim` with `Customer_ID` as the primary key.\n",
    "\n",
    "- **Dimensions as Attributes**:\n",
    "  These describe the properties of an entity (e.g., Product Color, Customer Age).\n",
    "  - Example: `Customer_Dim` with `Customer_Age` as an attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Two Flavors of Dimensions**\n",
    "1. **Slowly-Changing Dimensions (SCDs)**:\n",
    "   - Dimensions that change over time and require tracking historical values.\n",
    "   - Example:\n",
    "     - `Employee_Dim` tracks department changes.\n",
    "   - Types of SCDs:\n",
    "     - **Type 1**: Overwrite old data.\n",
    "     - **Type 2**: Add a new row with versioning.\n",
    "     - **Type 3**: Add a new column for old values.\n",
    "\n",
    "2. **Fixed Dimensions**:\n",
    "   - Dimensions that remain constant and do not change over time.\n",
    "   - Example:\n",
    "     - `Country_Dim` where countries rarely change.\n",
    "\n",
    "---\n",
    "\n",
    "### Knowing Your Consumer\n",
    "The way you model data depends heavily on who will consume it. Each type of consumer has different needs.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Data Analysts/Scientists**\n",
    "- **Primary Requirement**: Data should be **easy to query**.\n",
    "  - Avoid complex nested structures (e.g., Struct, Array).\n",
    "  - Use **flat tables** with descriptive column names.\n",
    "- **Reason**: They rely on SQL or BI tools to extract insights and need intuitive datasets.\n",
    "\n",
    "Example:  \n",
    "Instead of using a `Struct` for address details, split it into multiple columns:\n",
    "```sql\n",
    "SELECT Customer_ID, Street, City, Zip FROM Customer_Dim;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Other Data Engineers**\n",
    "- **Primary Requirement**: Pipelines should be **compact and reusable**.\n",
    "  - Nested types (e.g., Struct, Array) are acceptable as they reduce redundancy and storage requirements.\n",
    "- **Master Datasets**: Unified datasets created from multiple pipelines, used as a trusted data source.\n",
    "  - Example:\n",
    "    - A `Customer_Master` dataset combines `Customer_Dim` and `Transaction_Fact` for reusable analytics.\n",
    "\n",
    "---\n",
    "\n",
    "#### **ML Models/Engineers**\n",
    "- **Primary Requirement**: Data should match the training model's needs.\n",
    "  - Use **flat datasets** with:\n",
    "    - Identifiers (e.g., `Customer_ID`).\n",
    "    - Features (e.g., numeric or categorical columns).\n",
    "    - Labels (e.g., churn prediction flag).\n",
    "  - For advanced models, nested columns (e.g., Array of events) might be needed.\n",
    "\n",
    "Example:\n",
    "A dataset for a churn prediction model:\n",
    "```csv\n",
    "Customer_ID, Age, Monthly_Spend, Churn_Flag\n",
    "12345, 35, 120.50, 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Customers**\n",
    "- **Primary Requirement**: Data should translate into **easily interpretable outputs** (e.g., dashboards, charts).\n",
    "  - Focus on simplified, aggregated data rather than raw tables.\n",
    "  - Example:\n",
    "    - Create a bar chart showing monthly sales by region.\n",
    "\n",
    "---\n",
    "\n",
    "### Important Considerations\n",
    "1. **Scalability**:\n",
    "   - Ensure your models can handle increasing data volumes efficiently.\n",
    "2. **Versioning**:\n",
    "   - Track changes for reproducibility.\n",
    "3. **Performance**:\n",
    "   - Optimize for query performance, considering indexing and partitioning.\n",
    "4. **Governance**:\n",
    "   - Ensure compliance with data privacy and security regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711b2d9",
   "metadata": {},
   "source": [
    "### OLTP vs Master Data vs OLAP  \n",
    "\n",
    "---\n",
    "\n",
    "#### **OLTP (Online Transaction Processing)**  \n",
    "- **Definition**: OLTP systems manage real-time transaction data and are designed for quick, reliable, and consistent operations.  \n",
    "- **Key Characteristics**:  \n",
    "  - Used by **software engineers** to ensure online systems (e.g., e-commerce, banking apps) run smoothly.  \n",
    "  - **Normalization**:  \n",
    "    - Reduces data duplication for consistency and efficiency.  \n",
    "    - Uses **linker tables**, **primary/foreign keys**, and **constraints** to maintain relationships.  \n",
    "    - Example: A `Customer` table links to `Order` and `Payment` tables via keys.  \n",
    "  - **Optimization**:  \n",
    "    - Focused on **low-latency, low-volume queries** to serve single-entity operations (e.g., retrieving a user profile).  \n",
    "  - Example Use Case: Updating a bank account balance after a transaction.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **OLAP (Online Analytical Processing)**  \n",
    "- **Definition**: OLAP systems are designed for analyzing large volumes of data to derive insights.  \n",
    "- **Key Characteristics**:  \n",
    "  - Optimized for **large-volume GROUP BY queries** with minimal JOINs for efficiency.  \n",
    "  - Typically operates on aggregated or historical data.  \n",
    "  - Focuses on answering business questions across many entities (e.g., trends, patterns).  \n",
    "  - **Difference with OLTP**: OLTP looks at individual records, while OLAP looks at large datasets.  \n",
    "  - Example Use Case: Summarizing total monthly sales across all stores.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Master Data**  \n",
    "- **Definition**: Master data serves as a trusted middle layer between OLTP and OLAP, consolidating and deduplicating data.  \n",
    "- **Key Characteristics**:  \n",
    "  - Focuses on the **completeness of entity definitions** (e.g., customer, product).  \n",
    "  - Ensures **deduplication** for a single version of truth.  \n",
    "  - Acts as a **foundation** for analytical and operational systems.  \n",
    "  - Example Use Case: A `Customer_Master` table combining OLTP data (customer transactions) into a consistent format for OLAP.  \n",
    "\n",
    "---\n",
    "\n",
    "### Some of the Biggest Problems in Data Engineering Occur When Data is Modeled for the Wrong Consumer  \n",
    "1. **Case 1: Flattened OLAP Structure for OLTP**:  \n",
    "   - **Problem**: A flattened OLAP table is used in an OLTP system, making updates slow and prone to errors.  \n",
    "   - **Impact**: High query latency for transactions; complex data manipulation increases operational inefficiency.  \n",
    "\n",
    "2. **Case 2: Highly Normalized Data for Analysts**:  \n",
    "   - **Problem**: Analysts are given normalized OLTP data instead of aggregated OLAP tables.  \n",
    "   - **Impact**: Analysts spend excessive time writing complex queries with many JOINs.  \n",
    "\n",
    "3. **Case 3: No Master Data for ML Models**:  \n",
    "   - **Problem**: Training models with raw OLTP or heavily aggregated OLAP data.  \n",
    "   - **Impact**: Models produce inconsistent results due to incomplete or noisy input data.  \n",
    "\n",
    "---\n",
    "\n",
    "### OLTP and OLAP as a Continuum  \n",
    "The journey from OLTP to metrics involves several layers, each with specific roles:  \n",
    "\n",
    "#### **1. Production Database Snapshots**  \n",
    "- **Definition**: Raw OLTP data snapshots in their original transactional format.  \n",
    "- **Characteristics**:  \n",
    "  - High granularity; contains all transactions.  \n",
    "  - Ideal for operational systems, not analysis.  \n",
    "- **Challenges**:  \n",
    "  - Raw, uncleaned data is difficult to query directly for insights.  \n",
    "\n",
    "#### **2. Master Data**  \n",
    "- **Definition**: A consolidated and deduplicated dataset that serves as a trusted source of truth.  \n",
    "- **Role**:  \n",
    "  - Bridges OLTP and OLAP.  \n",
    "  - Provides normalized, cleaned data.  \n",
    "- **Benefits**:  \n",
    "  - Makes querying easier for downstream users.  \n",
    "  - Reduces redundancy and ambiguity.  \n",
    "- **Consequences Without It**:  \n",
    "  - Inconsistent results across systems.  \n",
    "  - Increased complexity for analytics and reporting.  \n",
    "\n",
    "#### **3. OLAP Cube**  \n",
    "- **Definition**: A multidimensional dataset designed for analysis.  \n",
    "- **Role**:  \n",
    "  - Optimized for slicing and dicing data (e.g., filtering, grouping).  \n",
    "  - Analysts and scientists perform aggregations like SUM, AVG.  \n",
    "- **Favorite Part for Analysts**: Flexibility in querying and analyzing data from various dimensions.  \n",
    "\n",
    "#### **4. Metrics**  \n",
    "- **Definition**: Aggregated results derived from OLAP cubes.  \n",
    "- **Role**:  \n",
    "  - Simplified datasets or single numbers for dashboards and decision-making.  \n",
    "  - Example: Total revenue last month.  \n",
    "\n",
    "---\n",
    "\n",
    "### Diagram: The Four Layers in Data Modeling  \n",
    "\n",
    "```plaintext\n",
    "+-----------------------------+\n",
    "|      Metrics (Aggregated)   |  \n",
    "| (e.g., total revenue, KPIs) |\n",
    "+-----------------------------+\n",
    "             ↑\n",
    "+-----------------------------+\n",
    "|         OLAP Cube           |\n",
    "| (Slicing, dicing, analysis) |\n",
    "+-----------------------------+\n",
    "             ↑\n",
    "+-----------------------------+\n",
    "|         Master Data          |\n",
    "| (Deduped, consistent format) |\n",
    "+-----------------------------+\n",
    "             ↑\n",
    "+-----------------------------+\n",
    "| Production Database Snapshots|\n",
    "|   (Raw OLTP, normalized)     |\n",
    "+-----------------------------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### How the Master Data Layer Helps  \n",
    "1. **Easier Querying**:  \n",
    "   - Provides clean, deduplicated data in consistent formats for downstream processing.  \n",
    "   - Reduces query complexity for analysts and scientists.  \n",
    "\n",
    "2. **Standardization**:  \n",
    "   - Establishes a single version of truth, preventing data conflicts.  \n",
    "\n",
    "3. **Consequences of Missing Master Data**:  \n",
    "   - Ambiguous insights due to inconsistent raw data.  \n",
    "   - Redundant queries and increased maintenance.  \n",
    "\n",
    "--- \n",
    "\n",
    "This layered approach ensures data is fit for every consumer, reducing inefficiencies and improving productivity across the organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e907e",
   "metadata": {},
   "source": [
    "### **Cumulative Table Design**\n",
    "\n",
    "Cumulative tables are designed to **capture and retain historical data** by accumulating changes over time. These tables grow incrementally, making them ideal for historical and transition tracking.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Core Components**  \n",
    "1. **Two Data Frames/Tables**:  \n",
    "   - **Yesterday's Table**: Contains the state of data as of the previous day.  \n",
    "   - **Today's Table**: Contains the current state of data.  \n",
    "\n",
    "2. **Full Outer Join**:  \n",
    "   - Why?  \n",
    "     - Ensures all rows from both tables are preserved.  \n",
    "     - Tracks new additions, changes, and deletions across time.  \n",
    "     - Identifies records present in one but missing in the other (e.g., inactive/deleted entities).  \n",
    "\n",
    "3. **COALESCE Values**:  \n",
    "   - Retains non-NULL values by merging the two tables.  \n",
    "   - Helps carry forward history and ensures no data loss.  \n",
    "\n",
    "4. **History Retention**:  \n",
    "   - Every record is kept to ensure historical accuracy.  \n",
    "   - Changes and deletions are preserved as part of the history.  \n",
    "\n",
    "5. **Table Growth**:  \n",
    "   - The table size typically increases daily as historical data is accumulated.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Usages**  \n",
    "1. **Growth Analytics**:  \n",
    "   - Example: `DIM_ALL_USERS` at Facebook, where all user states (active, inactive, deleted) are tracked over time.  \n",
    "\n",
    "2. **State Transition Tracking**:  \n",
    "   - Example: Monitoring changes in user subscription states (e.g., trial → active → canceled).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Diagram**  \n",
    "\n",
    "```plaintext\n",
    "+---------------------------------------------+\n",
    "|           Cumulative Table (History)        |\n",
    "+---------------------------------------------+\n",
    "| User_ID | State       | Date       | Info    |\n",
    "|---------|-------------|------------|---------|\n",
    "| 1       | Active      | 2024-11-15 | ...     |\n",
    "| 2       | Inactive    | 2024-11-16 | ...     |\n",
    "| 3       | Deleted     | 2024-11-17 | ...     |\n",
    "+---------------------------------------------+\n",
    "\n",
    "        ↑                                ↑\n",
    "+-------------+        Full Outer Join         +-------------+\n",
    "| Yesterday's |   →   +-----------------+   ←  | Today's     |\n",
    "| Table       |          COALESCE()          | Table         |\n",
    "+-------------+                               +-------------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Strengths**  \n",
    "1. **Historical Analysis Without Shuffle**:  \n",
    "   - Easily query historical trends or transitions without recomputing data.  \n",
    "\n",
    "2. **Transition Analysis**:  \n",
    "   - Tracks changes (e.g., state, value) over time seamlessly.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Drawbacks**  \n",
    "1. **Sequential Backfilling**:  \n",
    "   - Adding past data must follow the timeline sequentially, making corrections or reprocessing time-consuming.  \n",
    "\n",
    "2. **PII Handling**:  \n",
    "   - Dealing with Personally Identifiable Information (PII) is challenging because deleted or inactive users remain in the history unless purged.\n",
    "\n",
    "---\n",
    "\n",
    "### **The Compactness vs. Usability Tradeoffs**\n",
    "\n",
    "The design of tables often involves balancing **compactness** and **usability** depending on the use case.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Most Usable Tables**  \n",
    "- **Characteristics**:  \n",
    "  - No **complex data types** like Structs or Arrays.  \n",
    "  - Data is easily filtered and aggregated with `WHERE` and `GROUP BY` clauses.  \n",
    "  - Readable and intuitive.  \n",
    "\n",
    "- **Use Cases**:  \n",
    "  - Designed for **analysts** and non-technical consumers.  \n",
    "  - Example: A sales summary table showing daily revenue by region.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Most Compact Tables**  \n",
    "- **Characteristics**:  \n",
    "  - Data is **compressed** for storage and speed.  \n",
    "  - Uses techniques like encoding or binary storage formats.  \n",
    "  - Often includes **complex/nested data types**.  \n",
    "  - Harder to query without decoding or special tools.  \n",
    "\n",
    "- **Use Cases**:  \n",
    "  - Designed for **production systems** where speed and efficiency matter.  \n",
    "  - Example: Compressed event logs for real-time tracking in a web application.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Middle Ground Tables**  \n",
    "- **Characteristics**:  \n",
    "  - Balance **compactness** with usability by leveraging complex data types (e.g., Struct, Array, Map).  \n",
    "  - Compact but still queryable with more effort.  \n",
    "\n",
    "- **Use Cases**:  \n",
    "  - Designed for **data engineers** building pipelines or creating derived datasets.  \n",
    "  - Example: Master data tables consolidating raw data for downstream systems.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison**  \n",
    "\n",
    "| Aspect                  | Most Usable Tables      | Most Compact Tables     | Middle Ground Tables       |\n",
    "|-------------------------|-------------------------|-------------------------|----------------------------|\n",
    "| **Complexity**          | Simple                 | Highly compressed       | Moderate                   |\n",
    "| **Ease of Querying**    | High                   | Low                     | Medium                     |\n",
    "| **Primary Consumers**   | Analysts, Non-technical| Engineers, Developers   | Data Engineers             |\n",
    "| **Use Case**            | Analytics              | Production Systems      | Master Data                |\n",
    "\n",
    "---\n",
    "\n",
    "By understanding these tradeoffs, you can design tables tailored to the specific needs of your data consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e86212",
   "metadata": {},
   "source": [
    "### **Struct vs. Array vs. Map**  \n",
    "\n",
    "| **Feature**              | **Struct**                          | **Array**                           | **Map**                          |\n",
    "|--------------------------|--------------------------------------|-------------------------------------|-----------------------------------|\n",
    "| **Definition**            | A fixed collection of key-value pairs. | An ordered collection of values.     | A flexible collection of key-value pairs. |\n",
    "| **Key Definition**        | **Rigid**: Keys must be predefined. | **N/A**: No keys, just indices.      | **Flexible**: Keys are not predefined. |\n",
    "| **Value Types**           | Can be of **any type** (mixed types allowed). | All values must be of the **same type**. | All values must be of the **same type**. |\n",
    "| **Ordering**              | **Not ordered**                     | **Ordered** by index.               | **Not ordered**.                |\n",
    "| **Compression**           | **High** (due to fixed structure).  | **Medium** (depends on size).        | **Moderate** (varies with key flexibility). |\n",
    "| **Key Access**            | Access by **name** (e.g., `struct.key`). | Access by **index** (e.g., `array[0]`). | Access by **key** (e.g., `map['key']`). |\n",
    "| **Flexibility**           | Low: Schema must be defined upfront.| Medium: Can vary in size.            | High: Keys and size can vary.     |\n",
    "| **Usability**             | Great for structured data (e.g., nested records). | Ideal for lists or sequences.        | Useful for key-value mappings.   |\n",
    "| **Example Use Case**      | Nested records like addresses or configurations. | A list of items like product IDs.    | Dynamic key-value pairs like attributes and properties. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Examples**\n",
    "\n",
    "#### **Struct**\n",
    "```sql\n",
    "-- Example of a Struct\n",
    "SELECT \n",
    "    STRUCT('John Doe' AS name, 30 AS age, 'Engineer' AS job) AS person;\n",
    "```\n",
    "- Keys: `name`, `age`, `job`.\n",
    "- Values: `\"John Doe\"`, `30`, `\"Engineer\"`.\n",
    "- Good for **nested, structured data** like user profiles.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Array**\n",
    "```sql\n",
    "-- Example of an Array\n",
    "SELECT ARRAY[1, 2, 3, 4, 5] AS numbers;\n",
    "```\n",
    "- Values: `[1, 2, 3, 4, 5]`.\n",
    "- Good for **sequential data** like ordered lists.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Map**\n",
    "```sql\n",
    "-- Example of a Map\n",
    "SELECT MAP('key1', 'value1', 'key2', 'value2') AS attributes;\n",
    "```\n",
    "- Keys: `\"key1\"`, `\"key2\"`.\n",
    "- Values: `\"value1\"`, `\"value2\"`.\n",
    "- Good for **dynamic key-value pairs** like user preferences or metadata.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "- Use **Struct** when the schema is fixed and requires diverse value types.\n",
    "- Use **Array** for sequential, homogeneous data.\n",
    "- Use **Map** for dynamic key-value relationships with the same value type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41a2a7",
   "metadata": {},
   "source": [
    "### **Temporal Cardinality Explosions of Dimensions**\n",
    "\n",
    "Temporal cardinality explosions occur when a dimension's granularity significantly increases over time due to the accumulation of time-sensitive or historical data. This phenomenon often results in bloated tables, degraded query performance, and higher storage costs. It is a critical issue in time-based data models, especially in analytical databases and reporting systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Breaking it Down:**\n",
    "\n",
    "#### **What Causes Temporal Cardinality Explosions?**\n",
    "1. **High Granularity of Time**:\n",
    "   - Storing data at a fine granularity (e.g., milliseconds, seconds) leads to rapid growth of distinct keys in a dimension table.\n",
    "   - Example: Tracking every click event with a unique timestamp.\n",
    "\n",
    "2. **Tracking Historical Changes**:\n",
    "   - Slowly Changing Dimensions (SCDs), especially Type 2, retain history by creating a new row for every update, leading to increased cardinality over time.\n",
    "\n",
    "3. **Excessive Dimension Updates**:\n",
    "   - Frequent updates to attributes (e.g., product price changes, user address changes) create new entries in dimension tables.\n",
    "\n",
    "4. **Multi-dimensional Time Attributes**:\n",
    "   - Combining multiple temporal attributes (e.g., start time, end time) further increases the number of unique combinations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Impact of Temporal Cardinality Explosions**\n",
    "1. **Performance Degradation**:\n",
    "   - Larger dimensions slow down joins and aggregations due to higher cardinality.\n",
    "   - Increased memory usage in query execution.\n",
    "\n",
    "2. **Storage Overhead**:\n",
    "   - Rapid growth in dimension table size increases storage requirements.\n",
    "\n",
    "3. **Complexity in Maintenance**:\n",
    "   - High cardinality dimensions are harder to manage and optimize.\n",
    "\n",
    "4. **Query Complexity**:\n",
    "   - Analysts may need to write more complex queries to handle time ranges or deduplicate data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Real-World Example**\n",
    "\n",
    "**E-commerce Transactions**:\n",
    "- A `customer_dimension` table tracks customer attributes.\n",
    "- Attributes like \"Membership Tier\" or \"Preferred Store\" change over time.\n",
    "- Using SCD Type 2 to preserve history:\n",
    "  - Each change results in a new row with a start and end date.\n",
    "  - Over time, the number of rows explodes as customers update their information frequently.\n",
    "\n",
    "---\n",
    "\n",
    "### **Strategies to Mitigate Temporal Cardinality Explosions**\n",
    "\n",
    "1. **Adjust Granularity**:\n",
    "   - Aggregate data to coarser granularity (e.g., daily instead of hourly).\n",
    "\n",
    "2. **Hybrid Dimension Modeling**:\n",
    "   - Combine SCD Types (e.g., Type 1 for non-critical attributes, Type 2 for critical attributes).\n",
    "\n",
    "3. **Partitioning**:\n",
    "   - Partition dimension tables by time ranges or other relevant keys to isolate subsets of data.\n",
    "\n",
    "4. **Data Archival**:\n",
    "   - Move older, less-frequently-used data to cheaper storage solutions.\n",
    "\n",
    "5. **Normalized Dimensions**:\n",
    "   - Split high-cardinality columns into separate, smaller dimensions to reduce the load on the primary dimension table.\n",
    "\n",
    "6. **Decouple Temporal Data**:\n",
    "   - Use a fact table to store temporal data (e.g., time-series metrics) while keeping dimensions lean.\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualization of Temporal Cardinality Explosion**\n",
    "\n",
    "#### **Before Explosion:**\n",
    "| Customer ID | Membership Tier | Start Date   | End Date     |\n",
    "|-------------|-----------------|--------------|--------------|\n",
    "| 101         | Silver          | 2022-01-01   | 2022-06-01   |\n",
    "| 101         | Gold            | 2022-06-01   | 2022-12-31   |\n",
    "\n",
    "#### **After Explosion:**\n",
    "| Customer ID | Membership Tier | Start Date   | End Date     |\n",
    "|-------------|-----------------|--------------|--------------|\n",
    "| 101         | Silver          | 2022-01-01   | 2022-03-01   |\n",
    "| 101         | Silver          | 2022-03-01   | 2022-06-01   |\n",
    "| 101         | Gold            | 2022-06-01   | 2022-08-01   |\n",
    "| 101         | Platinum        | 2022-08-01   | 2022-12-31   |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "- Temporal cardinality explosions are a critical consideration in dimensional modeling when dealing with time-sensitive data.\n",
    "- Strategies like adjusting granularity, hybrid modeling, and partitioning can alleviate the problem.\n",
    "- Proper planning and understanding of data consumer needs are essential to balancing performance, storage, and usability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e61ac",
   "metadata": {},
   "source": [
    "### **Solutions to Temporal Cardinality Explosions**\n",
    "\n",
    "Temporal cardinality explosions can be mitigated with thoughtful design and appropriate optimization techniques. Below are solutions to address this issue, categorized by focus areas:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Dimension Table Design Improvements**\n",
    "#### **a. Adjust Time Granularity**\n",
    "- Reduce granularity where possible to limit the number of unique keys.\n",
    "  - **Example**: Instead of storing timestamps at the millisecond level, aggregate them to a daily or hourly granularity.\n",
    "  - Use a single date column instead of separate `start_time` and `end_time` columns unless necessary.\n",
    "\n",
    "#### **b. Hybrid Slowly Changing Dimensions (SCDs)**\n",
    "- Combine SCD types:\n",
    "  - **Type 1**: Overwrite non-critical attributes to avoid unnecessary history storage.\n",
    "  - **Type 2**: Preserve history only for attributes where historical analysis is necessary.\n",
    "\n",
    "#### **c. Attribute Splitting**\n",
    "- Separate frequently changing attributes into their own mini-dimensions.\n",
    "  - Example: Create a `customer_membership_tier_dimension` separate from the `customer_dimension`.\n",
    "\n",
    "#### **d. Surrogate Keys**\n",
    "- Use surrogate keys in place of natural keys to keep dimensions compact and simplify joins.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Fact Table Strategies**\n",
    "#### **a. Decouple Time-Sensitive Data**\n",
    "- Move frequently changing temporal data to the fact table rather than duplicating it in the dimension.\n",
    "  - Include references to the dimension table and timestamps in the fact table for temporal tracking.\n",
    "\n",
    "#### **b. Add Time-Effective Partitioning**\n",
    "- Partition fact tables by date or other relevant temporal keys to optimize queries and reduce processing overhead.\n",
    "  - Example: Queries for \"last 30 days\" only scan a specific partition, improving performance.\n",
    "\n",
    "#### **c. Snapshot Tables**\n",
    "- Use daily or periodic snapshots to capture the state of dimensions at a point in time rather than creating individual historical records for each change.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Archiving and Storage**\n",
    "#### **a. Historical Data Archival**\n",
    "- Move older data to cold storage solutions like Amazon S3, Azure Blob Storage, or Google Cloud Storage.\n",
    "  - Older data can be queried as needed without bloating active tables.\n",
    "\n",
    "#### **b. Use Time-Based Data Retention Policies**\n",
    "- Automatically purge records older than a specific time window if historical analysis is unnecessary.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Query Optimization**\n",
    "#### **a. Materialized Views**\n",
    "- Create materialized views that pre-aggregate data for common queries, reducing the need to process high-cardinality dimensions repeatedly.\n",
    "\n",
    "#### **b. Pre-join Fact and Dimension Tables**\n",
    "- Pre-join dimensions with fact tables for frequent queries, reducing the reliance on joining high-cardinality dimension tables during runtime.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Data Modeling Techniques**\n",
    "#### **a. Normalize Dimensions**\n",
    "- Break down high-cardinality dimensions into multiple normalized tables, each with a smaller subset of attributes.\n",
    "\n",
    "#### **b. Use Bridge Tables**\n",
    "- Use bridge tables for many-to-many relationships to reduce the size of primary dimension tables.\n",
    "  - Example: Instead of duplicating customers with multiple products, maintain a bridge table between `customer_dimension` and `product_dimension`.\n",
    "\n",
    "#### **c. Aggregate Data**\n",
    "- Roll up data into aggregated dimensions (e.g., monthly or quarterly data instead of daily data).\n",
    "- Use hierarchies to manage data granularity (e.g., year > month > day).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Technology-Specific Features**\n",
    "#### **a. Partitioning and Clustering**\n",
    "- Use database-specific optimizations such as **clustering in Snowflake** or **partition pruning in BigQuery** to handle large temporal datasets efficiently.\n",
    "\n",
    "#### **b. Columnar Storage**\n",
    "- Use columnar databases like Snowflake, Redshift, or BigQuery, which are optimized for analytical workloads and large datasets.\n",
    "\n",
    "#### **c. Compression**\n",
    "- Leverage database compression techniques to reduce the storage footprint of high-cardinality dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Metadata-Driven Approach**\n",
    "#### **a. Use Metadata Tables**\n",
    "- Maintain metadata tables to describe and manage frequently changing attributes without duplicating them in dimensions.\n",
    "  - Example: Keep a log of changes in a separate metadata table and link it to the dimension table.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Real-World Scenario Example**\n",
    "#### **Scenario: E-commerce Platform**\n",
    "**Problem**: `customer_dimension` grows uncontrollably due to frequent updates in \"Preferred Store\" and \"Membership Tier.\"\n",
    "  \n",
    "**Solution**:\n",
    "1. Separate `Preferred Store` and `Membership Tier` into their own dimensions.\n",
    "2. Use Type 1 SCD for \"Preferred Store\" since historical data is not critical.\n",
    "3. Apply Type 2 SCD for \"Membership Tier\" to track promotions.\n",
    "4. Store timestamped changes in a fact table for temporal queries.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Diagram of Optimized Data Flow**\n",
    "\n",
    "```\n",
    "+------------------------+\n",
    "| Aggregated Dimensions  | <--- Reduce granularity\n",
    "+------------------------+\n",
    "           |\n",
    "+------------------------+\n",
    "| Snapshot Fact Tables   | <--- Decouple time-sensitive data\n",
    "+------------------------+\n",
    "           |\n",
    "+------------------------+\n",
    "| Archival Data Storage  | <--- Archive older data\n",
    "+------------------------+\n",
    "           |\n",
    "+------------------------+\n",
    "| Materialized Views     | <--- Pre-aggregate for queries\n",
    "+------------------------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **Combine Techniques**: Use a mix of SCD optimization, decoupling temporal data, and partitioning for the best results.\n",
    "- **Understand Your Consumer**: Tailor solutions to the needs of analysts, engineers, or automated systems.\n",
    "- **Balance Storage and Performance**: Compress data, archive old records, and optimize querying mechanisms to manage growth sustainably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead09cc3",
   "metadata": {},
   "source": [
    "### **Run-Length Encoding (RLE) Compression**\n",
    "\n",
    "**Run-Length Encoding (RLE)** is a simple compression algorithm used to reduce the size of repetitive data. It replaces sequences (runs) of repeated values with a single value and its count.\n",
    "\n",
    "---\n",
    "\n",
    "### **How RLE Works**\n",
    "1. **Identify Repeated Values**: Find consecutive repeated values (a \"run\") in the data.\n",
    "2. **Replace the Run**: Substitute the repeated sequence with a pair of values:\n",
    "   - The repeated value.\n",
    "   - The count of how many times it appears consecutively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "#### Input Data:\n",
    "```plaintext\n",
    "AAAABBBCCDAA\n",
    "```\n",
    "\n",
    "#### RLE-Compressed Data:\n",
    "```plaintext\n",
    "4A3B2C1D2A\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "- **4A**: Four occurrences of `A`.\n",
    "- **3B**: Three occurrences of `B`.\n",
    "- **2C**: Two occurrences of `C`.\n",
    "- **1D**: One occurrence of `D`.\n",
    "- **2A**: Two occurrences of `A`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of RLE**\n",
    "1. **High Compression Ratio**: Works well with data that contains long sequences of repeated values, such as:\n",
    "   - Text with many repeated characters (e.g., `\"AAAABBB\"`).\n",
    "   - Images with large areas of uniform color (e.g., black-and-white images).\n",
    "2. **Simplicity**: Easy to implement and requires minimal computational resources.\n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages of RLE**\n",
    "1. **Ineffective for Random Data**: If data has no runs or very short runs, RLE may increase the data size.\n",
    "   - Example: `\"ABCD\"` becomes `\"1A1B1C1D\"`.\n",
    "2. **Not Suitable for High-Entropy Data**: Works poorly with highly varied datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of RLE**\n",
    "1. **Image Compression**:\n",
    "   - Used in formats like **TIFF** and **BMP** for compressing black-and-white or simple colored images.\n",
    "   - **Example**: Run lengths represent consecutive pixels of the same color.\n",
    "\n",
    "2. **Text Compression**:\n",
    "   - Reduces file sizes for repetitive text data, such as logs or certain types of documents.\n",
    "\n",
    "3. **Genomics**:\n",
    "   - Compresses DNA sequences with repetitive patterns (e.g., `\"AAAACTT\"`).\n",
    "\n",
    "4. **Data Transmission**:\n",
    "   - Reduces bandwidth by compressing repeated signals in a transmission stream.\n",
    "\n",
    "---\n",
    "\n",
    "### **RLE Algorithm (Pseudocode)**\n",
    "\n",
    "1. **Initialize an empty result list**.\n",
    "2. Iterate through the input data:\n",
    "   - Keep a counter for consecutive occurrences of the current value.\n",
    "   - When a new value is encountered, append the current value and its count to the result.\n",
    "3. **Handle the last value**: Add it to the result after the loop ends.\n",
    "4. Return the compressed result.\n",
    "\n",
    "---\n",
    "\n",
    "### **RLE Python Implementation**\n",
    "\n",
    "```python\n",
    "def run_length_encode(data):\n",
    "    if not data:\n",
    "        return []\n",
    "    \n",
    "    result = []\n",
    "    count = 1\n",
    "    \n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] == data[i - 1]:\n",
    "            count += 1\n",
    "        else:\n",
    "            result.append((data[i - 1], count))\n",
    "            count = 1\n",
    "    \n",
    "    # Add the last run\n",
    "    result.append((data[-1], count))\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "data = \"AAAABBBCCDAA\"\n",
    "encoded = run_length_encode(data)\n",
    "print(encoded)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```plaintext\n",
    "[('A', 4), ('B', 3), ('C', 2), ('D', 1), ('A', 2)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Decoding RLE**\n",
    "\n",
    "#### Algorithm:\n",
    "1. Read the encoded data pairs.\n",
    "2. Repeat the value by its count and concatenate to reconstruct the original data.\n",
    "\n",
    "#### Python Implementation:\n",
    "```python\n",
    "def run_length_decode(encoded):\n",
    "    result = []\n",
    "    for value, count in encoded:\n",
    "        result.extend([value] * count)\n",
    "    return ''.join(result)\n",
    "\n",
    "# Example usage\n",
    "decoded = run_length_decode(encoded)\n",
    "print(decoded)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```plaintext\n",
    "AAAABBBCCDAA\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Visual Representation**\n",
    "\n",
    "#### Input: `AAABBCCCCDDA`\n",
    "| Character | Count | Encoded |\n",
    "|-----------|-------|---------|\n",
    "| A         | 3     | 3A      |\n",
    "| B         | 2     | 2B      |\n",
    "| C         | 4     | 4C      |\n",
    "| D         | 2     | 2D      |\n",
    "| A         | 1     | 1A      |\n",
    "\n",
    "#### Final RLE: `3A2B4C2D1A`\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizations for RLE**\n",
    "1. **Threshold-Based Application**:\n",
    "   - Use RLE only if compression reduces data size.\n",
    "2. **Hybrid Compression**:\n",
    "   - Combine RLE with other algorithms like Huffman encoding for higher efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- RLE is a simple, effective method for compressing repetitive data.\n",
    "- It’s best suited for structured data with significant repetition.\n",
    "- Ineffective for random or highly variable datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b3414",
   "metadata": {},
   "source": [
    "### **Run-Length Encoding (RLE) and Temporal Cardinality Explosion**\n",
    "\n",
    "Temporal cardinality explosion occurs when dimension tables grow massively due to the high granularity of time-bound data. This leads to repeated values for dimensions across multiple timestamps. **RLE can help mitigate this problem by compressing the repetitive data sequences, especially in scenarios where values across time intervals are highly repetitive.**\n",
    "\n",
    "---\n",
    "\n",
    "### **How RLE Addresses Temporal Cardinality Explosion**\n",
    "\n",
    "1. **Identify Redundancies in Time-Series Data**:\n",
    "   - Time-series data often includes repeating values for attributes over time, such as:\n",
    "     - Status values (e.g., `\"active\"`, `\"inactive\"`).\n",
    "     - Metrics or identifiers remaining unchanged across several timestamps.\n",
    "\n",
    "2. **Group Repetitive Values**:\n",
    "   - Instead of storing every timestamp with the same dimension data, RLE groups consecutive identical values into a compressed format.\n",
    "\n",
    "3. **Compression of Dimension Values**:\n",
    "   - Dimension attributes like `status`, `location`, or `category` are stored with a \"run\" length indicating how many timestamps the value is applicable.\n",
    "   - Reduces storage requirements for high-cardinality, time-series datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Temporal Data Before and After RLE**\n",
    "\n",
    "#### Input Table (Without RLE):\n",
    "| Timestamp     | Dimension | Value     |\n",
    "|---------------|-----------|-----------|\n",
    "| 2024-11-17 01 | Status    | Active    |\n",
    "| 2024-11-17 02 | Status    | Active    |\n",
    "| 2024-11-17 03 | Status    | Active    |\n",
    "| 2024-11-17 04 | Status    | Inactive  |\n",
    "| 2024-11-17 05 | Status    | Inactive  |\n",
    "\n",
    "#### Compressed with RLE:\n",
    "| Start Timestamp | End Timestamp   | Dimension | Value     |\n",
    "|------------------|-----------------|-----------|-----------|\n",
    "| 2024-11-17 01    | 2024-11-17 03  | Status    | Active    |\n",
    "| 2024-11-17 04    | 2024-11-17 05  | Status    | Inactive  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Strengths of RLE for Temporal Cardinality Explosion**\n",
    "\n",
    "1. **Reduced Storage Requirements**:\n",
    "   - Instead of storing each timestamp, RLE combines rows with the same dimension value into a single entry.\n",
    "   - Especially effective for datasets with long periods of stability in values (e.g., a sensor reporting the same status for hours).\n",
    "\n",
    "2. **Efficient Historical Analysis**:\n",
    "   - Temporal data can be reconstructed efficiently while avoiding redundant storage.\n",
    "\n",
    "3. **Improved Query Performance**:\n",
    "   - Queries operating on aggregated or stable time spans (e.g., `\"status = 'active' for last 3 hours\"`) become faster due to fewer rows in the table.\n",
    "\n",
    "4. **Optimized for Sparse Changes**:\n",
    "   - Works best when dimension values don't change frequently over time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Drawbacks and Considerations**\n",
    "\n",
    "1. **Handling Frequent Changes**:\n",
    "   - If dimension values change frequently (high temporal granularity), the effectiveness of RLE diminishes.\n",
    "   - In such cases, compression ratio decreases, and performance benefits may be limited.\n",
    "\n",
    "2. **Reconstruction Overhead**:\n",
    "   - When querying data, runs need to be expanded or interpreted, which adds slight computational overhead.\n",
    "\n",
    "3. **Integration with Existing Systems**:\n",
    "   - Additional logic is needed to apply RLE during ETL pipelines and decompress data during queries.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimized Implementation**\n",
    "\n",
    "To leverage RLE effectively in managing temporal cardinality explosion:\n",
    "\n",
    "1. **Pre-Processing in ETL Pipelines**:\n",
    "   - Apply RLE during data ingestion or transformation.\n",
    "   - Group consecutive rows with the same dimension values.\n",
    "\n",
    "2. **Hybrid Storage Models**:\n",
    "   - Combine RLE with other compression techniques (e.g., dictionary encoding) for attributes with high variability.\n",
    "\n",
    "3. **Partitioning**:\n",
    "   - Partition the data by time intervals (e.g., daily/hourly) to limit the scope of RLE runs, balancing compression and access speed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: RLE in SQL**\n",
    "\n",
    "Suppose you have a temporal dataset in a table `temporal_data`:\n",
    "\n",
    "#### Original Data:\n",
    "```sql\n",
    "SELECT * FROM temporal_data;\n",
    "\n",
    "| Timestamp     | Dimension | Value  |\n",
    "|---------------|-----------|--------|\n",
    "| 2024-11-17 01 | Status    | Active |\n",
    "| 2024-11-17 02 | Status    | Active |\n",
    "| 2024-11-17 03 | Status    | Active |\n",
    "| 2024-11-17 04 | Status    | Inactive |\n",
    "| 2024-11-17 05 | Status    | Inactive |\n",
    "```\n",
    "\n",
    "#### SQL to Compress with RLE:\n",
    "```sql\n",
    "SELECT \n",
    "    MIN(Timestamp) AS Start_Timestamp,\n",
    "    MAX(Timestamp) AS End_Timestamp,\n",
    "    Dimension,\n",
    "    Value\n",
    "FROM (\n",
    "    SELECT \n",
    "        Timestamp,\n",
    "        Dimension,\n",
    "        Value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY Dimension, Value ORDER BY Timestamp) \n",
    "          - ROW_NUMBER() OVER (ORDER BY Timestamp) AS Run_Group\n",
    "    FROM temporal_data\n",
    ") t\n",
    "GROUP BY Dimension, Value, Run_Group;\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "| Start_Timestamp | End_Timestamp   | Dimension | Value     |\n",
    "|------------------|-----------------|-----------|-----------|\n",
    "| 2024-11-17 01    | 2024-11-17 03  | Status    | Active    |\n",
    "| 2024-11-17 04    | 2024-11-17 05  | Status    | Inactive  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "Run-Length Encoding reduces temporal cardinality explosions by compressing repetitive data sequences in time-series datasets. While effective in scenarios with sparse changes, its success depends on the stability of dimension values over time. RLE can be a powerful tool when integrated with data pipelines to manage high-cardinality dimensions and optimize historical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b90fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
